<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistical Modeling on Computational Psychology</title>
    <link>http://haines-lab.com/categories/statistical-modeling/</link>
    <description>Recent content in Statistical Modeling on Computational Psychology</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Nathaniel Haines, PhD</copyright>
    <lastBuildDate>Sun, 10 Jan 2021 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://haines-lab.com/categories/statistical-modeling/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Series on Building Formal Models of Classic Psychological Effects: Part 1, the Dunning-Kruger Effect</title>
      <link>http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/</guid>
      <description>Introduction The Dunning-Kruger effect is perhaps one of the most well-known effects in all of social psychology, defined as the phenomenon wherein people with low “objective” skill tend to over-estimate their objective skill, whereas people with high objective skill tend to under-estimate their skill. In the popular media, the Dunning-Kruger effect is often summarized in a figure like the one below (source):
Caption: The Dunning-Kruger Effect in Popular Media
 Despite the widespread use of figures such as the one above, the specific form of the effect is misleading–it suggests that people with low objective skill perceive their skill to be higher than those with the highest skill, which is not what Dunning and Kruger actually found in their original 1999 study.</description>
    </item>
    
    <item>
      <title>On Curbing Your Measurement Error: From Classical Corrections to Generative Models</title>
      <link>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/</guid>
      <description>Introduction In this post, we will explore how measurement error arising from imprecise parameter estimation can be corrected for. Specifically, we will explore the case where our goal is to estimate the correlation between a self-report and behavioral measure–a common situation throughout the social and behavioral sciences.
For example, as someone who studies impulsivity and externalizing psychopathology, I am often interested in whether self-reports of trait impulsivity (e.g., the Barratt Impulsiveness Scale) correlate with performance on tasks designed to measure impulsive behavior (e.</description>
    </item>
    
    <item>
      <title>Thinking generatively: Why do we use atheoretical statistical models to test substantive psychological theories?</title>
      <link>http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/</guid>
      <description>The Reliability Paradox Defining Reliability In 2017, Hedge, Powell, and Sumner (2017) conducted a study to determine the reliability of a variety of of behavioral tasks. Reliability has many different meanings throughout the psychological literature, but what Hedge et al. were interested in was how well a behavioral measure consistently ranks individuals. In other words, when I have people perform a task and then measure their performance, does the measure that I use to summarize their behavior show high test-retest reliability?</description>
    </item>
    
    <item>
      <title>On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression</title>
      <link>http://haines-lab.com/post/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/</guid>
      <description>Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:
\[y = \beta_{0}+X\beta + \epsilon\tag{1}\] where \(\epsilon\) is the error, which is normally distributed as:
\[\epsilon \sim \mathcal{N}(0, \sigma)\tag{2}\] Unlike these traditional linear regression models, regularized linear regression models produce biased estimates for the \(\beta\) weights.</description>
    </item>
    
  </channel>
</rss>