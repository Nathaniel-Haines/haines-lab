<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Measurement Error | Computational Psychology</title>
    <link>http://haines-lab.com/category/measurement-error/</link>
      <atom:link href="http://haines-lab.com/category/measurement-error/index.xml" rel="self" type="application/rss+xml" />
    <description>Measurement Error</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Nathaniel Haines, PhD</copyright><lastBuildDate>Sun, 10 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://haines-lab.com/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_512x512_fill_lanczos_center_3.png</url>
      <title>Measurement Error</title>
      <link>http://haines-lab.com/category/measurement-error/</link>
    </image>
    
    <item>
      <title>A Series on Building Formal Models of Classic Psychological Effects: Part 1, the Dunning-Kruger Effect</title>
      <link>http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Dunning-Kruger effect is perhaps one of the most well-known effects in all of social psychology, defined as the phenomenon wherein people with low “objective” skill tend to over-estimate their objective skill, whereas people with high objective skill tend to under-estimate their skill. In the popular media, the Dunning-Kruger effect is often summarized in a figure like the one below (&lt;a href=&#34;https://www.businesstimes.com.sg/brunch/not-so-blissful-ignorance-the-dunning-kruger-effect-at-work&#34;&gt;source&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/dunning-kruger_misinterpretation.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: The Dunning-Kruger Effect in Popular Media&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the widespread use of figures such as the one above, the specific form of the effect is misleading–it suggests that people with low objective skill perceive their skill to be &lt;em&gt;higher than those with the highest skill&lt;/em&gt;, which is not what Dunning and Kruger actually found in their &lt;a href=&#34;https://doi.org/10.1037/0022-3514.77.6.1121&#34;&gt;original 1999 study&lt;/a&gt;. As clarified by many others (e.g., see &lt;a href=&#34;https://www.talyarkoni.org/blog/2010/07/07/what-the-dunning-kruger-effect-is-and-isnt/&#34;&gt;here&lt;/a&gt;), Dunning and Kruger actually found the following:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/orig_dunning-kruger_plots.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Original Data from Dunning &amp;amp; Kruger (1999)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The basic pattern across all these different domains is consistent with our original definition of the effect–that people with low objective skill tend to over-estimate their skill to quite a large extent, yet those with high objective skill under-estimate their skill, but to a lesser extent. However, unlike the popularized interpretation, the relationship between objective and perceived skill appears to be monotonic, such that, on average, people with low skill still perceive themselves to be less skilled compared to high-skilled people.&lt;/p&gt;
&lt;p&gt;Regardless of misinterpretations in the popular media, the notoriety of the Dunning-Kruger effect cannot be understated. As of January 2021, the original study has been cited upwards of 6500 times on Google Scholar alone, and the basic Dunning-Kruger effect has been consistently replicated across samples, domains, and contexts (see &lt;a href=&#34;https://doi.org/10.1016/B978-0-12-385522-0.00005-6&#34;&gt;here&lt;/a&gt;). In many ways, research on the Dunning-Kruger effect theorefore embodies the highest ideals of contemporary psychological science, passing the bar on replicability, generalizability, and robustness that psychological scientists have been striving for since the advent of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Replication_crisis&#34;&gt;replication crisis&lt;/a&gt; (see also &lt;a href=&#34;https://www.psychologytoday.com/us/blog/how-do-you-know/202012/dunning-kruger-isnt-real&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;There is just one small problem…&lt;/p&gt;
&lt;div id=&#34;wait-the-dunning-kruger-effect-is-just-measurement-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wait, the Dunning-Kruger Effect is just Measurement Error?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/media/measurement_error_meme.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perhaps surprisingly, the traditional methods used to measure the Dunning-Kruger effect have always been heavily scrutinized–even the original authors acknowledged potential issues with measurement error. Specifically, it is unclear to what extent that the classic effect arises due to an actual psychological bias, versus more mundain statistical measurement error resulting from regression to the mean. In fact, in their 1999 article, Dunning and Kruger explicitly recognize this possibility (p. 1124):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“… the reader may point to the regression effect as an alternative interpretation of our results. … Because perceptions of ability are imperfectly correlated with actual ability, the regression effect virtually guarantees this result. … Despite the inevitability of the regression effect, we believe that the overestimation we observed was more psychological than artifactual. For one, if regression alone were to blame for our results, then the magnitude of miscalibration among the bottom quartile would be comparable with that of the top quartile. A glance at Figure 1 quickly disabuses one of this notion.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Although Dunning and Kruger were willing to state their belief that their results were attributable more so to psychological rather than statistical effects, they did not provide an explicit generative model for their pattern of results. Their reasoning appears sound (i.e. that differences in magnitude of mis-estimation for those under/over average indicates something beyond regression to the mean), but without a generative model, it is unclear how much faith we should place in the authors’ beliefs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-the-dunning-kruger-effect&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating the Dunning-Kruger Effect&lt;/h2&gt;
&lt;p&gt;Fortunately, a number of simulation studies have since been conducted to show exactly how measurement error can generate data consistent with the Dunning-Kruger effect. &lt;a href=&#34;https://doi.org/10.1016/S0191-8869(01)00174-X&#34;&gt;Ackerman et al. (2002)&lt;/a&gt; was one of the first, which showed how the pattern found in Dunning and Kruger’s original study could arise from plotting any two variables with less than a perfect correlation (i.e. &lt;span class=&#34;math inline&#34;&gt;\(r=1\)&lt;/span&gt;) using the objective skill quantile versus perceived skill percentile convention from the original study. For example, if observed objective skill and perceived skill are correlated at &lt;span class=&#34;math inline&#34;&gt;\(r=.19\)&lt;/span&gt;, using the standard plotting convention, Ackerman et al. (2002) obtained the following pattern:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/ackerman_2002.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Figure 1 from Ackerman et al. (2002)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The take-away from the above results is that any less-than-perfect correlation between two variables will produce the general pattern found in Dunning and Kruger’s original study, which results from a statistical phenomenon termed &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_toward_the_mean&#34;&gt;&lt;em&gt;regression to the mean&lt;/em&gt;&lt;/a&gt;. If the concept of regression to the mean seems a bit magical (it certainly was for me when I was first introduced), it is useful to simulate data to get a sense of what is going on. The R code below replicates previous simulation studies, but at three different correlations between objective and perceived skill:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Libraries we will use throughout the post
library(mvtnorm)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gghighlight)
library(patchwork)
library(foreach)
library(rstan)
library(bayesplot)
library(hBayesDM)
library(httr)

# RNG seed for reproducing
set.seed(43202)

# Number of simulated participants
n &amp;lt;- 100

# Group-level means/SDs on &amp;quot;objective&amp;quot; and perceived skill assessments 
mu    &amp;lt;- c(0,0)
sigma &amp;lt;- c(1,1)

# Three different correlations between objective and perceived skill
obj_per_cor &amp;lt;- c(0, .5, 1)

# Simulate data
sim_dk_plots &amp;lt;- foreach(i=obj_per_cor) %do% {
  # Make correlation matrix
  R &amp;lt;- matrix(c(1, i,
                i, 1),
            ncol = 2)

  # Construct covariance matrix
  Epsilon &amp;lt;- diag(sigma)%*%R%*%diag(sigma)
  
  # Simulate correlated data
  sim &amp;lt;- rmvnorm(n, mu, Epsilon)
  
  # Compute quantiles, summarize and save out plot for given correlation
  sim %&amp;gt;%
    as.data.frame() %&amp;gt;%
    rename(obs_obj = V1,
           obs_per = V2) %&amp;gt;%
    mutate(quantile = cut_number(obs_obj, n = 4, labels = F)) %&amp;gt;%
    group_by(quantile) %&amp;gt;%
    summarize(mu_obj = mean(obs_obj),
              mu_per = mean(obs_per)) %&amp;gt;%
    ggplot() +
    geom_point(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;)) +
    geom_line(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;)) +
    geom_point(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;)) +
    geom_line(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;)) +
    annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 2.5, y = -1, 
             color = I(&amp;quot;black&amp;quot;), size = 5) +
    annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.8, y = .5, 
             color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
    ggtitle(paste0(&amp;quot;r = &amp;quot;, i)) +
    xlab(&amp;quot;Quantile&amp;quot;) +
    ylab(&amp;quot;Score&amp;quot;) +
    ylim(-1.7, 1.5) +
    theme_minimal(base_size = 15) +
    theme(panel.grid = element_blank(), 
          plot.title = element_text(hjust=.5))
}

# Plot objective versus perceived quantiles
sim_dk_plots[[1]] | sim_dk_plots[[2]] | sim_dk_plots[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-1-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Very interesting! We are able to replicate the simulation studies from before. More importantly, the interpretation of the Dunning-Kruger effect is not looking so good. As in previous simulation studies, our simulated data looks strikingly similar to that reported by Dunning and Kruger in their original study, yet our model contains no psychological mechanisms that could produce the effect–the results are purely due to the lack of correlation between objective and perceived skill, which could result from a number of sources including measurement error. In fact, a few recent studies (see &lt;a href=&#34;http://dx.doi.org/10.5038/1936-4660.9.1.4&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;http://dx.doi.org/10.5038/1936-4660.10.1.4&#34;&gt;here&lt;/a&gt;) have used this line of reasoning to claim that the traditional psychological interpretation of the Dunning-Kruger effect is likely incorrect, and a recent &lt;a href=&#34;mcgill.ca/oss/article/critical-thinking/dunning-kruger-effect-probably-not-real&#34;&gt;scicomm article&lt;/a&gt; brought the controversy to academic Twitter (&lt;a href=&#34;https://twitter.com/Nate__Haines/status/1345859629444706304?s=20&#34;&gt;which I participated in&lt;/a&gt;, inspiring this post!).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-generative-model-of-the-dunning-kruger-effect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building a Generative Model of the Dunning-Kruger Effect&lt;/h1&gt;
&lt;div id=&#34;a-simple-noise-bias-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Simple Noise + Bias Model&lt;/h2&gt;
&lt;p&gt;So far, we have found that the group-level Dunning-Kruger effect can be simulated through regression to the mean, which we expect to see anytime we are observing the relationship between any two variables that have less than a perfect correlation. However, there are two problems with relying on regression to the mean &lt;strong&gt;&lt;em&gt;as an explanation&lt;/em&gt;&lt;/strong&gt; for how the Dunning-Kruger effect is generated:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression to the mean alone cannot account for the differences in mis-estimation found in real data between those with high versus low objective skill, and&lt;/li&gt;
&lt;li&gt;Regression to the mean results from the correlation between two variables, but what is it exactly that causes a high or low correlation between objective and perceived skill?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Regarding (1), if you compare the simulated data from either Ackerman et al. (2002) or from our own example in the preceeding section to the emprical data in Dunning and Kruger (1999), you will see that our simulated data indeed misses this difference in mis-estimation. In the emprical data, the perceived skill line is shifted upward relative to what we would expect. Therefore, it is clear that regression to the mean alone will not re-produce all aspects of the traditional Dunning-Kruger effect.&lt;/p&gt;
&lt;p&gt;Point (2) is a bit more nuanced, and it is something that we can only really answer by thinking about the data-generating process hollistically. For example, if we assume that there is some latent, underlying skill that leads to both observed performance on an objective measure and perceived performance on a subjective measure, how could this single latent skill produce patterns that are consistent with real-world data? &lt;a href=&#34;https://doi.org/10.1037/0022-3514.90.1.60&#34;&gt;Burson et al. (2006)&lt;/a&gt; took exactly this approach, which &lt;strong&gt;&lt;em&gt;I term the generative approach&lt;/em&gt;&lt;/strong&gt; (see &lt;a href=&#34;https://psyarxiv.com/xr7y3&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/&#34;&gt;here&lt;/a&gt;). They proposed a noise + bias model (see their model specification in the appendix, on p. 77), which assumes that each participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;’s observed perceived skill rating &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}}\)&lt;/span&gt; results from their objective underlying skill level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;, in addition to some noise &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; and a bias term &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; that captures over-confidence. Conceptually, there are many ways we could implement such a noise + bias model, but the authors chose to implement it as a linear model with normally distributed errors (note that the original model assumes that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; can vary across participants, but I simplified here for illustration):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  y_{i,\text{per}} &amp;amp; = \theta_i+ b + \epsilon \\
  \epsilon &amp;amp; \sim \mathcal{N}(0,\sigma)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is useful to take a moment and reflect on the difference between how the problem is formulated here, as opposed to, say, thinking only in terms of the observed correlation between objective and perceived skill. The above formulation puts forth a generative model of how the person-level observed data arise, and the parameters now have direct psychological interpretations. For example, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is interpreted as a psychological bias that leads to over-confident perceptions if &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; can be thought of as a “perception noise” parameter that controls how far one’s observed perception judgements deviate from their objective underlying skill &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;. Before, when thinking about the problem in terms of the observed correlation, there was not a clear mapping from our psychological theory to the observed data. Our statistical toolbox consisted only of group-level means, standard deviations, and correlations, yet our target for inference was on person-level psychological processes that give rise to observed data. As a result, the theoretical value of our statistical model suffered.&lt;/p&gt;
&lt;p&gt;Now, before using our generative model to explain the Dunning-Kruger effect, we first need to make a small extention. Specifically, Burson et al. (2006) specified the relationship between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; (the underlying “objective” skill) and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}}\)&lt;/span&gt; (the observed perception ratings), but they did not specify how &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; produces observed data on the “objective” skill measure, which we will term &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}}\)&lt;/span&gt;. To do so, we will add a second noise term to the model that is specific to the objective measure, which gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  y_{i,\text{obj}} &amp;amp; = \theta_i + \epsilon_{\text{obj}} \\
  y_{i,\text{per}} &amp;amp; = \theta_i + b + \epsilon_{\text{per}} \\
  \epsilon_{\text{obj}} &amp;amp; \sim \mathcal{N}(0,\sigma_{\text{obj}}) \\
  \epsilon_{\text{per}} &amp;amp; \sim \mathcal{N}(0,\sigma_{\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After this modification, we have a model of how one’s underlying skill (&lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;) generates observed data on both the objective measure (&lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}}\)&lt;/span&gt;) and the subjective measure (&lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}}\)&lt;/span&gt;). Therefore, we can simulate person-level data from the model. Before doing so, however, I prefer to rearrange the terms on the model to clarify the generative structure (see also &lt;a href=&#34;https://drbenvincent.medium.com/the-dunning-kruger-effect-probably-is-real-9c778ffd9d1b&#34;&gt;Ben Vincent’s recent blog&lt;/a&gt;, in which he formulates the same generative model; note also that we have added an assumption about how &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters are distributed):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  y_{i,\text{obj}} &amp;amp; \sim \mathcal{N}(\theta_i, \sigma_{\text{obj}}) \\
  y_{i,\text{per}} &amp;amp; \sim \mathcal{N}(\theta_i + b, \sigma_{\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite being mathematically identical to the first formulation, I believe this new formulation makes our assumptions more clear–that each person’s observed data are generated by a normal distribution with a mean determined by their latent skill (&lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;), which is shifted by a bias term (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;) when making perceived jugdements on their skill. Additionally, there is a context-specific &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter, which controls how much oberved performance on the objective measure and perceived ratings of skill vary around the objective underlying skill. Because the observed data are assumed to follow a normal distribution, I will term this the &lt;strong&gt;&lt;em&gt;normal noise + bias model&lt;/em&gt;&lt;/strong&gt;. However, it is worth emphasizing that this model is only one implementation of the core theoretical constructs of perception noise and bias, and we would need to modify the model–while retaining these core theoretical constructs–to extend it into domains wherein we do not expect continuous data. For example, if participants performed a True/False test to measure skill in some domain, a generative model should respect the structure of the observed data by producing dichotomous responses.&lt;/p&gt;
&lt;p&gt;Alright! Enough on assumptions and limitations (for now :D), here is what happens when we simulate data from the normal noise + bias model, and then summarize the results using the traditional Dunning-Kruger plotting convention:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# RNG seed for reproducing
set.seed(43204)

# Number of simulated participants
n &amp;lt;- 100

# Latent &amp;quot;objective&amp;quot; skill
theta &amp;lt;- rnorm(n, 0, 1)

# Set noise parameters
sigma_obj &amp;lt;- 1
sigma_per &amp;lt;- 1

# Set bias parameter
b &amp;lt;- 1

# Simulate objective and perceived observed data
sim_obj &amp;lt;- rnorm(n, theta, sigma_obj)
sim_per &amp;lt;- rnorm(n, theta + b, sigma_per)
  
# Compute quantiles, summarize and plot 
data.frame(sim_obj = sim_obj,
           sim_per = sim_per) %&amp;gt;%
  mutate(quantile = cut_number(sim_obj, n = 4, labels = F)) %&amp;gt;%
  group_by(quantile) %&amp;gt;%
  summarize(mu_obj = mean(sim_obj),
            mu_per = mean(sim_per)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 1) +
  geom_point(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 1) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 2.2, y = -1, 
           color = I(&amp;quot;black&amp;quot;), size = 5) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.8, y = .5, 
           color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
  xlab(&amp;quot;Quantile&amp;quot;) +
  ylab(&amp;quot;Score&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like before, we are able to re-produce the Dunning-Kruger effect quite well! Unlike before, our generative model assumes that the effect arises through two psychological mechanisms–a general overconfidence (i.e. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;), and perception noise (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{obj}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{per}}\)&lt;/span&gt;). Importantly, it is the perception noise parameters that actually lead to a discrepancy between the observed objective and subjective responses. Intuitively, if &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{obj}} = \sigma_{\text{per}} = 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}} = \theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}} = \theta_i + b\)&lt;/span&gt;. In this case, the correlation between &lt;span class=&#34;math inline&#34;&gt;\(\text{cor}(\mathbf{y}_{\text{obj}}, \mathbf{y}_{\text{per}}) = 1\)&lt;/span&gt;, and there is subsequently no regression to the mean. Instead, the perception ratings are simply shifted upward according to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. Conversely, as either &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{obj}}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{per}}\)&lt;/span&gt; increase toward &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{cor}(\mathbf{y}_{\text{obj}}, \mathbf{y}_{\text{per}}) \to 0\)&lt;/span&gt;, and we get results like those shown in our previous simulations where &lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt; (although the perception ratings line would still be shifted upward by &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;extending-the-noise-bias-model-to-dichotomous-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extending the Noise + Bias Model to Dichotomous Data&lt;/h3&gt;
&lt;p&gt;Before moving on, we need to modify the model for application in settings where the observed data are dichotomous in nature. The primary purpose for this is not theoretical, but instead to prepare ourselves for fitting the model to a real-world dataset in upcoming sections. In other words, we would like to modify only the part of our model that connects the core theoretical constructs of perception noise and bias to the observed data. We will get into specific details regarding the dataset later on, but for now, it is sufficient to say that we want a model that could generate data in a setting where:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Objective” skill is assessed on an “objective” multiple item test, where each item is scored correct or incorrect, and&lt;/li&gt;
&lt;li&gt;Perceived skill is assessed by having participants estimate how many questions they answered correctly on the objective test.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many possible ways to approach this general problem, but for our purposes, we will begin with the simplest model: the bernoulli model. As described in a &lt;a href=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/&#34;&gt;previous post&lt;/a&gt;, the bernoulli distribution describes data that arise through a “biased coin flipping” process. For example, imagine that we have a biased coin, such that it lands “heads” 70% of the time, and “tails” 30% of the time. But suppose that we do not know the actual bias of the coin–how might we figure it out? Well, clearly we should flip the coin a few times! But how would we then estimate the underlying “success probability” (if we arbitrarily define success as landing “heads”)? This is exactly the same problem we encounter when aiming to estimate the underlying “objective skill” of a person from an objective test where each item is scored as correct or incorrect! As with a coin flip, if we assume that each item response &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is independent and equally informative with respect to the underlying objective probability of answering correctly (which directly corresponds to the “objective skill”), we can use a bernoulli model, defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Pr}(y_{t}=1) = p = 1 - \text{Pr}(y_{t}=0) = 1 - q\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(y_{t} = 1\)&lt;/span&gt; indicates that item &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; was answered correctly (a “success” trial), and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,t} = 0\)&lt;/span&gt; indicates that it was answered incorrectly (a “failure” trial). The likelihood of observing a correct response is simply &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, and the likelihood of observing an incorrect response is then &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;. &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34;&gt;Per Wikipedia&lt;/a&gt;, the likelihood of a single response–termed a &lt;em&gt;bernoulli trial&lt;/em&gt;–can also be written more compactly as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p^{k}(1-p)^{1-{k}}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is indicates a correct (&lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt;) or incorrect (&lt;span class=&#34;math inline&#34;&gt;\(k=0\)&lt;/span&gt;) response. The re-writing is useful when we want to define how likely we are to observe, say, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; successes within &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; trials. In our case, for a given participant, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; would be the number of of questions they answered correctly, and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; the total number of questions on the test. In this case, we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; bernoulli trials, and the likelihood of observing their sum follows a &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34;&gt;binomial distribution&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{n!}{k!(n-k)!}p^{k}(1-p)^{n-k}\]&lt;/span&gt;
This equation may appear a bit daunting at first glance, but it is actually quite straightforward! The first term is the binomial coefficient, and it probably looks familiar. Specifically, if you had to take the GRE for grad shcool applications, you have likely been exposed to this equation, also sometimes called an “n choose k formula”. Far from being arbitrary, the binomial coefficient is a useful, compact representation of how many different ways you could receive &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; successes within a given &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; trials. For intuition, imagine that we have a test with 3 items (&lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt;), and our participant answers 2 items correctly (&lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;). If we somehow know that their objective underlying probability of responding correctly is .8 (&lt;span class=&#34;math inline&#34;&gt;\(p=.8\)&lt;/span&gt;), how likely are our observed data? To get the answer, we need to first consider how many different ways in which we could observe 2 correct responses in any 3 trials. For example, we could have observed any of the following sequences of correct/incorrect responses: &lt;span class=&#34;math inline&#34;&gt;\(\{1,1,0\}, \{1,0,1\}, \{0,1,1\}\)&lt;/span&gt;. Of course, it is a pain to write out all these different possibilities just to find out how many ways observed data could have arised–and herein lies the power of the binomial coefficient! Working through &lt;span class=&#34;math inline&#34;&gt;\(\frac{3!}{2!(3-2)!}\)&lt;/span&gt;, we get &lt;span class=&#34;math inline&#34;&gt;\(\frac{3 \times2 \times 1}{2 \times 1 \times 1} = 3\)&lt;/span&gt;–the same answer. Knowing that we could get 2 correct responses in 3 trials in 3 different ways, we then need to determine how likely the actual responses are. Since &lt;span class=&#34;math inline&#34;&gt;\(p=.8\)&lt;/span&gt; and trials are assumed indpendent, we can get the joint probability by taking the product as &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(k=1) \times \text{Pr}(k=1) \times \text{Pr}(k=0) = .8 \times .8 \times .2 = .128\)&lt;/span&gt;. Given that these particular responses could have been generated in 3 different ways, we then multiply everything together to get &lt;span class=&#34;math inline&#34;&gt;\(3 \times .128= .384\)&lt;/span&gt;. What this means is that, if our participant has a objective &lt;span class=&#34;math inline&#34;&gt;\(p=.8\)&lt;/span&gt;, and they were to take the same 3 item test an infinite number of times (without remembering the questions from before–indeed a nightmarish situation, but let’s roll with it anyway), we would expect them to get 2 of the 3 question correct 38.4% of the time.&lt;/p&gt;
&lt;p&gt;To confirm, we can do a very simple simulation in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Draw 3 samples from a binomial distribution where n=1 and p=.8 
# (equivalent to bernoulli trials), and then take the sum 
sum(rbinom(3,1,.8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Replicate the example above 10000 times, and compute the proportion of
# times that the sum is equal to 2
mean(replicate(10000, sum(rbinom(3,1,.8)))==2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3796&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simulation is pretty close to the analytical solution! With the binomial likelihood covered, we are ready to move on to the next step–incorporating the theoretical constructs of perception noise and bias into a model that generates data as described by (1) and (2) above.&lt;/p&gt;
&lt;p&gt;Now that we are assuming a binomial generative model, we need to think of our problem in terms of how to model the succuss probability, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; as opposed to the mean of a normal distribution, as in the normal noise + bias model in the previous section. The most intuitive way to do this is to take our underlying objective skill parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; from before, and apply a transformation to them such that &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta_i &amp;lt; 1\)&lt;/span&gt; as opposed to &lt;span class=&#34;math inline&#34;&gt;\(-\infty &amp;lt; \theta_i &amp;lt; +\infty\)&lt;/span&gt;. Retaining the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\theta_i \sim \mathcal{N}(0,1)\)&lt;/span&gt;, a good option is the cumultative distribution function of the standard normal distribution, or the probit transformation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \Phi(\theta_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The probit transtormation has a useful interpretation—we can now think of the underlying &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters as z-scores, and the resulting success probabilities &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; as the area under the curve of the standard normal distribution up to the z-score &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;. We can represent this graphically in R as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Visualize cumulative distribution function of normal distribution

# For the first plot, we look at the area under \theta = -1
p1 &amp;lt;- data.frame(x = seq(-5, 5, length.out = 100)) %&amp;gt;% 
  mutate(y = dnorm(x)) %&amp;gt;%
  ggplot(aes(x, y)) + 
  geom_area(fill = &amp;quot;#8F2727&amp;quot;) + 
  gghighlight(x &amp;lt; -1) +
  ggtitle(expression(theta~&amp;quot; = -1&amp;quot;)) +
  xlab(expression(theta)) +
  ylab(&amp;quot;density&amp;quot;) +
  annotate(geom=&amp;quot;text&amp;quot;, x = -3, y = .05, label = &amp;quot;.16&amp;quot;, 
           color = I(&amp;quot;#8F2727&amp;quot;), size = 8) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(hjust=.5))

# For the second plot, we look at the area under \theta = +1
p2 &amp;lt;- data.frame(x = seq(-5, 5, length.out = 100)) %&amp;gt;% 
  mutate(y = dnorm(x)) %&amp;gt;%
  ggplot(aes(x, y)) + 
  geom_area(fill = &amp;quot;#8F2727&amp;quot;) + 
  gghighlight(x &amp;lt; 1) +
  ggtitle(expression(theta~&amp;quot; = +1&amp;quot;)) +
  xlab(expression(theta)) +
  annotate(geom=&amp;quot;text&amp;quot;, x = -3, y = .05, label = &amp;quot;.84&amp;quot;, 
           color = I(&amp;quot;#8F2727&amp;quot;), size = 8) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(hjust=.5))

# Plot together for comparison
p1 | p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the area under the curve up to the given value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; is highlighted in red, and the ratio of red area to total area is shown next to the highlighted portion–these are the values for &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; that result from the probit transformation. If we make the simplifying assumption that observed responses on the objective skill test are directly related to &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; through the binomial generative model (i.e. no perception noise or bias), we can begin constructing our model as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i)\\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite the fact that we do not have a noise term in the model, the relationship between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}}\)&lt;/span&gt; is still probabilistic, as in the normal noise + bias model. This is because the binomial model is inherently probabilistic–with a set number of trials &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{obj}}\)&lt;/span&gt; and a given underlying correct response probability &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;, the model will produce different observed responses if we generate data from it repeatedly. We can do this in R, looking at 50 different realizations from each of the example participants illustrated in the previous figure, where we demonstrated the probit transformation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of trials to simulate
n_trials &amp;lt;- 10

# Binomial sums of trials
# For participant where \theta = -1
subj1_obj &amp;lt;- replicate(50, sum(rbinom(n_trials, 1, pnorm(-1))))
# For participant where \theta = +1
subj2_obj &amp;lt;- replicate(50, sum(rbinom(n_trials, 1, pnorm(1))))

# Plotting binomial counts
p1_obj &amp;lt;- ggplot() +
  geom_bar(aes(x = as.factor(subj1_obj)), 
           fill = &amp;quot;#8F2727&amp;quot;, stat = &amp;quot;count&amp;quot;) +
  geom_vline(xintercept = 6, linetype = 2) +
  ggtitle(expression(theta~&amp;quot; = -1&amp;quot;)) +
  xlab(&amp;quot;Number of Correct Responses&amp;quot;) +
  scale_x_discrete(limit = as.factor(seq(0,10,1))) +
  coord_cartesian(ylim=c(0,22)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))
p2_obj &amp;lt;- ggplot() +
  geom_bar(aes(x = as.factor(subj2_obj)), 
           fill = &amp;quot;#8F2727&amp;quot;, stat = &amp;quot;count&amp;quot;) +
  geom_vline(xintercept = 6, linetype = 2) +
  ggtitle(expression(theta~&amp;quot; = +1&amp;quot;)) +
  xlab(&amp;quot;Number of Correct Responses&amp;quot;) +
  scale_x_discrete(limit = as.factor(seq(0,10,1))) +
  coord_cartesian(ylim=c(0,22)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(), 
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))

# plot together, as before
p1_obj | p2_obj&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the black dotted line indicates a sum of 5, which I include simply as a reference point to compare the two “participants”. As anticipated, across the 50 different simulations of the model, each with &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{obj}}=10\)&lt;/span&gt;, the observed sum of correct responses varies around the underlying generating probability for each participant, &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;. It is also worth recognizing that these distributions are quite skewed, such that they cannot be fully appreciated with a single summary statistic as is often done in studies (e.g., using the proportion of correct responses as an independent/dependent variable in a model).&lt;/p&gt;
&lt;!-- We can start with a conceptual question. What is the function of noise? Per the normal noise + bias model, perception noise is a form of uncertainty or general lack of information that manifests as participants having imperfect knowledge of their objective underlying skill, $\theta_i$. Time for a thought experiment! Imagine you were in a scenario where you took a 10 item test. Upon finishing, for some reason, you were highly uncertain regarding your level of skill in the tested domain. If this were the case (really try to imagine it!), and you were subsequently asked to estimate how many items you answered correctly, what would be your best guess? You may be tempted to answer &#34;none&#34;, but wouldn&#39;t that imply that you thought the test was difficult, which can only be in reference to your skill level. So, what is your answer? Now, imagine the opposite case--that after having taken the test, you were completely certain regarding your skill level. If you were again asked to estimate how many items you answered correctly, what would you say? For even better intuition, imagine that I have a completed, 10 item test from an individual, but I will tell you nothing at all about them or the test--the test could be anything from an experiement administered to a rat to my 3rd grade history exam, and I will leave you with utter, complete uncertainty. Now, how many questions were correct on this test?  --&gt;
&lt;p&gt;The next step is to extend our model so that it can also account for perceived judgements. In our case, the perceived judgements take the form of asking participants to estimate how many items that they got correct on the objective skill assessment. This is nice, because it means that we can still use the binomial distribution as a generative model, although we do need to somehow incorporate the perception noise and bias terms. To do so, we need to think more deeply about the underlying decision process. For perception bias, the simplest implementation is to add a term, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, to &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; before the probit transformation, which will lead to decrease or increase the resulting perceived probability of responding correctly to each item correctly (&lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt;) depending on whether &lt;span class=&#34;math inline&#34;&gt;\(b&amp;lt;0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Perception noise will be a bit more tricky to deal with. Per the normal noise + bias model, perception noise is a form of uncertainty or general lack of information that manifests as participants having imperfect knowledge of their objective underlying skill, &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;, and subsequently their objective probability of getting items correct, &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;. One way to capture this idea is to take inspiration from signal detection theory (which is a reasonable starting place, as SDT has been used to model exactly these types of subjective, uncertain judgements; see &lt;a href=&#34;https://doi.org/10.1016/0030-5073(80)90045-8&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1037/0033-295X.101.3.490&#34;&gt;here&lt;/a&gt;). In this framework, we can model noise as the standard deviation of a normal distribution, which scales the difference between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}} = \Phi(\frac{\theta_i + b}{\sigma})\)&lt;/span&gt;, which gives us the following full model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i)\\
  p_{i,\text{per}} &amp;amp; = \Phi(\frac{\theta_i + b}{\sigma}) \\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}}) \\
  y_{i,\text{per}} &amp;amp; \sim \text{Binomial}(n_{\text{per}}, p_{i,\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can gain a better intuition of the noise and bias terms by visualizing how they work. First, a look at how the noise term influences resulting &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt; values when &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set parameters
theta &amp;lt;- 1
b     &amp;lt;- 0           # no bias
sigma &amp;lt;- c(.5, 1, 2) # three values for sigma

# Loop over different sigma values
sdt_plots &amp;lt;- foreach(sig=sigma) %do% {
  # Plot the &amp;quot;noise distribution&amp;quot;, centered at \theta+b
  data.frame(x = seq(-5.5, 5.5, length.out = 100)) %&amp;gt;% 
      mutate(y = dnorm(x, theta+b, sig)) %&amp;gt;%
      ggplot(aes(x, y)) + 
      geom_area(fill = &amp;quot;#8F2727&amp;quot;) + 
      gghighlight(x &amp;gt; 0) +
      geom_vline(xintercept = 0, linetype = 2) +
      ggtitle(paste0(&amp;quot;sigma = &amp;quot;, sig)) +
      xlab(expression(theta+b)) +
      ylab(&amp;quot;density&amp;quot;) +
      ylim(0,.8) +
      annotate(geom=&amp;quot;text&amp;quot;, x = -3.2, y = .25, 
               label = round(pnorm((theta+b)/sig), 2), 
               color = I(&amp;quot;#8F2727&amp;quot;), size = 8) +
      theme_minimal(base_size = 15) +
      theme(panel.grid = element_blank(),
            axis.text.y = element_blank(),
            plot.title = element_text(hjust=.5))
}

# Plot examples
sdt_plots[[1]] | sdt_plots[[2]] | sdt_plots[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-6-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this function, as &lt;span class=&#34;math inline&#34;&gt;\(\sigma \to +\infty\)&lt;/span&gt;, perception noise increases indefinitely, and &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}} \to .5\)&lt;/span&gt;. As &lt;span class=&#34;math inline&#34;&gt;\(\sigma \to 0\)&lt;/span&gt;, perception noise becomes non-existent, and we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{i,\text{per}} \to
\begin{cases} 
  0, ~\text{ if } \theta+b &amp;lt; 0 \\ 
  1, ~\text{ if } \theta+b &amp;gt; 0 \\ 
  .5, ~\text{if } \theta+b = 0 
\end{cases}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Intuitively, when perception noise is very low, people with above average objective skill (indicated by &lt;span class=&#34;math inline&#34;&gt;\(\theta_i = 0\)&lt;/span&gt;) will perceive their performance to be top-notch, whereas those below average will perceive their performance to be absolutely terrible. Conversely, if noise is very high, the opposite pattern emerges, where those with very low objective skill will perceive their performance to be better than it truly is, and those with high skill will perceive their performance to be worse than it truly is. Finally, when combined with a general bias, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the amount of over- or under-confidence can be shifted. Therefore, if &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;1\)&lt;/span&gt;, bias and noise work together to produce an effect where those with low skill overestimate their skill to a large extend, whereas those with high skill underestimate their skill, but to a lesser extent–the classic Dunning-Kruger effect! To see this, it is useful to plot out &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obs}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set parameters
thetas &amp;lt;- seq(-5, 5, length.out = 100) # whole range of theta
bs     &amp;lt;- c(0, 1)     # two levels of bias
sigmas &amp;lt;- c(.5, 1, 2) # three levels of noise

# Loop over different b and sigma values
noise_bias_dat &amp;lt;- foreach(b=bs, .combine = &amp;quot;rbind&amp;quot;) %do% {
  foreach(sig=sigma, .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Generate p_obj and p_per from theta, sigma, and b
    data.frame(theta = thetas) %&amp;gt;% 
        mutate(b     = b,
               sigma = sig, 
               p_obj = pnorm(theta),
               p_per = pnorm((theta+b)/sig))
  }
}

# Plot different combinations of noise and bias
noise_bias_dat %&amp;gt;%
  ggplot(aes(p_obj, p_per)) + 
  geom_line(color = I(&amp;quot;#8F2727&amp;quot;), size = 1.5) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  facet_grid(b ~ sigma, labeller = labeller(.rows = label_both, 
                                            .cols = label_both)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-7-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I don’t know about you, but I think this is really cool. What we see is that two simple mechanisms of perception noise and bias can generate a diverse range of patterns, of which the Dunning-Kruger effect is a special case (see the bottom right panel, where &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;1\)&lt;/span&gt;). Additionally, there is a special case where participants are expected to be perfectly callibrated to their underlying “objective” skill level, which occurs when &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; (see the top middle panel). Not only can this model give us an explanation for the Dunning-Kruger effect, but it also facilitates making novel predictions. For example, if we design an experiment to manipulate uncertainty in people’s confidence judgements, we would expect this to influence perception noise, which then produces specific patterns of behavior depending on whether we increase or decrease uncertainty.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-perception-distortion-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Perception Distortion Model&lt;/h2&gt;
&lt;p&gt;Before moving on to fit our model to real-world data, I thought it would be worth considering a different model inspired by the confidence judgement literature (in particular, see &lt;a href=&#34;https://doi.org/10.1016/j.jmp.2010.08.011&#34;&gt;here&lt;/a&gt;). This model does not include noise per se, but instead it relies on a function describing how people’s judgements become distorted when they are accessed. This distortion could arise from various different sources unrelated to uncertainty, but as we will see, it generates data analagous to the binomial noise + bias model in the previous section.&lt;/p&gt;
&lt;p&gt;To begin, we will assume that the objective performance component for our new model is the same as in the binomial noise + bias model, giveing us:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i)\\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So far, so good–nothing different from before. Next, we will use a similar function as described by &lt;a href=&#34;https://doi.org/10.1016/j.jmp.2010.08.011&#34;&gt;Merkle et al. (2011)&lt;/a&gt;, which is grounded in the probability estimation literature:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{i,\text{per}} = \frac{\delta p_{i,\text{obj}}^{\gamma}}{\delta p_{i,\text{obj}}^{\gamma} + (1-p_{i,\text{obj}})^{\gamma}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; is a bias parameters akin to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; in the binomial noise + bias model, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a &lt;strong&gt;&lt;em&gt;perception distortion&lt;/em&gt;&lt;/strong&gt; parameter, which controls how strongly participants over- or under-weight below- and above-average levels of skill, as indexed by &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;. Per usual, it is best to visualize the function to better interpret the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Perception distortion function
per_dist &amp;lt;- function(p_obj, gamma, delta) {
  delta*(p_obj^gamma) / (delta*(p_obj^gamma) + (1-p_obj)^gamma)
}

# Set parameters
thetas &amp;lt;- seq(-5, 5, length.out = 100) # whole range of theta
deltas &amp;lt;- c(1, 2)     # two levels of bias
gammas &amp;lt;- c(2, 1, .3) # three levels of noise

# Loop over different delta and gamma values
per_dist_dat &amp;lt;- foreach(delta=deltas, .combine = &amp;quot;rbind&amp;quot;) %do% {
  foreach(gamma=gammas, .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Generate p_obj and p_per from theta, gamma, and delta
    data.frame(theta = thetas) %&amp;gt;% 
        mutate(delta = delta,
               gamma = gamma, 
               p_obj = pnorm(theta),
               p_per = per_dist(pnorm(theta), gamma, delta))
  }
}

# Plot different combinations of perception distortion and bias
per_dist_dat %&amp;gt;%
  mutate(gamma = factor(gamma, 
                        levels = c(2, 1, .3),
                        labels = c(2, 1, .3))) %&amp;gt;%
  ggplot(aes(p_obj, p_per)) + 
  geom_line(color = I(&amp;quot;#8F2727&amp;quot;), size = 1.5) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  facet_grid(delta ~ gamma, labeller = labeller(.rows = label_both, 
                                               .cols = label_both)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks familiar, doesn’t it? We get the same basic pattern using this perception distortion model as we do with the noise + bias model. In many ways, it appears that these models may be statistically indistinguishable without a very specific experimental design. Alternatively, one could make the arguement that the noise + bias model is more theoretically informative, as it provides a plausible psychological explanation for how perception distortion may arise, whereas the perception distortion model itself does not have an obvious psychological interpretation . Still, it is useful to show the correspondence between these models, because this perception distortion function is widely used in the decision-making literature. In fact, it makes up a key component of &lt;a href=&#34;https://doi.org/10.1007/BF00122574&#34;&gt;cumulative prospect theory&lt;/a&gt;, wherein it is used to model how people assign subjective weights to objective probabilities, thereby producing patterns of risk preferences that we observe in real data.&lt;/p&gt;
&lt;p&gt;The full binomial perception distortion model can then be written as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i) \\
  p_{i,\text{per}} &amp;amp; = \frac{\delta p_{i,\text{obj}}^{\gamma}}{\delta p_{i,\text{obj}}^{\gamma} + (1-p_{i,\text{obj}})^{\gamma}} \\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}}) \\
  y_{i,\text{per}} &amp;amp; \sim \text{Binomial}(n_{\text{per}}, p_{i,\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-our-models-to-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting Our Models to Data&lt;/h1&gt;
&lt;p&gt;We have almost made it! Thanks for sticking around for this long :D. We are now ready to try fitting our models to real-world data to determine whether our models can adequately account for people’s actual perception judgements. Further, the parameter estimates we obtain will allow us to determine if there is evidence for the traditional interpretation of the Dunning-Kruger effect in our data.&lt;/p&gt;
&lt;p&gt;First, we need to find a dataset. Fortunately, with the recent increase in open science practices in psychology, I was able to locate a perfect dataset for us. The data we will use are from Study 1 of &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1242-7&#34;&gt;Pennycook et al. (2017)&lt;/a&gt;, and can be found at the following &lt;a href=&#34;https://osf.io/3kndg/&#34;&gt;Open Science Foundation Repo&lt;/a&gt; (huge shout-out for making your data openly available!). In this study, participants engaged in a cognitive reflection task with 8 items, where questions were designed to ellicit an “intuitive” answer that was nevertheless incorrect. For example, one item was as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If you’re running a race and you pass the person in second place, what place are you in? (intuitive answer: first; correct answer: second).”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The other items were similar, and overall they were quite fun to think through! Participants answered each of the 8 items, were not given feedback on their answers, and were then asked to give an estimate of the numberof items that they think they got correct. Therefore, our data are formatted such that we have two “counts” for each participant, one reflecting the number of items they answered correctly, and the other reflecting the perceived number of items they answered correctly. In both cases, there are 8 items in total, meaning that &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{obj}} = n_{\text{per}} = 8\)&lt;/span&gt;. This experiment is perfect for the binomial models that we developed!&lt;/p&gt;
&lt;p&gt;The code below downloads the data from the OSF repo into R, and then makes a scatterplot of the objective number of correct items versus perceived number of correct itmes across participants:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download data first
osf_url &amp;lt;- &amp;quot;https://osf.io/3kndg/download&amp;quot; # url for Study 1 csv file
filename &amp;lt;- &amp;#39;pennycook_2017.csv&amp;#39;
GET(osf_url, write_disk(filename, overwrite = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://files.osf.io/v1/resources/p8xjs/providers/osfstorage/589ddf839ad5a1020acb69c8?action=download&amp;amp;direct&amp;amp;version=1]
##   Date: 2022-01-26 04:07
##   Status: 200
##   Content-Type: text/csv
##   Size: 88.2 kB
## &amp;lt;ON DISK&amp;gt;  /Users/nathanielhaines/Dropbox/Building/my_website/haines-lab_dev/content/post/2021-01-10-modeling-classic-effects-dunning-kruger/pennycook_2017.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;osf_dat &amp;lt;- read.csv(filename, header=TRUE)

# Actual versus perceived number correct
obs_scatter &amp;lt;- qplot(osf_dat$CRT_sum, osf_dat$Estimate, 
      color = I(&amp;quot;#8F2727&amp;quot;), size = 1, alpha = .01) +
  ggtitle(paste0(&amp;quot;N = &amp;quot;, nrow(osf_dat), 
                 &amp;quot;; r = &amp;quot;, round(cor(osf_dat$CRT_sum, 
                                     osf_dat$Estimate), 2))) +
  xlab(&amp;quot;Objective Number Correct&amp;quot;) +
  ylab(&amp;quot;Perceived Number Correct&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;, 
        plot.title = element_text(hjust=.5))
obs_scatter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the scatterplot alone, it looks like there is not much going on–there is a modest positive correlation between the objective and perceived number of correct items, but how should we interpret this? We know from our initial analyses that such a low correlation should produce the Dunning-Kruger effect when plotted using the traditional quantile visualization by way of regression to the mean, although regression to the mean alone should not produce any systematic biases (i.e. over- or under-confidence). Let’s see what the quantile plot looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make quantile plot with observed data
data.frame(obs_obj = osf_dat$CRT_sum,
           obs_per = osf_dat$Estimate) %&amp;gt;%
  mutate(quantile = cut_number(obs_obj, n = 4, labels = F)) %&amp;gt;%
  group_by(quantile) %&amp;gt;%
  summarize(mu_obj = mean(obs_obj),
            mu_per = mean(obs_per)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 1) +
  geom_point(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 1) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 1.5, y = 1, 
             color = I(&amp;quot;black&amp;quot;), size = 5) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.5, y = 4, 
             color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
  ylim(0,8) +
  xlab(&amp;quot;Quantile&amp;quot;) +
  ylab(&amp;quot;Average Sum Within Quantile&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-10-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, we see the general Dunning-Kruger pattern, where participants with low objective scores over-estimate their scores, and those with high objective scores under-estimate their scores, but to a lesser extent. Knowing that the effect is there in the data, it is time to fit our models to see what insights they reveal!&lt;/p&gt;
&lt;div id=&#34;fitting-the-binomial-noise-bias-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the Binomial Noise + Bias Model&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; code below implements the binomial noise + bias model:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower=1&amp;gt; N;     // Number of participants
  int&amp;lt;lower=1&amp;gt; n_obj; // Number of items on objective measure
  int&amp;lt;lower=1&amp;gt; n_per; // Number of items on perception measure
  int y_obj[N];       // Objective number of items correct
  int y_per[N];       // Perceived number of items correct
}

parameters {
  vector[N] theta;     // Underlying skill
  real&amp;lt;lower=0&amp;gt; sigma; // Perception noise 
  real b;              // Bias
}

transformed parameters {
  vector[N] p_obj;
  vector[N] p_per;

  // Probit transforming our parameters
  for (i in 1:N) {
    p_obj[i] = Phi_approx(theta[i]);
    p_per[i] = Phi_approx((theta[i]+b)/sigma);
  }
}

model {
  // Prior distributions
  theta ~ normal(0,1);
  sigma ~ lognormal(0,1);
  b     ~ normal(0,1);
  
  // Likelihood for both objective and perceived number of correct items
  y_obj ~ binomial(n_obj, p_obj);
  y_per ~ binomial(n_per, p_per);
}

generated quantities {
  int y_obj_pred[N];
  int y_per_pred[N];
  
  // Generate posterior predictions to compare against observed data
  y_obj_pred = binomial_rng(n_obj, p_obj);
  y_per_pred = binomial_rng(n_per, p_per);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have written the code above to be as close to the formal definitions described throughout this post as possible, in an effort tomake the Stan code easier to understand. Overall, the model is rather simple–the most complex part is the noise component, but hopefully the visualizations we made above make the meaning clear. Next, we just need to format our data and fit the model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note that you will need to compile the model first. Mine is
# already compiled within the Rmarkdown file used to make the post:
# binomial_noise_bias &amp;lt;- stan_model(&amp;quot;path_to_model/file_name.stan&amp;quot;)

# Format data for stan
stan_dat &amp;lt;- list(N = nrow(osf_dat),
                 n_obj = 8,
                 n_per = 8,
                 y_obj = osf_dat$CRT_sum,
                 y_per = osf_dat$Estimate)

# Fit the model!
fit_noise_bias &amp;lt;- sampling(binomial_noise_bias, 
                           data   = stan_dat, 
                           iter   = 1000, # 1000 MCMC samples
                           warmup = 300,  # 300 used for warm-up
                           chains = 3,    # 3 MCMC chains
                           cores  = 3,    # parallel over 3 cores
                           seed   = 43210)

# Once finished, check convergence using traceplots 
traceplot(fit_noise_bias)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;
Simple as that! The model runs very quickly, and we can see from the traceplots above that the model seems to have converged well (the traceplots should look like “furry caterpillars”). We would normally check traceplots for more parameters than just the 10 participant &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters above, but for now, we will just check the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; statistics, which should be close to 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot R-hat statistics
stan_rhat(fit_noise_bias)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-13-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They look great! Given that these diagnostics look good, and that our model is relatively simple, I am confident that the model has converged, and we can now look at our parameter estimates.&lt;/p&gt;
&lt;p&gt;Remember, the binomial noise + bias model produces the Dunning-Kruger effect when both &lt;span class=&#34;math inline&#34;&gt;\(\sigma &amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;, so we are looking for evidence that our parameters meet these criteria. For our purposes, we can just visually check the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract posterior samples from model fit object
pars_noise_bias &amp;lt;- rstan::extract(fit_noise_bias)

# Prior versus posterior distribution for sigma
post_sigma &amp;lt;- ggplot() +
  geom_vline(xintercept = 1, linetype = 2) + 
  geom_density(aes(x = pars_noise_bias$sigma), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dlnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(sigma)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(0,5) +
  ylim(0,1.75) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(size = 15))
post_b &amp;lt;- ggplot() +
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_density(aes(x = pars_noise_bias$b), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(b)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(-3,3) +
  ylim(0,1.75) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 15))
# Plot together
post_sigma | post_b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-14-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What you are seeing above is the posterior distribution for both parameters (in dard red), compared to each parameter’s prior distribution (dotted gray line). Further, the black dotted lines indicate &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt; (note that I chose priors that placed 50% of the prior mass over and under the “critical” values of &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt;). Importantly, there is very strong evidence that both &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;–indeed, the entire posterior distribution for each parameter is above the critical value, suggesting strong evidence of perception noise and bias, which together produce the classic Dunning-Kruger effect.&lt;/p&gt;
&lt;p&gt;Of course, before we get too excited, we should confirm that the model can reproduce theoretically relevant patterns in the observed data. First, we can check how well the model can predict both the objective and perceived number of items correct for each participant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set the color scheme for maximum vibes
color_scheme_set(&amp;quot;red&amp;quot;)

# We will order the plots by the objective number correct
ord &amp;lt;- order(stan_dat$y_obj)

# Posterior predictions for objective number correct
pp_obj &amp;lt;- ppc_intervals(y = stan_dat$y_obj[ord],
                        yrep = pars_noise_bias$y_obj_pred[,ord],
                        prob = .5, 
                        prob_outer = .95) +
  xlab(&amp;quot;Participant&amp;quot;) +
  ylab(&amp;quot;Objective Number Correct&amp;quot;) +
  ylim(0,8) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)

# Posterior predictions for perceived number correct
pp_per &amp;lt;- ppc_intervals(y = stan_dat$y_per[ord],
                        yrep = pars_noise_bias$y_per_pred[,ord],
                        prob = .5, 
                        prob_outer = .95) +
  xlab(&amp;quot;Participant&amp;quot;) +
  ylab(&amp;quot;Perceived Number Correct&amp;quot;) +
  ylim(0,8) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank())

# Plot together
pp_obj | pp_per&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-15-1.svg&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the dark red points indicate the observed sums, and the lighter red points and intervals indicate the posterior model predictions and 50% and 95% highest density intervals, respectively (i.e. the darker intervals are 50% HDIs, and lighter intervals 95% HDIs). We want to ensure that our uncertainty intervals are not biased in any systematic way, and that the observed data are reasonably well described by the model predictions. In our case, it is pretty clear that the objective number correct is captured quite well, although there is still some uncertainty due to the fact that we only have 8 items to work with. Conversely, there is much more uncertainty for the perceived number of correct, although the model does seem to have reasonably well callibrated uncertainty intervals. For example, across participants, it is pretty clear that at least 95% of the observed points are contained within the 95% HDIs. Although, 1 participant in particular is apparently VERY underconfident… :’(. Overall, the model performs as we should expect it to–with the additional perception noise component, the predictions should naturally be more uncertain relative to the objective number correct results.&lt;/p&gt;
&lt;p&gt;We should also check to see if the model can re-produce the main effect of interest–the Dunning-Kruger effect using a traditional quantile plot! The code below draws 50 samples from the posterior distribution, each time summarizing the data and plotting according to a traditional Dunning-Kruger style plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Pick 50 random draws from our 2100 total posterior samples
samps &amp;lt;- sample(1:2100, size = 50, replace = F)

# Posterior predictions, summarized with quantile plot
foreach(i=samps, .combine = &amp;quot;rbind&amp;quot;) %do% {
  data.frame(obj = pars_noise_bias$y_obj_pred[i,],
             per = pars_noise_bias$y_per_pred[i,]) %&amp;gt;%
    mutate(quantile = cut_number(obj, n = 4, labels = F)) %&amp;gt;%
    pivot_longer(cols = c(&amp;quot;obj&amp;quot;, &amp;quot;per&amp;quot;), names_to = &amp;quot;type&amp;quot;) %&amp;gt;%
    group_by(type, quantile) %&amp;gt;%
    summarize(mu = mean(value)) %&amp;gt;%
    mutate(iter = i)
} %&amp;gt;% 
  ggplot(aes(x = quantile, y = mu, color = type, 
             group = interaction(type, iter))) +
  geom_line(alpha = .1, size = 1) +
  geom_point(alpha = .1, size = 2) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, &amp;quot;#8F2727&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 1.5, y = 1, 
             color = I(&amp;quot;black&amp;quot;), size = 5) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.5, y = 4, 
             color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
  ylim(0,8) +
  xlab(&amp;quot;Quantile&amp;quot;) +
  ylab(&amp;quot;Average Sum Within Quantile&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-16-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see here that the model indeed captures the traditional Dunning-Kruger effect, although now we have the added insight of being able to interpret the effect through the lens of the model. It is also worth noting that, by plotting samples from the posterior, we get an idea of what the variability in the data look like, and by extention we know what to expect if we were to run the same experiment again.&lt;/p&gt;
&lt;p&gt;Finally, we can reporoduce the scatterplot we made of the original data to see if the correlation looks as expected (this time, using only one sample from the posterior for clarity):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Draw random sample from posterior
samp &amp;lt;- sample(1:2100, size = 1, replace = F)

# Plotting 
pred_scatter &amp;lt;- qplot(pars_noise_bias$y_obj_pred[samp,], 
                      pars_noise_bias$y_per_pred[samp,], 
      color = I(&amp;quot;#8F2727&amp;quot;), size = 1, alpha = .01) +
  ggtitle(paste0(&amp;quot;r = &amp;quot;, 
                 round(cor(pars_noise_bias$y_obj_pred[samp,],
                           pars_noise_bias$y_per_pred[samp,]), 2))) +
  xlab(&amp;quot;Objective Number Correct&amp;quot;) +
  ylab(&amp;quot;Perceived Number Correct&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;, 
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(hjust=.5))

# Plot along with observed data to compare
obs_scatter | pred_scatter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-17-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks pretty good! You can get a better sense of the expected variability by taking different samples from the posterior and plotting those. You will find that the correlation will vary a decent amount, which is in part due to the uncertainty in the underlying parameters, but also partly due to the random nature of the binomial generative model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-binomial-perception-distortion-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting the Binomial Perception Distortion Model&lt;/h1&gt;
&lt;p&gt;We can now turn our attention toward the binomial perception distortion model to see if the results come back similar. We will not go into as much detail in this section, but we will at least fit the model and take a look at the parameters to see if they come out in the expected directions. The Stan code for our next model is below:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions {
  // Perception distortion function
  real per_dist(real p_obj, real gamma, real delta) {
    real p_per;
    if (p_obj == 0) {
      p_per = 0;
    } else {
      p_per = delta*pow(p_obj, gamma) / (delta*pow(p_obj, gamma) + pow((1-p_obj), gamma));
    }
    return p_per;
  }
}

data {
  int&amp;lt;lower=1&amp;gt; N;     // Number of participants
  int&amp;lt;lower=1&amp;gt; n_obj; // Number of items on objective measure
  int&amp;lt;lower=1&amp;gt; n_per; // Number of items on perception measure
  int y_obj[N];       // Objective number of items correct
  int y_per[N];       // Perceived number of items correct
}

parameters {
  vector[N] theta;     // Underlying skill
  real&amp;lt;lower=0&amp;gt; gamma; // Perception distortion 
  real&amp;lt;lower=0&amp;gt; delta; // Bias
}

transformed parameters {
  vector[N] p_obj;
  vector[N] p_per;

  // Probit transforming our parameters
  for (i in 1:N) {
    p_obj[i] = Phi_approx(theta[i]);
    p_per[i] = per_dist(p_obj[i], gamma, delta);
  }
}

model {
  // Prior distributions
  theta ~ normal(0,1);
  gamma ~ lognormal(0,1);
  delta ~ lognormal(0,1);
  
  // Likelihood for both objective and perceived number of correct items
  y_obj ~ binomial(n_obj, p_obj);
  y_per ~ binomial(n_per, p_per);
}

generated quantities {
  int y_obj_pred[N];
  int y_per_pred[N];
  
  // Generate posterior predictions to compare against observed data
  y_obj_pred = binomial_rng(n_obj, p_obj);
  y_per_pred = binomial_rng(n_per, p_per);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code is very similar to before, but we have added the perception distortion function, which we will use instead of the noise and bias terms from the previous model. Time fit the model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note that you will need to compile the model first. Mine is
# already compiled within the Rmarkdown file used to make the post:
# binomial_per_dist &amp;lt;- stan_model(&amp;quot;path_to_model/file_name.stan&amp;quot;)

# Fit the model!
fit_per_dist &amp;lt;- sampling(binomial_per_dist, 
                         data   = stan_dat, 
                         iter   = 1000, # 1000 MCMC samples
                         warmup = 300,  # 300 used for warm-up
                         chains = 3,    # 3 MCMC chains
                         cores  = 3,    # parallel over 3 cores
                         seed   = 43210)

# Once finished, check convergence using traceplots 
traceplot(fit_per_dist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-19-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Convergence looks good! We will skip the &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; plot for brevity, but it also came back looking good.&lt;/p&gt;
&lt;p&gt;Next, let’s plot the parameters! Remember, for this model, a pattern where &lt;span class=&#34;math inline&#34;&gt;\(\gamma&amp;lt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;gt;1\)&lt;/span&gt; produces a Dunning-Kruger effect, so we are looking to see of the posterior distributions meet these criteria:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract posterior samples from model fit object
pars_per_dist &amp;lt;- rstan::extract(fit_per_dist)

# Prior versus posterior distribution for gamma
post_gamma &amp;lt;- ggplot() +
  geom_vline(xintercept = 1, linetype = 2) + 
  geom_density(aes(x = pars_per_dist$gamma), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dlnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(gamma)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(0,4) +
  ylim(0,7.0) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(size = 15))
# For delta
post_delta &amp;lt;- ggplot() +
  geom_vline(xintercept = 1, linetype = 2) + 
  geom_density(aes(x = pars_per_dist$delta), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dlnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(delta)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(0,4) +
  ylim(0,7.0) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 15))
# Plot together
post_gamma | post_delta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-20-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once again, pretty clear evidence for the Dunning-Kruger effect! And once more, the parameters have a psychological interpretation that directly corresponds to the hypothesis underlying the Dunning-Kruger effect, which makes us more confident that the effect is indeed psychological, rather than an artefact of an arbitrary statistical modeling decision.&lt;/p&gt;
&lt;div id=&#34;a-psychological-dunning-kruger-effect-is-present-in-both-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Psychological Dunning-Kruger Effect is Present in Both Models&lt;/h2&gt;
&lt;p&gt;For one final plot, it is insightful to show what the estimated parameters for each model imply regarding the relationship beetween &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt;, which directly correspond to participants’ objective skill and perceived skill, respectively. This will be similar to previous plots that we made, but conditional on the observed data. Here we go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot different combinations of perception distortion and bias
p_noise_bias &amp;lt;- qplot(x = colMeans(pars_noise_bias$p_obj),
                      y = colMeans(pars_noise_bias$p_per),
                      color = I(&amp;quot;#8F2727&amp;quot;), alpha = .1, size = 1) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  ggtitle(&amp;quot;Noise + Bias&amp;quot;) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust=.5),
        legend.position = &amp;quot;none&amp;quot;)
# Same, for other model
p_per_dist &amp;lt;- qplot(x = colMeans(pars_per_dist$p_obj),
                    y = colMeans(pars_per_dist$p_per),
                    color = I(&amp;quot;#8F2727&amp;quot;), alpha = .1, size = 1) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  ggtitle(&amp;quot;Perception Distortion&amp;quot;) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust=.5),
        legend.position = &amp;quot;none&amp;quot;)
# Plot together
p_noise_bias | p_per_dist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-21-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What you are seeing above are the posterior expectations (i.e. averages across the posterior distribution) for the &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt; terms for each of the participants in the dataset, and for both models separately. As noted before, the models are practically indistinguishable! More importantly, though, is that this is a “psychological space” of sorts, where we can interpret these patterns as psychological effects. That said, the Dunning-Kruger effect is clear as day here!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wait-the-dunning-kruger-effect-is-real-after-all&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wait, the Dunning-Kruger Effect is Real After All?&lt;/h1&gt;
&lt;p&gt;Our results suggest that the Dunning-Kruger effect is indeed a real psychological phenomenon. Using two diferrent generative models with psychologically interpretable parameters, we found not only strong evidence for the effect, but revealed potential mechanisms through which the effect may arise. The first is through the concept of “perception noise”, as shown with our noise + bias model. The second is through a general “perception distortion” mechanism, captured in the perception distortion model (however, this distortion may itself be attributable to noise, so one could argue that we have identified a single mechanism). When either of these mechanisms is combined with a general over-estimation bias, the Dunning-Kruger effect appears in observed data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The Dunning-Kruger effect is saved!… perhaps… This was quite an interesting journey for me. It revealed to me, once again, just how much great work exists in the fields of mathematical psychology and cognitive science, which I took inspiration from when developing the models presented here. Moreover, it shows how powerful generative modeling can be, even for problems that may traditionally be viewed as outside the scope of mathematical modeling.&lt;/p&gt;
&lt;p&gt;Regarding the models we developed, there are some clear limitations, which I think future work could readily address. The most obvious extention, to me, would be to collect enough data from each participant to estimate a noise (or perception distortion) and bias parameters for each participant. Right now, our model assumes that these parameters are constant across participants, but this is likely untrue. A study that sampled each participant across the whole range of &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt; would be perfectly suited for this. Another interesting extention would be to test the models for &lt;a href=&#34;https://doi.org/10.1037/a0018435&#34;&gt;selective parameter influence&lt;/a&gt;, to determine if manipulating uncertainty/noise really does influence behavior in ways predicted by the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.0 (2021-05-18)
## Platform: aarch64-apple-darwin20 (64-bit)
## Running under: macOS 12.0.1
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] httr_1.4.2           hBayesDM_1.1.1       Rcpp_1.0.8          
##  [4] bayesplot_1.8.1      rstan_2.21.2         StanHeaders_2.21.0-7
##  [7] foreach_1.5.1        patchwork_1.1.1      gghighlight_0.3.2   
## [10] ggplot2_3.3.5        tidyr_1.1.3          dplyr_1.0.7         
## [13] mvtnorm_1.1-2       
## 
## loaded via a namespace (and not attached):
##  [1] prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.29     
##  [5] utf8_1.2.2         V8_3.4.2           R6_2.5.1           plyr_1.8.6        
##  [9] ggridges_0.5.3     stats4_4.1.0       evaluate_0.14      highr_0.9         
## [13] blogdown_1.7.3     pillar_1.6.2       rlang_0.4.12       curl_4.3.2        
## [17] rstudioapi_0.13    data.table_1.14.0  callr_3.7.0        rmarkdown_2.11    
## [21] labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0     
## [25] compiler_4.1.0     xfun_0.29          pkgconfig_2.0.3    pkgbuild_1.2.0    
## [29] htmltools_0.5.2    tidyselect_1.1.1   tibble_3.1.4       gridExtra_2.3     
## [33] bookdown_0.24      codetools_0.2-18   matrixStats_0.60.1 fansi_0.5.0       
## [37] crayon_1.4.1       withr_2.4.2        grid_4.1.0         jsonlite_1.7.3    
## [41] gtable_0.3.0       lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1    
## [45] formatR_1.11       scales_1.1.1       RcppParallel_5.1.4 cli_3.0.1         
## [49] stringi_1.7.6      reshape2_1.4.4     farver_2.1.0       ellipsis_0.3.2    
## [53] generics_0.1.0     vctrs_0.3.8        iterators_1.0.13   tools_4.1.0       
## [57] glue_1.6.0         purrr_0.3.4        processx_3.5.2     parallel_4.1.0    
## [61] fastmap_1.1.0      yaml_2.2.1         inline_0.3.19      colorspace_2.0-2  
## [65] knitr_1.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On Curbing Your Measurement Error: From Classical Corrections to Generative Models</title>
      <link>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post, we will explore how measurement error arising from imprecise parameter estimation can be corrected for. Specifically, we will explore the case where our goal is to estimate the correlation between a self-report and behavioral measure–a common situation throughout the social and behavioral sciences.&lt;/p&gt;
&lt;p&gt;For example, as someone who studies impulsivity and externalizing psychopathology, I am often interested in whether self-reports of trait impulsivity (e.g., the &lt;a href=&#34;http://www.impulsivity.org/measurement/bis11&#34;&gt;Barratt Impulsiveness Scale&lt;/a&gt;) correlate with performance on tasks designed to measure impulsive behavior (e.g., the &lt;a href=&#34;http://www.impulsivity.org/measurement/BART&#34;&gt;Balloon Analogue Risk task&lt;/a&gt;). In these cases, it is common for researchers to compute summed or averaged scores on the self-report measure (e.g., summing item responses and divding by the number of items) and use summary statistics of behavioral performance (e.g., percent risky choices) for each subject’s behavioral measure, wherein the resulting estimates are then entered into a secondary statistical model to make inference (e.g., correlation between self-reported trait and behavioral measures). Importantly, this two-stage approach to inference assumes that our summary measures both contain no measurement error, or alternatively that we have estimated these summary mesaures with perfect precision–a very strong assumption that is surely not met in practice.&lt;/p&gt;
&lt;p&gt;Here, we will explore how such assumptions can bias our statistical inferences on individual differences. As we will show, this bias arises because these two-stage approaches ignore important sources of measurement error. We will begin with an exploration of traditional methods developed within the context of classical test theory, and we will then transition to the use of more contemporary generative models. Throughout, we will explore relationships between classical and generative approaches, which are actually more similar than they are different in many ways.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;classical-corrections&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classical Corrections&lt;/h1&gt;
&lt;div id=&#34;imprecision-and-reliability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imprecision and Reliability&lt;/h2&gt;
&lt;p&gt;At the level of a single measure, we can think of reliability as directly corresponding to precision–or how close our estimates are to the underlying “true score”. As our estimates become more precise at the individual-level, we should be able to better infer differences between individuals.&lt;/p&gt;
&lt;p&gt;For example, assume that we are interested estimating a trait score for an individual, which we measure using a 10 item self-report questionnaire. For simplicity, let’s also assume that each item requires a yes/no endorse/not endorse response (coded 1 and 0, respectively), no items are reverse scored, and all items share the same difficulty. A typical approach to score this questionnaire would be to sum up the individual items &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and divide the sum by the number of items &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; (i.e. taking the average). So, the vector of responses &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}\)&lt;/span&gt; for a single subject may look something like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\textbf{x} = [1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We then compute our observed score like so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = \frac{1}{T}\sum^{T}_{t=1}\textbf{x}_{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The resulting observed score &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is simply the proportion of yes/endorsed responses, which is &lt;span class=&#34;math inline&#34;&gt;\(X = .7\)&lt;/span&gt; in this example. We then interpret &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as a quantitative measure of the underlying contruct. However, there are a few important, related questions worth asking regarding this estimate:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How close is our observed score measure &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to the “true score” we aim to measure?&lt;/li&gt;
&lt;li&gt;Can we use this measurement approach to make inference on individual differences?&lt;/li&gt;
&lt;li&gt;If so, how can we best estimate the “true score” for each individual?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the perspective of classical test theory, we can answer these questions by determining the reliability of our measure. Below, we will define reliability and discuss how it can be used to answer the questions above.&lt;/p&gt;
&lt;div id=&#34;defining-reliability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Defining Reliability&lt;/h3&gt;
&lt;p&gt;In psychology and education, reliability is often discussed within the context of classical test theory, which assumes that our estimates reflect some combination of the unobserved “true score” plus some error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = \theta + \epsilon\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is measurement error that contaminates the “true score” &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Note that there are different interpretations of what the truth actually is, but here we will use the following definition: &lt;strong&gt;the true score is the expected value of the observed score over many independent realizations&lt;/strong&gt; (for alternative definitions of the true score, see &lt;a href=&#34;https://psycnet.apa.org/record/1968-35040-000&#34;&gt;Lord, Novick, &amp;amp; Birnbaum, 1968&lt;/a&gt;). Mathematically, we can represent this as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta = \mathbb{E}[X]\]&lt;/span&gt;
Following this definition, the error for a single realization is then defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\epsilon = X - \mathbb{E}[X]\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\epsilon] = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\epsilon, \theta) = 0\)&lt;/span&gt;. In english, the expected error is 0, and the correlation/covariance between the error and true score is 0.&lt;/p&gt;
&lt;p&gt;Given these assumptions, reliability is then defined as the squared correlation between the true score and observed score, or &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt;. Inuitively, &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta} \rightarrow 1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \theta\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; is attenuated to the extent that &lt;span class=&#34;math inline&#34;&gt;\(X \ne \theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As a consequence of the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\epsilon, \theta) = 0\)&lt;/span&gt;, the observed, true, and error variances have the following relationship:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_X = \sigma^{2}_\theta + \sigma^{2}_\epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; can also be thought of as the proportion of variance that the true score accounts for in the observed score (similar to &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; in regression), or alternatively as 1 minus the ratio of error variance relative to observed score variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho^{2}_{X, \theta} = \frac{\sigma^{2}_\theta}{\sigma^{2}_{X}} = 1 - \frac{\sigma^{2}_{\epsilon}}{\sigma^{2}_{X}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now have a few different ways to think about reliability. Now, it is time to estimate it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-reliability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating Reliability&lt;/h3&gt;
&lt;p&gt;To estimate &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt;, we need to somehow estimate either the true score or error variance, neither of which are directly observed. How do we do this? The answer is actually quite nuanced, and it all comes down to how we want to conceptualize the error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we are simply interested in how precisely we can estimate the true score from observed data, it may be best to think of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt; as the uncertainty in our estimate across administrations of an identical mesaure (termed &lt;strong&gt;precision&lt;/strong&gt; or &lt;strong&gt;parallel forms reliability&lt;/strong&gt;). If we are interested in whether or not multiple items on a scale capture the same underlying construct, we may use something like Chronbach’s &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (termed &lt;strong&gt;internal consistency&lt;/strong&gt;). If we are interested in consistency of a measure over time, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt; may best be thought of as uncommon variance across different administrations of the same measure over an arbitrary period of time (termed &lt;strong&gt;termporal consistency&lt;/strong&gt; or &lt;strong&gt;test-retest reliability&lt;/strong&gt;). Crucially, the equations above underlie all these different forms of reliability–the big difference being how we conceptualize and compute &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given our focus here on precision, the current post will explore the first form of reliability mentioned above–parallel forms reliability. Typically, parallel forms reliability is estimated by having a group people take two &lt;em&gt;identical versions&lt;/em&gt; of the same measure and then estimating the correlation between measures across individuals. Here, &lt;em&gt;identical&lt;/em&gt; means that the measures tap into the same underlying trait/construct (i.e. the underlying true score is the same), share the same item characteristics, and have the same error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt;. Under these assumptions, the correlation between the observed scores across measure 1 and 2 (&lt;span class=&#34;math inline&#34;&gt;\(r_{X,X&amp;#39;}\)&lt;/span&gt;) is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{X,X&amp;#39;} = \frac{\sigma^{2}_\theta}{\sigma^{2}_{X}} = \rho^{2}_{X, \theta}\]&lt;/span&gt;
This relationship allows us to think of reliability in two different ways: (1) as the correlation between observed scores of identical measures, or (2) as the ratio of true score variance to observed score variance.&lt;/p&gt;
&lt;p&gt;However, what if we do not have parallel forms? Obviously, having two identical measures is infeasible in many settings. Assuming that there are no practice effects, we could always have our subjects re-take the measure and then estimate &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; as the correlation between their scores across administrations, although this would still demand more time on the part of participants.&lt;/p&gt;
&lt;div id=&#34;reliability-as-measurement-precision&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reliability as Measurement Precision&lt;/h4&gt;
&lt;p&gt;One of the key of framing reliability in terms of precision is that we can estimate &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; from a single administration of our measure. In this section, we will employ this method and compare it to the parallel forms/test-retest method where reliability is characterized as the correlation between observed scores of identical measures.&lt;/p&gt;
&lt;p&gt;Using precision to estimate the error variance of our measure is actually pretty straightforward. Returning to the example in the introduction, say we have a vector of yes/no responses for a single indivdual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; who responded to 10 items: &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_i = [1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\)&lt;/span&gt;. As before, if we take the mean of this vector as the observed score, we have &lt;span class=&#34;math inline&#34;&gt;\(X_i = \frac{1}{T}\sum^{T}_{t=1}\textbf{x}_{i,t} = .7\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because we have binary observed data, we can view each response as a bernoulli trial, allowing us to use the bernoulli distribution to estimate the error variance and &lt;a href=&#34;https://stats.stackexchange.com/questions/29641/standard-error-for-the-mean-of-a-sample-of-binomial-random-variables&#34;&gt;standard error&lt;/a&gt;. According to the wisdom of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34;&gt;Wikipedia&lt;/a&gt;, the mean of a bernoulli distribution is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the success probability (i.e. probability of observing 1 rather than 0). In our case, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is estimated by the observed mean &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Again taking from Wikipedia, the variance of a sum of bernoulli trials is given by &lt;span class=&#34;math inline&#34;&gt;\(pq\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is as defined above and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the failure probability, &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;. We can then compute the standard error of measurement for this individual &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\epsilon,i}\)&lt;/span&gt; (which we can square to get &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{\epsilon,i}\)&lt;/span&gt;) by dividing the variance by &lt;span class=&#34;math inline&#34;&gt;\(T-1\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the number of items:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_{\epsilon,i} = \frac{pq}{T-1} = \frac{.7 \times (1-.7)}{10-1} = 0.021\]&lt;/span&gt;
The standard error of measurement corresponds to the strandard deviation of the &lt;em&gt;sampling distribution&lt;/em&gt; of our observed score. The standard error of measurement thus gives us useful information about how close our observed score is to the true score. In particular, as we increase the number of items on our measure, the sampling distribution of the mean becomes more concentrated. In turn, on average, our observed score becomes closer to the true score.&lt;/p&gt;
&lt;p&gt;If these ideas are unfamiliar to you, it is useful to visualize what happens to the sampling distribution as we increase the number of items on the questionnaire. The R code below simulates the sampling distribution for each of a set of item sizes ranging from 10 to 100, using a success probability of .7:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First load some packages we will use
library(dplyr)
library(foreach)
library(ggplot2)
library(ggridges)
library(truncnorm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# num of items and success probability
n_items    &amp;lt;- seq(10, 100, by = 5)
success_pr &amp;lt;- .7

# number of samples from sampling distribution
n_reps &amp;lt;- 10000

# Generate binomial sampling distribution
results &amp;lt;- foreach(i=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
  data.frame(T = rep(n_items[i], n_reps),
             X = replicate(n_reps, 
                           mean(rbinom(n_items[i], 1, prob = success_pr))))
}

# Compute standard deviation of the sampling distribution 
# (aka the standard error of the mean)
sem &amp;lt;- results %&amp;gt;%
  mutate(T = factor(T, levels = rev(unique(sort(T))))) %&amp;gt;%
  group_by(T) %&amp;gt;%
  summarize(se_lo = mean(X) - sd(X), # mean - 1 SEM
            se_hi = mean(X) + sd(X)) # mean + 1 SEM

# Plot some cool ridges
results %&amp;gt;% 
  mutate(T = factor(T, levels = rev(unique(sort(T))))) %&amp;gt;%
  ggplot(aes(x = X, y = T, group = T)) +
  geom_density_ridges(aes(fill = I(&amp;quot;#DCBCBC&amp;quot;), color = I(&amp;quot;white&amp;quot;)), 
                      stat = &amp;quot;binline&amp;quot;, binwidth = .01, scale = 3) +
  geom_segment(aes(x = se_lo, xend = se_lo, y = as.numeric(T),
                   yend = as.numeric(T) + 1),
               data = sem, size = 1) +
  geom_segment(aes(x = se_hi, xend = se_hi, y = as.numeric(T),
                   yend = as.numeric(T) + 1),
               data = sem, size = 1) +
  coord_cartesian(xlim = c(.3, 1)) +
  ggtitle(&amp;quot;The Bernoulli Sampling Distribution&amp;quot;) +
  xlab(&amp;quot;Sample Mean (X)&amp;quot;) +
  ylab(&amp;quot;# Items (T)&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the black bars on either side of each distribution represent the mean &lt;span class=&#34;math inline&#34;&gt;\(\pm ~1\)&lt;/span&gt; standard deviation of the sampling distribution. As you can see, with an increasing number of items, the sampling distribution becomes more concentrated and more normal (i.e. Gaussian). For us, this means that the range of possible observed scores decreases as we use more items–therefore, an obsvered score computed with a measure containing many items is probably closer to the true score than a measure with a low number of items. In other words, our uncertainty becomes lower, and our estimate more precise.&lt;/p&gt;
&lt;p&gt;Importantly, the standard error of measurement has a direct relationship to the reliability of our measure. As described by &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1955.tb00054.x&#34;&gt;Lord (1955)&lt;/a&gt; (see also &lt;a href=&#34;https://doi.org/10.1177/001316445701700407&#34;&gt;Lord, 1957&lt;/a&gt;), if we calculate &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{\epsilon,i}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; individuals, the average of the resulting squared standard errors of measurement corresponds to the error variance we use to estimate reliability:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_\epsilon = \frac{1}{N}\sum_{i=1}^{N}\sigma^{2}_{\epsilon,i}\]&lt;/span&gt;
Then, if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the vector of observed scores for each individual, the total or observed variance is simply:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_X = \text{Var}(X)\]&lt;/span&gt;
Remember, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; here is a vector of observed scores across all individuals. Reliability is then computed using the same formula as always:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho^{2}_{X,\theta} = 1 - \frac{\sigma^{2}_\epsilon}{\sigma^{2}_X}\]&lt;/span&gt;
The implication here is that we can actually use what we know about the standard error/precision of our estimates for each individual to estimate reliability pretty easily from just a single measure. In fact, assuming that the underlying true score is equivalent across measures/timepoints, this approach is equivalent to the parallel forms or test-retest reliability estimate that we discussed above. We can check this correspondence with a quick simulation, wherein we can look at the reliability estimates for each approach as a function of the number of items:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(43202)

# Number of subjects and items
n_subjs &amp;lt;- 30
n_items &amp;lt;- seq(10, 500, length.out = 30)

# Random sample of &amp;quot;true&amp;quot; scores around success prob. p = .7
theta &amp;lt;- rnorm(n_subjs, .7, .1)

# Estimate standard error of measurement (squared)
est_se2 &amp;lt;- function(x) {
  # Success and failure probability
  n &amp;lt;- length(x)
  p &amp;lt;- mean(x)
  q &amp;lt;- 1 - p

  sig2_ep_i &amp;lt;- (p*q)/(n-1)

  return(sig2_ep_i)
}

# Estimate observed and true score
rel_dat &amp;lt;- foreach(t=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Parallel form 1 (or timepoint 1 administration)
    X_all_f1 &amp;lt;- foreach(i=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
      # Simulate from binomial distribution
      rbinom(n_items[t], 1, prob = theta[i])
    }
    # Parallel form 2 (or timepoint 2 administration)
    X_all_f2 &amp;lt;- foreach(i=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
      # note theta here is equivalent to above (i.e. true scores are the same)
      rbinom(n_items[t], 1, prob = theta[i])  
    }
    
    # Computing X_i (p) for each individual
    X_f1 &amp;lt;- rowMeans(X_all_f1)
    X_f2 &amp;lt;- rowMeans(X_all_f2)
    
    # Standard arror of measurement approach (just using form/timepoint 1)
    sig2_ep &amp;lt;- mean(apply(X_all_f1, 1, est_se2)) 
    sig2_X   &amp;lt;- var(X_f1)
    
    data.frame(n_items  = n_items[t],
               rho2_pf  = cor(X_f1, X_f2), # Parallel form/test-retest approach
               rho2_sem = 1 - (sig2_ep/sig2_X))
}
# ooooooh nice :D
rel_dat %&amp;gt;%
  ggplot() +
  geom_line(aes(x = n_items, y = rho2_pf), color = I(&amp;quot;#DCBCBC&amp;quot;)) +
  geom_line(aes(x = n_items, y = rho2_sem), color = I(&amp;quot;#8F2727&amp;quot;)) +
  ggtitle(&amp;quot;Approaches to estimating reliability&amp;quot;) +
  xlab(&amp;quot;Number of Items&amp;quot;) +
  ylab(bquote(&amp;quot;Reliability (&amp;quot; ~rho[X~~&amp;quot;,&amp;quot;~theta]^2~ &amp;quot;)&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, x = 220, y = 1, label = &amp;quot;Parallel Forms&amp;quot;, 
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 320, y = .8, label = &amp;quot;Standard Error of\nMeasurement&amp;quot;, 
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-3-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is clear from the above figure that both approaches give the same reliability estimates on average, although there is some noise due to the probabilistic nature of the simulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-reliability-to-improve-individual-level-inference&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Using Reliability to Improve Individual-level Inference&lt;/h4&gt;
&lt;p&gt;Importantly, we can use our reliability estimate to improve our inference on individual-level true scores.&lt;/p&gt;
&lt;p&gt;Specifically, Kelley demonstrated–in as early as 1920 (see &lt;a href=&#34;https://psycnet.apa.org/record/1951-05849-000&#34;&gt;Kelley, 1947&lt;/a&gt;)–that we can estimate the true scores for each individual, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_i\)&lt;/span&gt;, by regressing the observed scores on the reliability estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\theta}_i = (1-\rho^{2}_{X,\theta})\bar{X} + \rho^{2}_{X,\theta}X_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the mean of the observed scores across all subjects, given by &lt;span class=&#34;math inline&#34;&gt;\(\bar{X} = \frac{1}{N}\sum_{i=1}^{N}X_i\)&lt;/span&gt;. Intuitively, the true score for each subject is estimated by pooling the their observed score toward the group-level mean score in proportion to the reliability of the individual-level estimate. If &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X,\theta} = 1\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(X_i = \hat{\theta}_i\)&lt;/span&gt; for all individuals–there is no pooling. Conversely, as &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X,\theta} \rightarrow 0\)&lt;/span&gt;, the individual-level observed scores have no weight at all, and the estimated true score is simply the group-level mean for each individual.&lt;/p&gt;
&lt;p&gt;In addition to estimating the true scores as observed scores pooled toward the group mean, Kelley also showed that the standard error of true scores is reduced in the following manner (for a derivation see this &lt;a href=&#34;https://education.uiowa.edu/sites/education.uiowa.edu/files/documents/centers/casma/publications/casma-technical-report-5.pdf&#34;&gt;technical report by Brennan, 2012&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\sigma}_{\epsilon} = \sigma_{X}\sqrt{1-\rho^{2}_{X,\theta}}\sqrt{\rho^{2}_{X,\theta}}\]&lt;/span&gt;
This standard error of the true score estimates is lower than that of the observed scores, which is given by &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}_{\epsilon} = \sigma_{X}\sqrt{1-\rho^{2}_{X,\theta}}\)&lt;/span&gt;. Comparing the equations, we see that the true score standard error incorporates an additional term, &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\rho^{2}_{X,\theta}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These equations are rather interesting for a few reasons. First, they were discovered in 1920 (100 years ago!), long before &lt;a href=&#34;https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator&#34;&gt;James-Stein estimators&lt;/a&gt; (which similarly pool individual-level estimates toward the group-level mean) made their way into statistics (check out &lt;a href=&#34;http://cda.psych.uiuc.edu/web_407_spring_2014/prediction_week5.pdf&#34;&gt;these slides&lt;/a&gt; for some cool historical notes on this).&lt;/p&gt;
&lt;p&gt;Second, they have a Bayesian interpretation! In particular, if we assume that the “true scores” have a normal prior distribution, and that true scores are distributed normally with respect to the observed scores, empirical Bayesian estimation produces posterior means that are equivalent to the Kelley true score point estimates described above (see page 22 of &lt;a href=&#34;https://books.google.com/books?id=dMnoX8YnYgsC&amp;amp;lpg=PA22&amp;amp;ots=reS41nEOre&amp;amp;dq=kelley%201947%20regress&amp;amp;pg=PP11#v=onepage&amp;amp;q=kelley%201947%20regress&amp;amp;f=false&#34;&gt;de Gruijter &amp;amp; van der Kamp, 2008&lt;/a&gt;). It is also worth noting that many popular multilevel/hierarchical modeling software packages work using something akin to empirical Bayesian estimation (e.g., &lt;a href=&#34;https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html&#34;&gt;&lt;code&gt;lmer&lt;/code&gt; in R&lt;/a&gt;). It goes without saying that using group-level information can be exceedingly useful to improve individual-level inference.&lt;/p&gt;
&lt;p&gt;OK, so this has all been a bit abstract. To make things more concrete, we can run a simulation to observe how the true score estimation/pooling described above works. The R code below simulates data from a binomial distribution for 20 “subjects” with success probabilities centered around &lt;span class=&#34;math inline&#34;&gt;\(.7\)&lt;/span&gt;. Additionally, we will use item sizes of 10, 30, and 100 to demonstrate how pooling effects changes as a function of the number of items in a measure (due to the effect of number of items on resulting reliability).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(43202)

# Number of subjects and items
n_subj  &amp;lt;- 20
n_items &amp;lt;- c(10, 30, 100)

# Random sample of &amp;quot;true&amp;quot; scores around .7
theta  &amp;lt;- rnorm(n_subj, .7, .1)

# Estimate observed and true score
dis_dat &amp;lt;- foreach(i=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Generate observed data for each subject using &amp;quot;true&amp;quot; score
  X_all &amp;lt;- foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
   rbinom(n_items[i], 1, prob = theta[t]) 
  }
  
  # group average observed score
  X_bar &amp;lt;- mean(rowMeans(X_all))
  
  # Reliability
  X &amp;lt;- rowMeans(X_all)
  
  # Standard arror of measurement approach
  sig2_ep  &amp;lt;- mean(apply(X_all, 1, est_se2)) 
  sig2_X   &amp;lt;- var(X)
  rho      &amp;lt;- 1 - (sig2_ep/sig2_X)

  foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Using observed scores from parallel form 1
    X_obs &amp;lt;- X_all[t,]
    X_i   &amp;lt;- mean(X_obs)
    
    data.frame(subj_num  = t,
               n_items   = n_items[i],
               theta     = theta[t],
               rho       = rho,
               X         = X_i,
               se_obs    = sd(X)*sqrt(1-rho),
               se_hat    = sd(X)*sqrt(1-rho)*sqrt(rho),
               theta_hat = (1-rho)*X_bar + rho*X_i)
  }
}

# Plot true, observed, and estimated true scores
dis_dat %&amp;gt;%
  mutate(subj_num = reorder(subj_num, theta)) %&amp;gt;%
  ggplot(aes(x = subj_num, y = theta)) + 
  geom_point(color = I(&amp;quot;black&amp;quot;)) +
  geom_point(aes(x = subj_num, y = X),
             color = I(&amp;quot;#DCBCBC&amp;quot;),
             position = position_jitter(width=.2, height=0, seed = 1)) +
  geom_linerange(aes(x = subj_num,
                    ymin = X - 1.96*se_obs,
                    ymax = X + 1.96*se_obs),
                color = I(&amp;quot;#DCBCBC&amp;quot;),
                position = position_jitter(width=.2, height=0, seed = 1)) +
  geom_point(aes(x = subj_num, y = theta_hat), 
             color = I(&amp;quot;#8F2727&amp;quot;),
             position = position_jitter(width=.2, height=0, seed = 2)) +
  geom_linerange(aes(x = subj_num, 
                    ymin = theta_hat - 1.96*se_hat, 
                    ymax = theta_hat + 1.96*se_hat), 
                color = I(&amp;quot;#8F2727&amp;quot;), 
                position = position_jitter(width=.2, height=0, seed = 2)) +
  geom_hline(yintercept = X_bar, linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .4, label = expression(&amp;quot;True&amp;quot;~theta[i]), 
           color = &amp;quot;black&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .3, label = expression(&amp;quot;Obs&amp;quot;~X[i]),
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .2, label = expression(&amp;quot;Est&amp;quot;~hat(theta)[i]),
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  facet_wrap(c(&amp;quot;n_items&amp;quot;), nrow = 1) +
  ggtitle(&amp;quot;Regression-Based True Score Estimates&amp;quot;) +
  xlab(&amp;quot;Subject&amp;quot;) +
  ylab(&amp;quot;Value&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x.bottom = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I don’t know about you, but I think this is pretty cool for a technique developed in 1920 :D. What we see is that the Kelley regression-based point estimates are pooled toward the group-level average (represented by the horizontal gray dotted line) in proportion to the reliability of the measure, which increases as a function of the number of items. Further, the confidence intervals for the estimated true scores &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_i\)&lt;/span&gt; are much narrower than those of the observed scores–yet they still exhibit good coverage properties of the underlying true scores &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-brief-note-on-assumptions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A Brief Note on Assumptions&lt;/h4&gt;
&lt;p&gt;It is worth noting that the models we have discussed so far make many assumptions that might not be met in practice (i.e. normality of the sampling distribution, which is clearly not true when the number of items is low). Much of applied frequentist modeling relies on similar assumptions regarding normality when computing standard errors and p-values. Additionally, our model assumes that responses to each item are random with respect the the underlying true score, which is not true if items are of different difficulties. The need to relax these latter assumptions was the impetus for the development of what is now called &lt;em&gt;Generalizability Theory&lt;/em&gt;, or &lt;em&gt;G-Theory&lt;/em&gt;, which seeks to tease apart these various sources of error.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-reliability-to-disattenuate-a-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using Reliability to Disattenuate a Correlation&lt;/h2&gt;
&lt;p&gt;So, it is now clear that correcting for low reliability can improve inference on individual-level true scores. However, how do these ideas then translate to measuring the correlation between true scores of two different measures? When we are interested in such quantities, we need to take into account measurement error–else we obtain an attenuted, or biased, correlation estimate, and we may falsely infer that a relationship does not exist when it does at the level of true scores.&lt;/p&gt;
&lt;p&gt;The equation for attenutation is actually quite straightforward, and it is similar to Kelley’s regression-based true score estimation equation above. If we have two measures, say measures &lt;span class=&#34;math inline&#34;&gt;\(M_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_2\)&lt;/span&gt;, each with corresponding reliability estimates of &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{M_1,\theta_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{M_2,\theta_2}\)&lt;/span&gt;, the correlation between the osberved scores of the measures &lt;span class=&#34;math inline&#34;&gt;\(r_{M_1,M_2}\)&lt;/span&gt; is &lt;em&gt;attenuated&lt;/em&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{M_1,M_2} = \hat{r}_{\theta_1,\theta_2}\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\hat{r}_{\theta_1,\theta_2}\)&lt;/span&gt; is our estimate for what the correlation between true scores should be, but our observed correlation is &lt;span class=&#34;math inline&#34;&gt;\(r_{M_1,M_2}\)&lt;/span&gt; is attenuated by the reliability of each measure. Inuitively, if &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{M_1,\theta_1} = \rho^{2}_{M_2,\theta_2} = 1\)&lt;/span&gt;, then the observed scores on each measure are equal to the true scores (i.e. all &lt;span class=&#34;math inline&#34;&gt;\(\theta_1 = M_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_2 = M_2\)&lt;/span&gt;), and there is no attenuation. Otherwise, the true correlation becomes attenuated in proportion to the square root of the product of each measure’s reliability.&lt;/p&gt;
&lt;p&gt;To disattenuate the observed correlation, we can just use some algebra to rearrange the equation to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{r}_{\theta_1,\theta_2} = \frac{r_{M_1,M_2}}{\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although this disattenuation formula is widely accepted, how to best estimate the corresponding confidence interval–which is necessary for making statistical inference–is much more controversial. For our purposes, we will use a simplified approach, which applies the disattenuation formula above to the lower and upper bounds of the observed correlation confidence interval (see &lt;a href=&#34;https://www.jstor.org/stable/1434905?seq=1#metadata_info_tab_contents&#34;&gt;Winne &amp;amp; Belfry, 1982&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{Lower}{\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}} &amp;lt; \hat{r}_{\theta_1,\theta_2} &amp;lt; \frac{Upper}{\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will explore these disattenuation corrections in more detail after we cover generative models below.&lt;/p&gt;
&lt;div id=&#34;a-brief-note-on-disattenuation-and-the-meaning-of-test-retest-reliability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A Brief Note on Disattenuation and the Meaning of Test-retest Reliability&lt;/h4&gt;
&lt;p&gt;Before moving on, it is worth emphasizing the implications of the above attenuation equation for research on changes in individual differences over time. For example, if I am interested in how a psychological construct changes over the span of 1 month, I may have a group of participants take the same measure at two separate timepoints, followed by computing a correlation to determine if individual differences at time 1 are consistent at time 2. In this case, we would actually need to correct this correlation by the reliability of each measure if we wanted to infer whether or not observed changes result from actual changes in the underlying, supposedly trait-like construct (i.e. true score), versus measurement error resulting from imprecision. This is an important distinction, because confusion between these different sources of variation (i.e. actual change versus low precision) can result in strong conclusions regarding the utility of certain measures for individual differences research. If you are interested in more on this topic, I cover it in detail in &lt;a href=&#34;http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/&#34;&gt;a previous blog post&lt;/a&gt;, where I dive into the so-called “Reliability Paradox”. In fact, it isn’t so paradoxical after all.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generative-modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Generative Modeling&lt;/h1&gt;
&lt;p&gt;Generative models can accomplish all of what we described above pertaining to classical test theory, but in a more intuitive and easy to implement way–or at least that is what I hope to convince you of by the end of this post.&lt;/p&gt;
&lt;p&gt;When building generative models, as opposed to thinking about “true” and “observed” scores, we will think about “true” and “estimated” data-generating parameters, respectively. As opposed to referring to “error variance” or “standard errors of the mean”, we will discuss “uncertainty”–or precision–in our parameter estimates. Finally, as opposed to relying on the asymptotic assumptions necessary for us to estimate error variances using normal approximations, we will use hierarchical Bayesian modeling to jointly account for our uncertainty across all parameters of interest.&lt;/p&gt;
&lt;div id=&#34;the-goal-of-generative-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Goal of Generative Modeling&lt;/h2&gt;
&lt;p&gt;From the perspective of generative modeling, our goal is to specify a joint probability distribution that describes both the relationships between parameters within our model and the relationship between model parameters and observed data. By specifiying a generative model, all of our assumptions are made explicit, which is a useful practice conducive to theory development and testing.&lt;/p&gt;
&lt;p&gt;For the current application, we are interested in the relationship between a trait and behavioral measure, which are captured using a questionnaire and response time task, respectively. Our goal is to determine if individual differences in one measure relate to individual differences in the other. Therefore, we need to develop models of the data-generating process underlying both: (1) responding to questionnaire items, (2) response time distributions, and (3) the relationship between parameters of 1 and 2. Additionally, we need to ensure that our model accounts for the uncertainty in our individual-level parameter estimates (analagous to dissociating error from observed scores to get true scores within classical test theory). Finally, we will compare how well the generative model can uncover individual differences compared to the disattentuation approach used within the context of classical test theory.&lt;/p&gt;
&lt;div id=&#34;generative-model-of-questionnaire-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generative Model of Questionnaire Data&lt;/h3&gt;
&lt;p&gt;For the questionnaire data, we will use the bernoulli distribution, where the response to each item can be thought of as a bernoulli trial. Recall that we used the bernoulli distribution within the classical test theory example to estimate the standard error for our reliability calculation.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;bernoulli model&lt;/em&gt;, as we will now refer to it, is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Pr}(x_{i,t}=1) = p_{i} = 1 - \text{Pr}(x_{i,t}=0) = 1 - q_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Pretty simple! As in the classical test theory example, &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is the success probability, or the probability that the response of individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is 1. Then, &lt;span class=&#34;math inline&#34;&gt;\(q_i\)&lt;/span&gt; is the failure probability, or the probability that the response is 0 (which is simply &lt;span class=&#34;math inline&#34;&gt;\(1-p_i\)&lt;/span&gt;). We will then define &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; such that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{1}{1+\text{exp}(-\theta_i)}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_i ~(-\infty &amp;lt; \theta_i &amp;lt; +\infty)\)&lt;/span&gt; is unbounded, and the logistic function transforms &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; to ensure that &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is between 0 and 1. Note that we use this transformation because &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is a probability, and therefore must be between 0 and 1. We could use other types of tranformations, but I chose to use the logistic function because it may be more familiar to readers (logistic regression!) compared to other functions. In fact, this model can be thought of as a very simple Item Response Theory model, with only an “ability” parameter for each person (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;), and item difficulties and discriminabilities set to 1.&lt;/p&gt;
&lt;p&gt;Given the above generative specification, the parameter that we actually need to estimate for each individual is &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is then determined by the logistic function.&lt;/p&gt;
&lt;p&gt;However, in addition to specifying how the observed responses are generated, we also need to specify how the individual-level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters are generated. Similar to how the classical test theory approach requires us to assume a population-level sampling distribution, the generative modeling approach requires us to assume a group-level generative distribution. For simplicity, we may assume that the individual-level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters are generated by a standard normal distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_i \sim \mathcal{N}(0,1)\]&lt;/span&gt;
In Bayesian terminology, such a group-level distribution is often referred to as a &lt;em&gt;prior distribution&lt;/em&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, but I actually do not like this terminology–I instead prefer to think of is as our generative model of the parameters themselves. My reasoning is simple–we do not actually need to specify the parameters of the group-level normal distribution like we did above. Instead, we could estimate these group-level parameters from the data itself:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_i \sim \mathcal{N}(\mu_{\theta},\sigma_{\theta})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, we need to make a generative assumption regarding the group level mean and standard deviation parameters (or a prior on the group-level parameters in typical Bayes speak). In our case, we will assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\theta} \sim \mathcal{N}(0,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\theta} \sim \text{half-}\mathcal{N}(0,1)\)&lt;/span&gt;. Here, &lt;span class=&#34;math inline&#34;&gt;\(\text{half-}\mathcal{N}\)&lt;/span&gt; indicates a half-normal distribution, which only places probability mass on values greater than 0 (given that standard deviations must be positive). Note that the generative model is now considered hierarchical–as we fit the model, the the individual-level parameters will influence the group-level parameters, which will in turn influence the all individual-level parameters. Much like for the regression-based true score estimates in classical test theory, our individual-level parameters will become pooled toward the group-level mean, which will also shrink the uncertainty intervals at the individual-level.&lt;/p&gt;
&lt;p&gt;Because it is difficult to get a sense of how to interpret these generative assumptions with respect to the outcome, we can generate samples from the model to observe the distribution of &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; and ensure that it aligns with our expectations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of samples we will draw from the priors on group-level dist
n_draw &amp;lt;- 12
# Number of individual-level theta paramaeter drawn from group dist 
n_subj &amp;lt;- 30

# Loop through and plot
foreach(i=1:n_draw, .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Sample group-level parameters from priors 
  mu_theta &amp;lt;- rnorm(1,0,1)
  sig_theta &amp;lt;- rtruncnorm(1,0,1,a=0,b=Inf) # normal dist truncated at 0
  
  # Sample individual level parameters from group distribution
  theta &amp;lt;- rnorm(n_subj, mu_theta, sig_theta)
  # Logistic transform to ensure 0 &amp;lt; p &amp;lt; 1
  p &amp;lt;- 1/(1+exp(theta))
  data.frame(n_draw = rep(i, n_subj),
             p      = p)
} %&amp;gt;%
  ggplot(aes(x = p)) +
  geom_histogram(fill = I(&amp;quot;#8F2727&amp;quot;)) +
  xlab(expression(p[i])) +
  facet_wrap(&amp;quot;n_draw&amp;quot;, ncol = 3, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that each of these 12 panels are separate samples from the priors on the group-level distribution (i.e. separate samples from &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\theta} \sim \mathcal{N}(0,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\theta} \sim \text{half-}\mathcal{N}(0,1)\)&lt;/span&gt;). Within each panel, 30 individual-level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters have been sampled, which we then transformed to the bernoulli success probability &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; plotted above.&lt;/p&gt;
&lt;p&gt;From the panels, we can see that our generative specification can produce many different distributions of possible success probabilities–in other words, our model is relatively uninformative with respect to what the individual-level success probabilities will be. For our application, this is just fine! Because this is such a simple model, I feel comfortable looking at only the success probability generated from our model. When models are more complex, it is useful to do more rigorous prior predictive simulations, which simulate to the level of observed data.&lt;/p&gt;
&lt;div id=&#34;generative-model-parameters-vs-classical-test-theory-true-scores&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Generative Model Parameters vs Classical Test Theory True Scores&lt;/h4&gt;
&lt;p&gt;Before moving on, we can take a look at how the generative model estimates compare to the classical test theory regression-based true score estimates that we computed above. As a refresher, the regression-based true score estimates were pooled toward the group-level mean, and they were also more precise (i.e. smaller confidence intervals) compared to the observed score counterparts.&lt;/p&gt;
&lt;p&gt;First, here is the Stan code we will use to fit the generative bernoulli model (note that the model is saved in the R envirnoment to the variable &lt;code&gt;m_bernoulli&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load in rstan first
library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N;      // # of subjects
    int N_items; // # of timepoints
    int Y[N, N_items]; // Responses for each subject and item
}
parameters {
  // Group-level parameters
  // SDs
  real&amp;lt;lower=0&amp;gt; sigma_theta;
  // means
  real mu_theta;
  
  // Individual-level parameters
    vector[N] theta_pr;
}
transformed parameters {
  // Individual-level parameters 
  vector[N] theta;
  
  // Compute individual-level parameters from non-centered parameterization
  theta = mu_theta + sigma_theta * theta_pr;
}
model {
  // Priors on group-level means
  mu_theta ~ normal(0, 1);
  
  // Priors on group-level SDs
  sigma_theta ~ normal(0, 1);
  
  // Priors on individual-level parameters
  theta_pr ~ normal(0, 1);
    
  // For each subject
  for (i in 1:N) {
    // self-report model
    Y[i,:] ~ bernoulli_logit(theta[i]);
  }
}
generated quantities {
  vector[N] p;
  // Success probability estimate for each individual
  p = inv_logit(theta);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan code above uses a &lt;a href=&#34;https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html&#34;&gt;non-centered parameterization&lt;/a&gt; of the group-level portion of the model &lt;a href=&#34;http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/&#34;&gt;for reasons described in a previous post&lt;/a&gt;, but otherwise is mathematically equivalent to the generative model as described by the equations above.&lt;/p&gt;
&lt;p&gt;The R code below fits the generative bernoulli model to the same data we used above when estimating true scores, with item sizes of 10, 30, and 100 to demonstrate the effects of pooling:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# seeded to reproduce same theta&amp;#39;s and data as above for comparison
set.seed(43202) 

# Number of subjects and items
n_subj  &amp;lt;- 20
n_items &amp;lt;- c(10, 30, 100)

# Random sample of &amp;quot;true&amp;quot; scores around .7
theta  &amp;lt;- rnorm(n_subj, .7, .1)

# Estimate observed and true score
gen_dat &amp;lt;- foreach(i=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Generate observed data for each subject using &amp;quot;true&amp;quot; score
  X_all &amp;lt;- foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
   rbinom(n_items[i], 1, prob = theta[t]) # theta same as in above example
  }
  # Fit generative model
  fit_bernoulli &amp;lt;- sampling(m_bernoulli,
                            data   = list(N = n_subj,
                                          N_items = n_items[i],
                                          Y = X_all),
                            iter   = 3000,
                            warmup = 500,
                            chains = 4,
                            cores  = 4,
                            seed  = 43202) 
  pars &amp;lt;- rstan::extract(fit_bernoulli)
  foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Using observed scores from parallel form 1
    bayes_est &amp;lt;- pars$p[,t]
    hdi &amp;lt;- hBayesDM::HDIofMCMC(bayes_est)
    
    data.frame(subj_num  = t,
               n_items   = n_items[i],
               theta     = theta[t],
               bayes_theta = mean(bayes_est),
               bayes_lo    = hdi[1],
               bayes_hi    = hdi[2])
  }
}

# Plot true, observed, and estimated true scores
dis_dat %&amp;gt;%
  full_join(gen_dat) %&amp;gt;%
  mutate(subj_num = reorder(subj_num, theta)) %&amp;gt;%
  ggplot(aes(x = subj_num, y = theta)) + 
  geom_point(color = I(&amp;quot;black&amp;quot;)) +
  geom_point(aes(x = subj_num, y = theta_hat), 
             color = I(&amp;quot;#DCBCBC&amp;quot;),
             position = position_jitter(width=.3, height=0, seed = 2)) +
  geom_linerange(aes(x = subj_num, 
                    ymin = theta_hat - 1.96*se_hat, 
                    ymax = theta_hat + 1.96*se_hat), 
                color = I(&amp;quot;#DCBCBC&amp;quot;), 
                position = position_jitter(width=.3, height=0, seed = 2)) +
  geom_point(aes(x = subj_num, y = bayes_theta), 
             color = I(&amp;quot;#8F2727&amp;quot;),
             position = position_jitter(width=.3, height=0, seed = 1)) +
  geom_linerange(aes(x = subj_num, 
                    ymin = bayes_lo, 
                    ymax = bayes_hi), 
                color = I(&amp;quot;#8F2727&amp;quot;),
                position = position_jitter(width=.3, height=0, seed = 1)) +
  geom_hline(yintercept = X_bar, linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .4, label = expression(&amp;quot;True&amp;quot;~theta[i]), 
           color = &amp;quot;black&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .3, label = expression(&amp;quot;Est&amp;quot;~hat(theta)[i]),
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .2, label = expression(&amp;quot;Gen&amp;quot;~hat(theta)[i]), 
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  facet_wrap(c(&amp;quot;n_items&amp;quot;), nrow = 1) +
  ggtitle(&amp;quot;Regression-Based True Scores vs\nGenerative Model Estimates&amp;quot;) +
  xlab(&amp;quot;Subject&amp;quot;) +
  ylab(&amp;quot;Value&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x.bottom = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I think this is super cool! What we see is that the generative model expectations (i.e. the posterior means) and uncertainty intervals (i.e. the 95% highest density intervals) are &lt;em&gt;very&lt;/em&gt; similar to the corresponding regression-based true score estimates and 95% confidence intervals. In fact, the point esitmates for both approaches are almost indistinguishable. Given that the regression-based true scores have a Bayesian interpretation, this should not be too surprising, yet it is still nice to see it work out empirically.&lt;/p&gt;
&lt;p&gt;More generally, this example demonstrates that generative models can give us the same “true scores” that classical test theory does, yet we do not have to compute reliability, etc., to get there. Instead, we made generative, distributional assumptions regarding how our model parameters related to each other (e.g., the group-level generative model) and to the observed data. Then, we took the posterior expectation (i.e. the posterior mean) of the individual-level parameters to get our best estimates of the “true” underlying data-generating parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generative-model-of-response-time-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generative Model of Response Time Data&lt;/h3&gt;
&lt;p&gt;For the response time model, we will assume that response times for each individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and for each trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; arise from a normal distribution, which we will term the &lt;em&gt;normal model&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{RT}_{i,t} \sim \mathcal{N}(\delta_i, \sigma_i)\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; indicate the mean and standard deviation, respectively, of the response time distribution for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Of course, response times are not typically normally distributed in many beahvioral tasks, but we will ignore that for this demonstration.&lt;/p&gt;
&lt;p&gt;Like the bernoulli model for the questionnaire data, we can then assume that the
individual-level normal model parameters are themselves generated by normal distributions such that &lt;span class=&#34;math inline&#34;&gt;\(\delta_i \sim \mathcal{N}(\mu_{\delta},\sigma_{\delta})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{log}(\sigma_i) \sim \mathcal{N}(\mu_{\sigma},\sigma_{\sigma})\)&lt;/span&gt;. To specify the group-level parameters, it is important to remember that response times are positive valued, and typically greater than 200 milliseconds. We could encode this into the model in the prior distribution. However, for the sake of brevity, we will not focus too much on this.&lt;/p&gt;
&lt;p&gt;For the &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; parameters, we can assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\delta} \sim \mathcal{N}(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\delta} \sim \text{half-}\mathcal{N}(0,0.5)\)&lt;/span&gt;. Next, for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters, we can assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\sigma} \sim \mathcal{N}(-1,0.2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\sigma} \sim \text{half-}\mathcal{N}(0,0.5)\)&lt;/span&gt;. This parameterization whould ensure that response times are generally above 0, but there is still a good amount of variation. It is much harder to form an intuition for what types of response time distributions these priors would produce. Therefore, let us simulate!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of samples we will draw from the priors on group-level dist
n_draw &amp;lt;- 10
# Number of individual-level theta paramaeter drawn from group dist 
n_subj &amp;lt;- 5
# Number of trials to simulate from individual-level normal distributions
n_trials &amp;lt;- 200

# Loop through and plot
foreach(d=1:n_draw, .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Sample group-level parameters from priors 
  mu_delta  &amp;lt;- rnorm(1,1,1)
  sig_delta &amp;lt;- rtruncnorm(1,0,.5,a=0,b=Inf) # normal dist truncated at 0
  mu_sigma  &amp;lt;- rnorm(1,-1,.2)
  sig_sigma &amp;lt;- rtruncnorm(1,0,.5,a=0,b=Inf)
  
  # Sample individual-level parameters from group dist
  delta &amp;lt;- rnorm(n_subj, mu_delta, sig_delta)
  sigma &amp;lt;- exp(rnorm(n_subj, mu_sigma, sig_sigma))
    
  foreach(i=1:n_subj, .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Sample individual level response times from indiv dist
    RT &amp;lt;- rnorm(n_trials, delta[i], sigma[i])
    
    data.frame(n_draw = rep(d, n_trials),
               n_subj = rep(i, n_trials),
               RT     = RT)
  }
} %&amp;gt;%
  ggplot(aes(x = RT)) +
  geom_histogram(fill = I(&amp;quot;#8F2727&amp;quot;), binwidth = .5) +
  xlab(&amp;quot;Response Time&amp;quot;) +
  facet_grid(n_draw ~ n_subj, scales = &amp;quot;free&amp;quot;) +
  coord_cartesian(xlim = c(-2, 2)) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the rows represent different draws for the group-level distribution parameters, where the columns are the response time distributions generated by 5 different “subjects” drawn from the given group-level parameters. Note also that I have zoomed in so that the x-axis is between -2 and 2. There are clearly some aspects that we could improve on. For example, many response times are below 0, which is not possible (although this is not relevant for our example given our use of simulated data). That said, feel free to modify the priors to get a sense of what happens!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-joint-generative-model-of-questionnaire-and-response-time-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Joint Generative Model of Questionnaire and Response Time Data&lt;/h3&gt;
&lt;p&gt;We have now demonstrated how generative models relate to classical test theory through the use of regression-based true score estimates, and we have developed generative models for both the questionnaire and response time data that we would like to use to make inference on individual differences. In the current application, this amounts to estimating a correlation between the generative parameters for each task, or the correlation between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Although there are many ways to estimate such a correlation, one straightforward method is to assume that &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; are drawn from a multivariate normal distribution as opposed to from independent normal distributions. Mathematically, we can make this change by modifying &lt;span class=&#34;math inline&#34;&gt;\(\theta_i \sim \mathcal{N}(\mu_{\theta},\sigma_{\theta})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i \sim \mathcal{N}(\mu_{\delta},\sigma_{\delta})\)&lt;/span&gt; to instead be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{bmatrix} \theta_{i} \\ \delta_{i} \end{bmatrix} \sim \text{MVNormal} \bigg (\begin{bmatrix} \mu_{\theta} \\ \mu_{\delta} \end{bmatrix}, \mathbf{S}  \bigg )\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt; is a covariance matrix, which can be decomposed into the group-level standard deviations and a 2 by 2 correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R\)&lt;/span&gt; that captures the correlation between the individual-level generative parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\mathbf S &amp;amp; = \begin{pmatrix} \sigma_{\theta} &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\delta} \end{pmatrix} \mathbf R \begin{pmatrix} \sigma_{\theta} &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\delta} \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the correlation matrix contains one free parameter on the off-diagonal, &lt;span class=&#34;math inline&#34;&gt;\(\rho_{\theta,\delta}\)&lt;/span&gt;, which is the correlation of interest:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\mathbf R &amp;amp; = \begin{pmatrix} 1 &amp;amp; \rho_{\theta,\delta} \\ \rho_{\theta,\delta} &amp;amp; 1 \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The only other change we need to make is to set a prior distribution on the correlation matrix–or just &lt;span class=&#34;math inline&#34;&gt;\(\rho_{\theta,\delta}\)&lt;/span&gt; in our case. For our purposes, we will assume a non-informative prior, given by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R \sim \text{LKJcorr} (1)\)&lt;/span&gt;. The &lt;a href=&#34;http://bois.caltech.edu/distribution_explorer/multivariate_continuous/lkj.html&#34;&gt;LKJ distribution&lt;/a&gt; assumes that the correlations on the off-diagonal are &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta-distributed&lt;/a&gt;, ensuring that our correlation is between 0 and 1. In our case, the distribution is uniform across -1 to 1.&lt;/p&gt;
&lt;p&gt;We are now ready to fit the model! The Stan code is given by the following script, which is assigned to the &lt;code&gt;m_joint_RT_Bern&lt;/code&gt; variable in the R environment:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N;             // # of subjects
    int N_items;       // # of items
    int T;             // max # of RT trials across subjects
    real RT[N, T];     // Reaction times for each subject and trial
    int Y[N, N_items]; // Responses for each subject and item
}
parameters {
  // Group-level parameters
  // correlation matrix (cholesky factor for faster computation)
  cholesky_factor_corr[2] R_chol;  
  // SDs
  vector&amp;lt;lower=0&amp;gt;[2] pars_sigma; 
  real&amp;lt;lower=0&amp;gt; sigma_SD;
  // means
  vector[2] pars_mu;
  real sigma_mu;
  
  // Individual-level parameters
    matrix[2,N] pars_pr; 
    vector[N] sigma_pr;
}
transformed parameters {
  // Individual-level parameter off-sets (for non-centered parameterization)
  matrix[2,N] pars_tilde;
  
  // Individual-level parameters 
  vector[N] theta;
  vector[N] delta;
  vector[N] sigma;
  
  // Construct inidividual offsets (for non-centered parameterization)
  pars_tilde = diag_pre_multiply(pars_sigma, R_chol) * pars_pr;
  
  // Compute individual-level parameters from non-centered parameterization
  for (i in 1:N) {
    theta[i] = pars_mu[1] + pars_tilde[1, i];
    delta[i] = pars_mu[2] + pars_tilde[2, i];
    sigma[i] = exp(sigma_mu + sigma_SD * sigma_pr[i]);
  }
}
model {
  // Prior on cholesky factor of correlation matrix
  R_chol ~ lkj_corr_cholesky(1);
  
  // Priors on group-level means
  pars_mu[1]  ~ normal(0, 1);
  pars_mu[2]  ~ normal(1, 1);
  sigma_mu ~ normal(-1, .2);
  
  // Priors on group-level SDs
  pars_sigma[1] ~ normal(0, 1);
  pars_sigma[2] ~ normal(0, .5);
  sigma_SD   ~ normal(0, .5);
  
  // Priors on individual-level parameters
  to_vector(pars_pr)  ~ normal(0, 1);
  to_vector(sigma_pr) ~ normal(0, 1);
    
  // For each subject
  for (i in 1:N) {
    // response time model
    RT[i,1:T] ~ normal(delta[i], sigma[i]);
    
    // self-report model
    Y[i,:] ~ bernoulli_logit(theta[i]);
  }
}
generated quantities { 
  corr_matrix[2] R;
    // Reconstruct correlation matrix from cholesky factor
  R = R_chol * R_chol&amp;#39;;
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like the Stan code for the bernoulli model, the code above is using a non-centered parameterization for the group-level portion of the model. Additionally, we are using a Cholesky decomposition to better estimate the correlation matrix. These are not important for the results, and for our current purposes we can think of these modification as simply ways to more efficiently fit the models. Regardless, the model in the Stan code above is mathematically equivalent to the model described by the equations in the text.&lt;/p&gt;
&lt;p&gt;Time to fit! First, we will load some relevant R packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)
library(doParallel)
library(rstan)
library(hBayesDM)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, the R code below will simulate data where the correlation between the individual-level generative parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; (or the “true scores”) varies from 0 to 1. Then, we will use the classical, reliability-based disattenuation formula along with the joint generative model to recover the true correlation.&lt;/p&gt;
&lt;p&gt;The simulation uses 100 “participants”. Additionally, the questionnaire has 10 items, and the response time task has 50 trials. To the code!:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(43201)

# Number of subjects and items/trials for questionnaire/task
n_subj   &amp;lt;- 100
n_items  &amp;lt;- 10
n_trials &amp;lt;- 50

# Create grid of true generating correlations in (0,1)
true_r &amp;lt;- seq(0,1,length.out = 20)

# Parallel cluster
cl &amp;lt;- makeCluster(4)
registerDoParallel(cl)

# Parallelize across grid of true correlations
joint_dat &amp;lt;- foreach(r=seq_along(true_r), .combine = &amp;quot;rbind&amp;quot;, 
                   .packages = c(&amp;quot;mvtnorm&amp;quot;, &amp;quot;dplyr&amp;quot;, 
                                 &amp;quot;foreach&amp;quot;, &amp;quot;rstan&amp;quot;, &amp;quot;hBayesDM&amp;quot;)) %dopar% {
  
  # Contruct correlation and covariance matrices
  M  &amp;lt;- c(0, .8) # group-level means for theta and delta
  SD &amp;lt;- c(1, .1) # group-level standard deviations for theta and delta
  R  &amp;lt;- matrix(c(1, true_r[r], true_r[r], 1), nrow = 2)
  S  &amp;lt;- diag(SD) %*% R %*% diag(SD)
    
  # Draw individual-level parameters from multivariate normal
  pars &amp;lt;- rmvnorm(n_subj, M, S) %&amp;gt;%
    as.data.frame()
  theta &amp;lt;- pars[,1] # for bernoulli model
  delta &amp;lt;- pars[,2] # for normal model
  sigma &amp;lt;- rnorm(n_subj, .4, .05) # for normal model
  
  # Simulate questionnaire data (i.e. bernoulli generative model)
  X_Q_all &amp;lt;- foreach(i=1:n_subj, .combine = &amp;quot;rbind&amp;quot;) %do% {
    p  &amp;lt;- 1/(1 + exp(-theta[i])) # logistic transform
    rbinom(n_items, 1, prob = p)
  }
  # Simulate resposne time data (i.e. normal generative model)
  X_RT_all &amp;lt;- foreach(i=1:n_subj, .combine = &amp;quot;rbind&amp;quot;) %do% {
    rnorm(n_trials, delta[i], sigma[i])
  }
  
  # group averages of observed scores
  X_bar_Q  &amp;lt;- mean(rowMeans(X_Q_all))
  X_bar_RT &amp;lt;- mean(rowMeans(X_RT_all))
  
  # individual-level observed scores
  X_Q  &amp;lt;- rowMeans(X_Q_all)
  X_RT &amp;lt;- rowMeans(X_RT_all)
  
  # Average of individual-level error variances 
  sig2_ep_Q  &amp;lt;- mean(apply(X_Q_all, 1, est_se2)) # same SE function from earlier 
  sig2_ep_RT &amp;lt;- mean(apply(X_RT_all, 1, function(x) var(x)/(length(x)-1)))
  
  # Group-level observed score variance
  sig2_X_Q  &amp;lt;- var(X_Q)
  sig2_X_RT &amp;lt;- var(X_RT)
  
  # Compute reliability using SEM approach
  rho_Q  &amp;lt;- 1 - (sig2_ep_Q/sig2_X_Q)
  rho_RT &amp;lt;- 1 - (sig2_ep_RT/sig2_X_RT)

  # Observed correlation and 50% confidence interval
  obs_cor &amp;lt;- cor.test(X_Q, X_RT, conf.level = .5)
  obs_r   &amp;lt;- obs_cor$estimate
  obs_ci  &amp;lt;- obs_cor$conf.int
  
  # Disattenuated correlation and 50% confidence interval
  dis_r  &amp;lt;- obs_r / sqrt(rho_Q*rho_RT)
  dis_ci &amp;lt;- obs_ci / sqrt(rho_Q*rho_RT)
    
  # Stan data for joint generative model
  stan_data &amp;lt;- list(N       = n_subj,
                    N_items = n_items,
                    T       = n_trials,
                    Y       = X_Q_all,
                    RT      = X_RT_all)
  
  # Fit joint generative model
  fit_joint &amp;lt;- sampling(m_joint_RT_Bern, 
                        data   = stan_data,
                        iter   = 1000,
                        warmup = 200, 
                        chains = 1, 
                        cores  = 1,
                        seed   = 43201)
  pars &amp;lt;- rstan::extract(fit_joint) # extract parameters
  
  # Generative model correlation and 50% highest density interval
  bayes_r &amp;lt;- mean(pars$R[,1,2])
  hdi &amp;lt;- hBayesDM::HDIofMCMC(pars$R[,1,2], credMass = .5)
  
  # Save out data
  data.frame(true_r    = true_r[r],
             obs_r     = obs_r,
             obs_lo    = obs_ci[1],
             obs_hi    = obs_ci[2],
             dis_r     = dis_r,
             dis_lo    = dis_ci[1],
             dis_hi    = dis_ci[2],
             bayes_r   = bayes_r,
             bayes_lo  = hdi[1],
             bayes_hi  = hdi[2])
}
# Stop the parallel boi
stopCluster(cl)

# Hacky way to get some ggplot
qplot() +
  geom_line(aes(x = true_r, y = true_r), col = I(&amp;quot;black&amp;quot;),
            linetype = 2, size = 1) +
  geom_ribbon(aes(x = true_r,
                  ymin = joint_dat$obs_lo,
                  ymax = joint_dat$obs_hi,
                  fill = I(&amp;quot;gray&amp;quot;)), alpha = .2) +
  geom_ribbon(aes(x = true_r,
                  ymin = joint_dat$bayes_lo,
                  ymax = joint_dat$bayes_hi,
                  fill = I(&amp;quot;#8F2727&amp;quot;)), alpha = .2) +
  geom_ribbon(aes(x = true_r,
                  ymin = joint_dat$dis_lo,
                  ymax = joint_dat$dis_hi,
                  fill = I(&amp;quot;#DCBCBC&amp;quot;)), alpha = .2) +
  geom_line(aes(x = true_r, y = joint_dat$obs_r, col = I(&amp;quot;gray&amp;quot;))) +
  geom_line(aes(x = true_r, y = joint_dat$dis_r, col = I(&amp;quot;#DCBCBC&amp;quot;))) +
  geom_line(aes(x = true_r, y = joint_dat$bayes_r, col = I(&amp;quot;#8F2727&amp;quot;))) +
  annotate(&amp;quot;text&amp;quot;, x = .8, y = .45, label = &amp;quot;Observed&amp;quot;, 
           color = &amp;quot;gray&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = .35, y = .72, label = &amp;quot;Disattenuated&amp;quot;, 
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = .85, y = .75, label = &amp;quot;Generative&amp;quot;, 
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +
  ggtitle(&amp;quot;Self-report &amp;amp; Behavioral Task Convergence&amp;quot;) +
  xlab(&amp;quot;True Generating Correlation&amp;quot;) +
  ylab(&amp;quot;Estimated Correlation&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And there we have it! The generative and classical disattenuation approaches produce almost identical correlation estimates and 50% uncertainty intervals. In fact, if you are having trouble identifying the classical disattenutation line and intervals, it is because they are just that hard to distinguish from those of the generative model.&lt;/p&gt;
&lt;p&gt;Note that there is some noise across the range of “true” (or generating) correlations, which arises from the probabilistic nature of the simulated data. We could get a better sense of what the approaches produce in expectation by running many iterations for each true/generating correlation, but I prefer the above approach to get a sense of how each may work in a single study.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;The current post demonstrated that generative models are well-suited to make inference on individual differences, and in fact they can give us results similar to approaches developed within the framework of classical test theory (i.e. the correction for attenuation, true score estimation, etc.). Regardless of the approach you take, the results here make it clear that accounting for uncertainty (or reliability) is very important when our goal is to make inference on individual differences. Otherwise, our statistical inferences will be biased and overconfident. Moving forward, I hope that you consider accounting for such uncertainty in your models, regardless of the approach you decide to take.&lt;/p&gt;
&lt;p&gt;More generally, the generative modeling approach is easily extendable. Unlike the classical test theory approach, generative models do not require us to work out a new sampling distribution each time we modify the assumed data-generating model. Instead, we specify the model in a way that is consistent with our theory, and the joint estimation of all parameters allows us to account for uncertainty (or the lack of precision of parameter estimates) across all levels of analysis. Therefore, when doing generative modeling, we do not necessarily need to think about the reliability of our measure–instead we can think about uncertanity in our model parameters. The model can then be refined, extended, and even simulated forward to generate predictions for observed data, and once we are happy with our generative model, we can use Bayesian estimation to condition on the data. The result is a joint probability distribution that contains all the information we need to make subsequent inferences and decisions–in our case, a self-report to behavioral task correlation. If our uncertainty intervals are too wide to make a good decision, we can collect more data. Further, we can conduct formal decision analyses (e.g., see &lt;a href=&#34;https://twiecki.io/blog/2019/01/14/supply_chain/&#34;&gt;Wiecki &amp;amp; Kumar, 2019&lt;/a&gt;). Altogether, generative modeling provides a flexible framework for developing and testing theories.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-final-note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Final Note&lt;/h1&gt;
&lt;p&gt;I hope that this post was useful for you! I sure learned a lot throughout, and it was great to finally delve into the relationships between classical test theory and generative modeling. Perhaps you will consider generative modeling for your next research project :D.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.0 (2021-05-18)
## Platform: aarch64-apple-darwin20 (64-bit)
## Running under: macOS 12.0.1
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] hBayesDM_1.1.1       Rcpp_1.0.8           doParallel_1.0.16   
##  [4] iterators_1.0.13     mvtnorm_1.1-2        rstan_2.21.2        
##  [7] StanHeaders_2.21.0-7 truncnorm_1.0-8      ggridges_0.5.3      
## [10] ggplot2_3.3.5        foreach_1.5.1        dplyr_1.0.7         
## 
## loaded via a namespace (and not attached):
##  [1] prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.29     
##  [5] utf8_1.2.2         V8_3.4.2           R6_2.5.1           plyr_1.8.6        
##  [9] stats4_4.1.0       evaluate_0.14      highr_0.9          blogdown_1.7.3    
## [13] pillar_1.6.2       rlang_0.4.12       curl_4.3.2         rstudioapi_0.13   
## [17] data.table_1.14.0  callr_3.7.0        rmarkdown_2.11     labeling_0.4.2    
## [21] stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.1.0    
## [25] xfun_0.29          pkgconfig_2.0.3    pkgbuild_1.2.0     htmltools_0.5.2   
## [29] tidyselect_1.1.1   tibble_3.1.4       gridExtra_2.3      bookdown_0.24     
## [33] codetools_0.2-18   matrixStats_0.60.1 fansi_0.5.0        crayon_1.4.1      
## [37] withr_2.4.2        grid_4.1.0         jsonlite_1.7.3     gtable_0.3.0      
## [41] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     formatR_1.11      
## [45] scales_1.1.1       RcppParallel_5.1.4 cli_3.0.1          stringi_1.7.6     
## [49] farver_2.1.0       ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8       
## [53] tools_4.1.0        glue_1.6.0         purrr_0.3.4        processx_3.5.2    
## [57] fastmap_1.1.0      yaml_2.2.1         inline_0.3.19      colorspace_2.0-2  
## [61] knitr_1.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
