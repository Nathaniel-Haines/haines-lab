---
title: Human Choice and Reinforcement Learning (1)
author: Nathaniel Haines
date: '2017-04-04'
slug: ''
categories:
  - Reinforcement Learning
tags: ["R", "Modeling", "Learning"]
description: The first post in a series on modeling human choice using reinforcement learning.  
draft: no
keywords:
  - Reinforcement Learning
  - Value-based decision making
  - Modeling
topics: topic 1
type: post
---



<div id="short-history" class="section level2">
<h2>Short history</h2>
<p>In 1972, Robert Rescorla and Allan Wagner developed a formal theory of associative learning, the process through which multiple stimuli are associated with one-another. The most widely used example (Fig. 1) of associative learning comes straight from Psychology 101–Pavlov’s dog.</p>
<div id="figure-1" class="section level4">
<h4>Figure 1</h4>
<p><img src="http://www.savingstudentsmoney.org/psychimg/stangor-fig07_003.jpg" alt="Figure 1" style="height: 100%; width: 100%; object-fit: contain"/></p>
<p>The idea is simple, and it’s something that we experience quite often in everyday life. In the same way that Pavlov’s dog begins to drool after hearing a bell, certain cognitive and/or biological processes are triggered when we are exposed to stimuli that we have been exposed to in the past. But how can this learning process be modeled? That is to say, what sort of <em>equation</em> can we use to describe how an agent learns to associate multiple stimuli? To answer these questions, Rescorla and Wagner developed what is now know as the Rescorla-Wagner updating rule:</p>
<p><span class="math display">\[\Delta V_{A} = \alpha_{A} \beta_{1} (\lambda_{1} - V_{AX})\]</span></p>
<p><span class="math display">\[\Delta V_{X} = \alpha_{X} \beta_{1} (\lambda_{1} - V_{AX})\]</span></p>
<p>First off, note that the original Rescorla-Wagner rule was developed to explain <em>compound stimuli</em> (e.g. presentation of a bell and light, followed by food). Here, <span class="math inline">\(\Delta V_{A}\)</span> is the change in associative strength between stimulus <span class="math inline">\(A\)</span> (e.g. the bell) and the response (e.g. food). <span class="math inline">\(\Delta V_{X}\)</span> has the same interpretation, but refers to stimulus <span class="math inline">\(X\)</span> (e.g. the light).</p>
<p><span class="math inline">\(0 \leq \beta_{1} \leq 1\)</span> is a free parameter (i.e. we estimate it from the data) referred to as the <em>learning rate</em>. The learning rate controls how quickly updating takes place, where values near 0 and 1 reflect sluggish and rapid learning, respectively. Above, the learning rate is shared across stimuli.</p>
<p><span class="math inline">\(0 \leq \alpha_{A} \leq 1\)</span> is a free parameter which is determined by the salience of stimulus <span class="math inline">\(A\)</span>, and <span class="math inline">\(0 \leq \alpha_{X} \leq 1\)</span> for stimulus <span class="math inline">\(X\)</span>. Unlike the learning rate, which is shared across stimuli, the salience parameter is specific to each stimulus. Put simply, this just means that learning can occur at different rates depending on the type of stimulus (e.g. I may associate a light with food more quickly than a tone).</p>
<p><span class="math inline">\(\lambda_{1}\)</span> is described as “the asymptote of associative strength”. This is the upper-limit on how strong the association strength can be. In this way, <span class="math inline">\(\lambda_{1}\)</span> reflects the value being updated toward by the learning rate (<span class="math inline">\(\beta\)</span>) and stimulus salience (<span class="math inline">\(\alpha\)</span>).</p>
<p>Lastly, <span class="math inline">\(V_{AX}\)</span> is the total associative strength of the compound stimulus <span class="math inline">\(AX\)</span>. Rescorla and Wagner assume that this is a simple sum of both stimuli strengths:</p>
<p><span class="math display">\[V_{AX} = V_{A} + V_{X}\]</span></p>
<p>Interpretation of this model is actually quite simple–we update associative strength (<span class="math inline">\(V_{*}\)</span>) for each stimulus by taking steps (<span class="math inline">\(\alpha \beta\)</span>) toward the difference between the asymptote of learning (<span class="math inline">\(\lambda_{1}\)</span>) and the current associative strength of the compund stimulus (<span class="math inline">\(V_{AX}\)</span>). By continually exposing an agent to a tone or bell paired with a reward (or punishment), the agent learns the associative strength of the conditioned and unconditioned stimuli.</p>
<p>While the original Rescorla-Wagner model was successful for explaining associative learning for classical conditioning paradigms, what of operant conditioning? What if we are interested in how people learn to make decisions? In most current research, we are not interested in knowing how people learn to associate lights or tones with some reward. Instead, we would like a model that can describe how people learn to select the <em>best choice</em> among <em>multiple choices</em>. This model would need to explain how people assign values to multiple options as well as how they decide which option to choose. In statistical terms, we want to know how people solve the <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit"><em>multi-armed bandit</em></a> problem. In the following section, we will begin to solve this problem.</p>
</div>
</div>
<div id="current-implementations" class="section level2">
<h2>Current Implementations</h2>
<p><img src="http://777click.com/assets/templates/777/img/777banner.jpg" alt="Figure 2" style="height: 100%; width: 100%; object-fit: contain"/></p>
<p>As a motivating example, we will explore the simple problem of learning the probability that a slot machine will payoff (i.e. that you will win any amount after pulling the lever). To do so, the above equations only need minor modifications. Additionally, we will change the terminology–instead of learning an associative strength, we will now be learning the <em>probability of a winning outcome</em>. To start, we take the first equation above and write it for a single stimulus <span class="math inline">\(V\)</span>, but exchange <span class="math inline">\(V\)</span> with the probability of observing a win:</p>
<p><span class="math display">\[\Delta Pr(win) = \alpha \beta (\lambda - Pr(win))\]</span></p>
<p>Because <span class="math inline">\(\Delta Pr(win) = Pr(win)_{t+1} - Pr(win)_{t}\)</span> (where <span class="math inline">\(t\)</span> is the current trial), we can re-write the above equation into an iterative form:</p>
<p><span class="math display">\[Pr(win)_{t+1} = Pr(win)_{t} + \alpha \beta (\lambda_{t} - Pr(win)_{t})\]</span></p>
<p>Since we are using this model to explain how people learn the probability of a binary outcome, <span class="math inline">\(\lambda_{t} \in [0, 1]\)</span> now represents the outcome of slot machine roll on trial <span class="math inline">\(t\)</span>. Now, we drop the <span class="math inline">\(\alpha\)</span> parameter to simplify the model further. Because we have a single choice option, estimating both <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> would lead to an unidentifiable model. This is because as either one increases, the other can decrease and lead to the same exact predictions. Even with multiple choice options, this problem is still apparent. In current applications of the Rescorla-Wagner rule, we do not include a “salience” parameter. Now, we have:</p>
<p><span class="math display">\[Pr(win)_{t+1} = Pr(win)_{t} + \beta (\lambda_{t} - Pr(win)_{t})\]</span></p>
<p>The model is now much easier to interpret. The term <span class="math inline">\((\lambda_{t} - Pr(win)_{t})\)</span> can be thought of as the <em>prediction error</em>–the difference between the actual value <span class="math inline">\(\lambda_{t}\)</span> revealed after the choice was made and the expected value of the choice <span class="math inline">\(Pr(win)_{t}\)</span> for that trial. <span class="math inline">\(\beta\)</span> (the learning rate) then updates the current expectation <span class="math inline">\(Pr(win)_{t}\)</span> in the direction of the prediction error <span class="math inline">\((\lambda_{t} - Pr(win)_{t})\)</span>.</p>
</div>
<div id="r-example" class="section level2">
<h2>R Example</h2>
<p>To see the Rescorla-Wagner rule in action, let’s generate some fake data using the binomial distribution and try to estimate the rate parameter using various different values for the learning rate.</p>
<pre class="r"><code># For pretty images
library(ggplot2)

# Number of &#39;trials&#39;
num_trials &lt;- 100

# The win rate is 0.7
payoff &lt;- rbinom(n = num_trials, size = 1, prob = 0.7)

# Function that iterates the Rescorla-Wagner rule 
rw_update &lt;- function(lambda, beta, init) {
  # To store expected value for each trial
  Pr_win &lt;- vector(length=length(lambda))
  # Set initial value
  Pr_win[1] &lt;- init
  for (t in 1:(length(lambda)-1)) {
    Pr_win[t+1] &lt;- Pr_win[t] + beta * (lambda[t] - Pr_win[t])
  }
  return(Pr_win)
}

# With initial expectation = 0, try different learning rates
beta_05 &lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_25 &lt;- rw_update(lambda = payoff, beta = 0.25, init = 0)
beta_50 &lt;- rw_update(lambda = payoff, beta = 0.50, init = 0)
beta_75 &lt;- rw_update(lambda = payoff, beta = 0.75, init = 0)
beta_95 &lt;- rw_update(lambda = payoff, beta = 0.95, init = 0)

# Store in data.frame for plotting
all_data &lt;- stack(data.frame(beta_05 = beta_05,
                             beta_25 = beta_25,
                             beta_50 = beta_50,
                             beta_75 = beta_75,
                             beta_95 = beta_95))
# Add trial 
all_data[[&quot;trial&quot;]] &lt;- rep(1:num_trials, 5)
names(all_data)[2]  &lt;- &quot;Beta&quot;

# Plot results
p &lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) + 
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&quot;black&quot;, linetype = &quot;longdash&quot;) + 
  ggtitle(&quot;Expected Probability of Winning Outcome&quot;) + 
  xlab(&quot;Trial Number&quot;) + 
  ylab(&quot;Expected Pr(win)&quot;)  
p</code></pre>
<p><img src="/post/2017-04-04-choice_RL_1_files/figure-html/2017-04-02_fig1-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>It is easy to see that the higher learning rates (i.e. &gt; 0.05) are jumping around the true win rate (0.7, dashed line) quite a bit, whereas setting <span class="math inline">\(\beta = 0.05\)</span> allows for a more stable estimate. Let’s try again with learning rates closer to 0.05.</p>
<pre class="r"><code># Learning rates closer to 0.05
beta_01 &lt;- rw_update(lambda = payoff, beta = 0.01, init = 0)
beta_03 &lt;- rw_update(lambda = payoff, beta = 0.03, init = 0)
beta_05 &lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_07 &lt;- rw_update(lambda = payoff, beta = 0.07, init = 0)
beta_09 &lt;- rw_update(lambda = payoff, beta = 0.09, init = 0)

# Store in data.frame for plotting
all_data &lt;- stack(data.frame(beta_01 = beta_01,
                             beta_03 = beta_03,
                             beta_05 = beta_05,
                             beta_07 = beta_07,
                             beta_09 = beta_09))
# Add trial 
all_data[[&quot;trial&quot;]] &lt;- rep(1:num_trials, 5)
names(all_data)[2]  &lt;- &quot;Beta&quot;

# Plot results
p2 &lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) +
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&quot;black&quot;, linetype = &quot;longdash&quot;) + 
  ggtitle(&quot;Expected Probability of Winning Outcome&quot;) + 
  xlab(&quot;Trial Number&quot;) + 
  ylab(&quot;Expected Pr(win)&quot;)
p2</code></pre>
<p><img src="/post/2017-04-04-choice_RL_1_files/figure-html/2017-04-02_fig2-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>These results look a bit better. However, it is apparent that setting <span class="math inline">\(\beta\)</span> too low is making the updating very sluggish. If we run more trials, however, we should see the expectation converge.</p>
<pre class="r"><code># Number of &#39;trials&#39;
num_trials &lt;- 500

# The win rate is 0.7
payoff &lt;- rbinom(n = num_trials, size = 1, prob = 0.7)

# Learning rates closer to 0.05
beta_01 &lt;- rw_update(lambda = payoff, beta = 0.01, init = 0)
beta_03 &lt;- rw_update(lambda = payoff, beta = 0.03, init = 0)
beta_05 &lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_07 &lt;- rw_update(lambda = payoff, beta = 0.07, init = 0)
beta_09 &lt;- rw_update(lambda = payoff, beta = 0.09, init = 0)

# Store in data.frame for plotting
all_data &lt;- stack(data.frame(beta_01 = beta_01,
                             beta_03 = beta_03,
                             beta_05 = beta_05,
                             beta_07 = beta_07,
                             beta_09 = beta_09))
# Add trial 
all_data[[&quot;trial&quot;]] &lt;- rep(1:num_trials, 5)
names(all_data)[2]  &lt;- &quot;Beta&quot;

# Plot results
p2 &lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) +
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&quot;black&quot;, linetype = &quot;longdash&quot;) + 
  ggtitle(&quot;Expected Probability of Winning Outcome&quot;) + 
  xlab(&quot;Trial Number&quot;) + 
  ylab(&quot;Expected Pr(win)&quot;)
p2</code></pre>
<p><img src="/post/2017-04-04-choice_RL_1_files/figure-html/2017-04-02_fig3-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Over 500 trials, the expected value for the win rate converges to the true win rate, 70%.</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>In this post, we reviewed the original Rescorla-Wagner updating rule (a.k.a. the <em>Delta Rule</em>) and explored its contemporary instantiation. I have shown that the Delta Rule can be used to estimate the win rate of a slot machine on a trial-by-trial basis. While this example may first appear trivial (e.g. why not just take the average of all past outcomes?), we will explore more practical usages in later posts. For now, try playing with the above code yourself! If you want a slightly more challenging problem, try finding the solution to the following question:</p>
<ul>
<li><p><strong>How should you change the learning rate so that the expected win rate is always the average of all past outcomes?</strong></p></li>
<li><p>Hint –&gt; With some simple algebra, the Delta Rule can be re-written as follows:</p></li>
</ul>
<p><span class="math display">\[Pr(win)_{t+1} = (1 - \beta) \cdot Pr(win)_{t} + \beta \cdot \lambda_{t}\]</span></p>
<p>Happy solving! See the answer in the next post.</p>
</div>
