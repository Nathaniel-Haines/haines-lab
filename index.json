[{"authors":null,"categories":null,"content":"I am a data scientist with a PhD from The Ohio State University. I have published 20+ peer-reviewed papers and book chapters, many on applied use of generative modeling, Bayesian modeling, computational modeling, and psychometrics to answer complex research questions. I enjoy applying my expertise in both social science and data science to solve real-world modeling problems.\nAs Manager of Data Science Research at Ledger Investing, I lead teams building Bayesian models for insurance-linked securities, a novel asset class that provides alternative sources of capital for the insurance industry. In my role, I lead research initiatives on building time-series and forecasting models, developing robust Bayesian model comparison and model stacking methods, and designing internal software to improve model development and deployment.\n  Download my CV.\n","date":1623888000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1623888000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://haines-lab.com/author/nathaniel-haines/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nathaniel-haines/","section":"authors","summary":"I am a data scientist with a PhD from The Ohio State University. I have published 20+ peer-reviewed papers and book chapters, many on applied use of generative modeling, Bayesian modeling, computational modeling, and psychometrics to answer complex research questions.","tags":null,"title":"Nathaniel Haines","type":"authors"},{"authors":[],"categories":[],"content":"  Coming Soon!  ","date":1700697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700754605,"objectID":"9826497fdb2af2f7b12acd817a0d6176","permalink":"http://haines-lab.com/post/bayesian-design-optimization-an-application-to-item-reduction-in-scale-development-research/","publishdate":"2023-11-23T00:00:00Z","relpermalink":"/post/bayesian-design-optimization-an-application-to-item-reduction-in-scale-development-research/","section":"post","summary":"  Coming Soon!  ","tags":[],"title":"Bayesian Design Optimization: An Application to Item Reduction in Scale Development Research","type":"post"},{"authors":null,"categories":["Computational Reproducibility","R","GitHub","Docker"],"content":"  The Problem Have you ever wanted to run someone else’s R code (or your own—from a distant past), but upon downloading it, you realized that it relied on versions of either R or R libraries that are dramatically different from those on your own computer? This situation can easily spiral into dependency hell, a place that every researcher or data scientist will (always unintentionally) find themselves when trying to get random code from bygone days to run locally. Oftentimes, it may seem impossible to resolve The Problem, and giving up may seem like the only logical thing to do…\nCaption: Occasionally, dependency hell peak’s its head into the real world too\n  The Naive Attempt to Resolve The Problem If you are like me, the first thing you might do when finding yourself in dependency hell is to try installing different versions of the R libraries that R is complaining about. If that doesn’t work, you may even get desperate enough to try installing a different version of R in hopes that The Problem will magically disappear.\nIn some cases, the naive attempt can succeed. But alas, it will often fail, and even when it does succeed—that different version of R (or R libraries) you installed could lead to problems in other ongoing projects. You know, the projects that you briefly forgot about while trying to resolve The Problem for the current project on the to-do list. In this case, dependency hell has evolved into computational whac-a-mole—a rage-inducing game that your therapist told you to stop playing many sessions ago.\nCaption: You, disregarding your therapist\n  A Slightly Better Attempt, But It Requires Foresight If you truly want to resolve dependency issues, you will need to think ahead and build some form of dependency management into your workflow (I know, very unfortunate…). That said, it is getting easier with each passing year to automate a good deal of it. In the remainder of this post, we will walk through how to create a GitHub repository that takes advantage of renv, Docker, and GitHub Actions to make your R workflow reproducible into the future.\n(NOTE: Throughout this post, you can refer to my accompanying GitHub repo to see all the code in one place. I would recommend forking it, cloning to a local directory, and testing things out that way. Then, you can try out some of your own R scripts to see how things work!)\nAn Overview of renv renv is an R package that allows for us to install project-specific packages as opposed to relying on the same set of packages (and their associated versions) across all R projects. Further, we can take a snapshot of the packages we are using in a given project, save the snapshot for later, and then restore (i.e. reinstall) the packages later—even on different computers.\nStarting a New Project with renv Using renv is very easy. First, while in R, set your working directory to the folder that we want to create our project in:\n# This is the GitHub repo associated with this project setwd(\u0026#34;~/computational-reproducibility-in-r\u0026#34;) Next, install renv:\ninstall.packages(\u0026#34;renv\u0026#34;) Now, we simply load the package and then initialize the project:\nrenv::init() # * Initializing project ... # * Discovering package dependencies ... Done! # * Copying packages into the cache ... Done! # * Lockfile written to # \u0026#39;~/computational-reproducibility-in-r/renv.lock\u0026#39;. # Restarting R session...  Installing Packages with renv With the project initialized, we essentially proceed as normal, except that we will install all the packages we need along the way. For example, if our project uses dplyr and ggplot2, we install them as follows:\ninstall.packages(c(\u0026#34;dplyr\u0026#34;, \u0026#34;ggplot2\u0026#34;)) # Installing dplyr [1.0.7] ... # OK [linked cache] # Installing ggplot2 [3.3.5] ... # OK [linked cache] Now, both packages are available to use in our project. Let’s make a plot for proof!\nlibrary(dplyr) library(ggplot2) set.seed(43210) # Yes, your love grows exponentially love_for_renv \u0026lt;- rnorm(100, exp(.022*(1:100)), .8) data.frame(Time = 1:100, love_for_renv = love_for_renv) %\u0026gt;% ggplot(aes(x = Time, y = love_for_renv)) + geom_point() + geom_line() + ylab(\u0026#34;Love for renv\u0026#34;) + scale_y_continuous(breaks = 1:10, limits = c(0,11)) + theme_minimal(base_size = 15) + theme(panel.grid = element_blank()) Now, let’s say that we are wrapping up for the day and would like to push our changes to GitHub. After saving our R file that produces the beautiful graph above, we need to take a snapshot so that renv remembers which libraries we have installed for the current project:\nrenv::snapshot() # The following package(s) will be updated in the lockfile: # # # CRAN =============================== # - MASS [* -\u0026gt; 7.3-54] # - Matrix [* -\u0026gt; 1.3-3] # - R6 [* -\u0026gt; 2.5.1] # - RColorBrewer [* -\u0026gt; 1.1-2] # - cli [* -\u0026gt; 3.0.1] # - colorspace [* -\u0026gt; 2.0-2] # - crayon [* -\u0026gt; 1.4.1] # - digest [* -\u0026gt; 0.6.29] # - dplyr [* -\u0026gt; 1.0.7] # - ellipsis [* -\u0026gt; 0.3.2] # - fansi [* -\u0026gt; 0.5.0] # - farver [* -\u0026gt; 2.1.0] # - generics [* -\u0026gt; 0.1.0] # - ggplot2 [* -\u0026gt; 3.3.5] # - glue [* -\u0026gt; …","date":1642982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1642982400,"objectID":"e3d756e0a0556748c9d1b301100eb5cc","permalink":"http://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/","publishdate":"2022-01-24T00:00:00Z","relpermalink":"/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/","section":"post","summary":"The Problem Have you ever wanted to run someone else’s R code (or your own—from a distant past), but upon downloading it, you realized that it relied on versions of either R or R libraries that are dramatically different from those on your own computer?","tags":["R","Programming","Docker"],"title":"Automating Computational Reproducibility in R using renv, Docker, and GitHub Actions","type":"post"},{"authors":["Nathaniel Haines"],"categories":null,"content":"","date":1623888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623888000,"objectID":"51e7c3261d7cd41e01c48347a952cf5f","permalink":"http://haines-lab.com/talk/the-role-of-reward-and-punishment-learning-in-externalizing-adolescents-a-joint-generative-model-of-traits-and-behavior/","publishdate":"2021-06-17T00:00:00Z","relpermalink":"/talk/the-role-of-reward-and-punishment-learning-in-externalizing-adolescents-a-joint-generative-model-of-traits-and-behavior/","section":"event","summary":"This presentation is part of MathPsych/ICCM 2021. See more via http://mathpsych.org/conferences/2021/","tags":[],"title":"The Role of Reward and Punishment Learning in Externalizing Adolescents: A Joint Generative Model of Traits and Behavior","type":"event"},{"authors":["Nathaniel Haines"],"categories":null,"content":"","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623715200,"objectID":"a3918527163605c68b6eb09ae6c8135b","permalink":"http://haines-lab.com/publication/haines_2021_diss/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/publication/haines_2021_diss/","section":"publication","summary":"Dissertation is done wooooo!","tags":["Source Themes"],"title":"Integrating Trait and Neurocognitive Mechanisms of Externalizing Psychopathology: A Joint Modeling Framework for Measuring Impulsive Behavior","type":"publication"},{"authors":["Nathaniel Haines"],"categories":null,"content":"","date":1617580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617580800,"objectID":"15b41fa41368f498730cc5b854e0a7d0","permalink":"http://haines-lab.com/talk/an-introduction-to-bayesian-data-analysis/","publishdate":"2021-04-05T00:00:00Z","relpermalink":"/talk/an-introduction-to-bayesian-data-analysis/","section":"event","summary":"This is just a fun tutorial talk I presented for the Cognitive Proseminar at The Ohio State University. Hope you enjoy!","tags":[],"title":"An Introduction to Bayesian Data Analysis","type":"event"},{"authors":null,"categories":["Statistical Modeling","Measurement Error","Formal Models"],"content":"  Introduction The Dunning-Kruger effect is perhaps one of the most well-known effects in all of social psychology, defined as the phenomenon wherein people with low “objective” skill tend to over-estimate their objective skill, whereas people with high objective skill tend to under-estimate their skill. In the popular media, the Dunning-Kruger effect is often summarized in a figure like the one below (source):\nCaption: The Dunning-Kruger Effect in Popular Media\n Despite the widespread use of figures such as the one above, the specific form of the effect is misleading–it suggests that people with low objective skill perceive their skill to be higher than those with the highest skill, which is not what Dunning and Kruger actually found in their original 1999 study. As clarified by many others (e.g., see here), Dunning and Kruger actually found the following:\nCaption: Original Data from Dunning \u0026amp; Kruger (1999)\n The basic pattern across all these different domains is consistent with our original definition of the effect–that people with low objective skill tend to over-estimate their skill to quite a large extent, yet those with high objective skill under-estimate their skill, but to a lesser extent. However, unlike the popularized interpretation, the relationship between objective and perceived skill appears to be monotonic, such that, on average, people with low skill still perceive themselves to be less skilled compared to high-skilled people.\nRegardless of misinterpretations in the popular media, the notoriety of the Dunning-Kruger effect cannot be understated. As of January 2021, the original study has been cited upwards of 6500 times on Google Scholar alone, and the basic Dunning-Kruger effect has been consistently replicated across samples, domains, and contexts (see here). In many ways, research on the Dunning-Kruger effect theorefore embodies the highest ideals of contemporary psychological science, passing the bar on replicability, generalizability, and robustness that psychological scientists have been striving for since the advent of the replication crisis (see also here).\nThere is just one small problem…\nWait, the Dunning-Kruger Effect is just Measurement Error? Perhaps surprisingly, the traditional methods used to measure the Dunning-Kruger effect have always been heavily scrutinized–even the original authors acknowledged potential issues with measurement error. Specifically, it is unclear to what extent that the classic effect arises due to an actual psychological bias, versus more mundain statistical measurement error resulting from regression to the mean. In fact, in their 1999 article, Dunning and Kruger explicitly recognize this possibility (p. 1124):\n “… the reader may point to the regression effect as an alternative interpretation of our results. … Because perceptions of ability are imperfectly correlated with actual ability, the regression effect virtually guarantees this result. … Despite the inevitability of the regression effect, we believe that the overestimation we observed was more psychological than artifactual. For one, if regression alone were to blame for our results, then the magnitude of miscalibration among the bottom quartile would be comparable with that of the top quartile. A glance at Figure 1 quickly disabuses one of this notion.”\n Although Dunning and Kruger were willing to state their belief that their results were attributable more so to psychological rather than statistical effects, they did not provide an explicit generative model for their pattern of results. Their reasoning appears sound (i.e. that differences in magnitude of mis-estimation for those under/over average indicates something beyond regression to the mean), but without a generative model, it is unclear how much faith we should place in the authors’ beliefs.\n Simulating the Dunning-Kruger Effect Fortunately, a number of simulation studies have since been conducted to show exactly how measurement error can generate data consistent with the Dunning-Kruger effect. Ackerman et al. (2002) was one of the first, which showed how the pattern found in Dunning and Kruger’s original study could arise from plotting any two variables with less than a perfect correlation (i.e. \\(r=1\\)) using the objective skill quantile versus perceived skill percentile convention from the original study. For example, if observed objective skill and perceived skill are correlated at \\(r=.19\\), using the standard plotting convention, Ackerman et al. (2002) obtained the following pattern:\nCaption: Figure 1 from Ackerman et al. (2002)\n The take-away from the above results is that any less-than-perfect correlation between two variables will produce the general pattern found in Dunning and Kruger’s original study, which results from a statistical phenomenon termed regression to the mean. If the concept of regression to the mean seems a bit magical (it certainly was for me when I was first introduced), it is useful to simulate data to get …","date":1610236800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610236800,"objectID":"561a6f345e222fe576ce2dfec06fa2b2","permalink":"http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/","publishdate":"2021-01-10T00:00:00Z","relpermalink":"/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/","section":"post","summary":"Introduction The Dunning-Kruger effect is perhaps one of the most well-known effects in all of social psychology, defined as the phenomenon wherein people with low “objective” skill tend to over-estimate their objective skill, whereas people with high objective skill tend to under-estimate their skill.","tags":["R","Bayesian Statistics"],"title":"A Series on Building Formal Models of Classic Psychological Effects: Part 1, the Dunning-Kruger Effect","type":"post"},{"authors":["Nathaniel Haines"],"categories":null,"content":"","date":1602633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602633600,"objectID":"0fc3736b4bf9cafe352267c894ac9814","permalink":"http://haines-lab.com/talk/generative-models-can-advance-the-social-behavioral-and-brain-sciences/","publishdate":"2020-10-14T00:00:00Z","relpermalink":"/talk/generative-models-can-advance-the-social-behavioral-and-brain-sciences/","section":"event","summary":"This talk was presented to the Neuroscience and Mental Health Group at the Institute of Cognitive Neuroscience, University College London, on October 14th, 2020. The talk is based on my recent paper on the role of generative models for social, behavioral, and brain sciences, linked in the video.","tags":[],"title":"Generative Models Can Advance the Social, Behavioral, and Brain Sciences","type":"event"},{"authors":["Woo-Young Ahn","Hairong Gu","Yitong Shen","Nathaniel Haines","Hunter Hahn","Julie Teater","Jay Myung","Mark Pitt"],"categories":null,"content":"","date":1595289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595289600,"objectID":"31ac12fc9465842aa87bb23e68a34fa0","permalink":"http://haines-lab.com/publication/ahn_2020/","publishdate":"2020-07-21T00:00:00Z","relpermalink":"/publication/ahn_2020/","section":"publication","summary":"We show how Bayesian adaptive design optimization can improve measurement precision in behavioral tasks.","tags":["Source Themes"],"title":"Rapid, precise, and reliable measurement of delay discounting using a Bayesian learning algorithm","type":"publication"},{"authors":["Nathaniel Haines"],"categories":null,"content":"","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"0380ebaffcdede382667a67b139897ed","permalink":"http://haines-lab.com/talk/using-decision-theory-to-model-anxiety-impulsivity-interactions-and-impulsive-behavior/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/talk/using-decision-theory-to-model-anxiety-impulsivity-interactions-and-impulsive-behavior/","section":"event","summary":"This presentation is part of MathPsych/ICCM 2020. See more via http://mathpsych.org/conferences/2020/","tags":[],"title":"Using Decision Theory to Model Anxiety-Impulsivity Interactions and Impulsive Behavior","type":"event"},{"authors":["Nathaniel Haines","Theodore Beauchaine"],"categories":null,"content":"","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594684800,"objectID":"50b4e04663da227624625f982059005e","permalink":"http://haines-lab.com/publication/haines_psychopathology_2020/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/publication/haines_psychopathology_2020/","section":"publication","summary":"We discuss how computational modeling in combination with Bayesian analysis can advance the study of psychopathology in ways not afforded by the traditional theoretical and measurement models used throughout clinical psychology.","tags":["Source Themes"],"title":"Moving beyond Ordinary Factor Analysis in Studies of Personality and Personality Disorder: A Computational Modeling Perspective","type":"publication"},{"authors":null,"categories":["Statistical Modeling","Measurement Error"],"content":"  Introduction In this post, we will explore how measurement error arising from imprecise parameter estimation can be corrected for. Specifically, we will explore the case where our goal is to estimate the correlation between a self-report and behavioral measure–a common situation throughout the social and behavioral sciences.\nFor example, as someone who studies impulsivity and externalizing psychopathology, I am often interested in whether self-reports of trait impulsivity (e.g., the Barratt Impulsiveness Scale) correlate with performance on tasks designed to measure impulsive behavior (e.g., the Balloon Analogue Risk task). In these cases, it is common for researchers to compute summed or averaged scores on the self-report measure (e.g., summing item responses and divding by the number of items) and use summary statistics of behavioral performance (e.g., percent risky choices) for each subject’s behavioral measure, wherein the resulting estimates are then entered into a secondary statistical model to make inference (e.g., correlation between self-reported trait and behavioral measures). Importantly, this two-stage approach to inference assumes that our summary measures both contain no measurement error, or alternatively that we have estimated these summary mesaures with perfect precision–a very strong assumption that is surely not met in practice.\nHere, we will explore how such assumptions can bias our statistical inferences on individual differences. As we will show, this bias arises because these two-stage approaches ignore important sources of measurement error. We will begin with an exploration of traditional methods developed within the context of classical test theory, and we will then transition to the use of more contemporary generative models. Throughout, we will explore relationships between classical and generative approaches, which are actually more similar than they are different in many ways.\n Classical Corrections Imprecision and Reliability At the level of a single measure, we can think of reliability as directly corresponding to precision–or how close our estimates are to the underlying “true score”. As our estimates become more precise at the individual-level, we should be able to better infer differences between individuals.\nFor example, assume that we are interested estimating a trait score for an individual, which we measure using a 10 item self-report questionnaire. For simplicity, let’s also assume that each item requires a yes/no endorse/not endorse response (coded 1 and 0, respectively), no items are reverse scored, and all items share the same difficulty. A typical approach to score this questionnaire would be to sum up the individual items \\(t\\) and divide the sum by the number of items \\(T\\) (i.e. taking the average). So, the vector of responses \\(\\textbf{x}\\) for a single subject may look something like:\n\\[\\textbf{x} = [1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\\]\nWe then compute our observed score like so:\n\\[X = \\frac{1}{T}\\sum^{T}_{t=1}\\textbf{x}_{t}\\]\nThe resulting observed score \\(X\\) is simply the proportion of yes/endorsed responses, which is \\(X = .7\\) in this example. We then interpret \\(X\\) as a quantitative measure of the underlying contruct. However, there are a few important, related questions worth asking regarding this estimate:\nHow close is our observed score measure \\(X\\) to the “true score” we aim to measure? Can we use this measurement approach to make inference on individual differences? If so, how can we best estimate the “true score” for each individual?  From the perspective of classical test theory, we can answer these questions by determining the reliability of our measure. Below, we will define reliability and discuss how it can be used to answer the questions above.\nDefining Reliability In psychology and education, reliability is often discussed within the context of classical test theory, which assumes that our estimates reflect some combination of the unobserved “true score” plus some error:\n\\[X = \\theta + \\epsilon\\] where \\(\\epsilon\\) is measurement error that contaminates the “true score” \\(\\theta\\). Note that there are different interpretations of what the truth actually is, but here we will use the following definition: the true score is the expected value of the observed score over many independent realizations (for alternative definitions of the true score, see Lord, Novick, \u0026amp; Birnbaum, 1968). Mathematically, we can represent this as:\n\\[\\theta = \\mathbb{E}[X]\\] Following this definition, the error for a single realization is then defined as:\n\\[\\epsilon = X - \\mathbb{E}[X]\\] where \\(\\mathbb{E}[\\epsilon] = 0\\) and \\(\\text{Cov}(\\epsilon, \\theta) = 0\\). In english, the expected error is 0, and the correlation/covariance between the error and true score is 0.\nGiven these assumptions, reliability is then defined as the squared correlation between the true score and observed score, or \\(\\rho^{2}_{X, \\theta}\\). Inuitively, \\(\\rho^{2}_{X, \\theta} \\rightarrow 1\\) as …","date":1592006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592006400,"objectID":"22a1493047bb09009be63aa4f15d86ea","permalink":"http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/","publishdate":"2020-06-13T00:00:00Z","relpermalink":"/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/","section":"post","summary":"Introduction In this post, we will explore how measurement error arising from imprecise parameter estimation can be corrected for. Specifically, we will explore the case where our goal is to estimate the correlation between a self-report and behavioral measure–a common situation throughout the social and behavioral sciences.","tags":["R","Bayesian Statistics"],"title":"On Curbing Your Measurement Error: From Classical Corrections to Generative Models","type":"post"},{"authors":["Nathaniel Haines","Theodore Beauchaine","Matthew Galdo","Andrew Rogers","Hunter Hahn","Mark Pitt","Jay Myung","Brandon Turner","Woo-Young Ahn"],"categories":null,"content":"","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"153d1d64b5d707f33be6c25bfce945f4","permalink":"http://haines-lab.com/publication/haines_cps_2020/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/publication/haines_cps_2020/","section":"publication","summary":"We used hierarchical Bayesian analysis to identify determine how impulsivity and anxiety jointly determine impulsive behavior in the context of a delay discounting task.","tags":["Source Themes"],"title":"Anxiety Modulates Preference for Immediate Rewards among Trait-Impulsive Individuals: A Hierarchical Bayesian Analysis","type":"publication"},{"authors":["Ricardo Romeu","Nathaniel Haines","W.-Y. Ahn","J. Busemeyer","J. Vassileva"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"82657745c6e3ca22d33424d5273d16b5","permalink":"http://haines-lab.com/publication/romeu_haines_dad_2019/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/romeu_haines_dad_2019/","section":"publication","summary":"We developed a novel model for the CGT and used it to investigate differences in decision mechanisms between groups of people with versus without substance use disorders.","tags":["Source Themes"],"title":"A computational model of the Cambridge gambling task with applications to substance use disorders","type":"publication"},{"authors":null,"categories":["Statistical Modeling","Measurement"],"content":"  The Reliability Paradox Defining Reliability In 2017, Hedge, Powell, and Sumner (2017) conducted a study to determine the reliability of a variety of of behavioral tasks. Reliability has many different meanings throughout the psychological literature, but what Hedge et al. were interested in was how well a behavioral measure consistently ranks individuals. In other words, when I have people perform a task and then measure their performance, does the measure that I use to summarize their behavior show high test-retest reliability? Those studying individual differences—including but not limited to personality psychologists, clinical psychologists, and many developmental psychologists—are likley to be familiar with test-retest reliability, as we often desire measures that help us understand differences between people.\nDespite what many of us may first think when we hear the word reliable, as Hedge et al. note, test-retest reliability is only one of many different definitions of reliable used throughout the psychological literature. For example, when someone states that an effect is reliable, they often mean that the effect is easily replicable.\nReplicable Behavioral Effects Aren’t Reliabile? A commonly cited example of a replicable effect is the Stroop effect, which is often measured as the mean difference in response time between incongruent and congruent word-color pairs. For example, when tasked with responding to the color of a word, people tend to respond much more quickly when the word is consistent with its color (“Red” colored red) compared to when it is not (“Red” colored blue). Since the original study in 1935, this basic effect of an average difference in response times between congruent and incongruent trials has been replicated countless times (see MacLeod, 1991), thereby becoming one of the most well-known effects in all of psychology. In fact, at the time of writing this post, the original 1935 paper has almost 20,000 citations on Google Scholar alone.\nDespite how replicable it is, Hedge et al. show that the Stroop effect is unreliable, in that it shows low test-retest reliability. While Hedge et al. assess many examples of such effects, we will focus on the Stroop effect throughout this post for brevity. Using their first experiment as an example, they had a a group of 50 college students complete a Stroop task two separate times separated by a period of three weeks. The Stroop task consisted of three conditions containing 240 trials each: (1) incongruent, (2) neutral, and (3) congruent conditions (for 720 trials total). On each trial, participants responded to the color of a word presented on a computer monitor which could be colored either red, blue, green, and yellow. Responses were collected from key presses. The word could be the same as the font color (congruent condition; e.g., “Red” colored red), unrelated to the font color (neutral; e.g., “lot” colored red), or conflicting with the font color (incongruent; e.g., “Blue” colored red).\nAfter subjects completed the above Stroop task at both timepoints, Hedge et al. used the following behavioral model to estimate each individual’s Stroop effect at both timepoints:\n\\[ \\begin{aligned} Stroop_{i,\\text{time}} \u0026amp; = \\overline{RT}_{i,\\text{time},\\text{incongruent}} - \\overline{RT}_{i,\\text{time}, \\text{congruent}} \\end{aligned}\\tag{1} \\] In plain English, for each person \\(i\\) at timepoint \\(\\text{time}\\), the Stroop effect is equal to the average response time across incongruent trials (\\(\\overline{RT}_{i,\\text{time},\\text{incongruent}}\\)) minus the average response time across congruent trials (\\(\\overline{RT}_{i,\\text{time},\\text{congruent}}\\)). Then, to estimate test-retest reliability, Hedge et al. use an Intraclass Correlation Coefficient (ICC) using a two-way random effects model for absolute agreement (\\(ICC(2,1)\\)). The \\(ICC(2,1)\\) is similar to a traditional Pearson’s \\(r\\), except that it is also sensitive to differences in the mean rather than just the variance of our observations. For example, if \\(A = \\{1,2,3\\}\\) and \\(B = \\{4,5,6\\}\\), the Pearson’s \\(r\\) between \\(A\\) and \\(B\\) is \\(1\\). However, the ICC(2,1) between these vectors is attenuated to \\(.18\\) because the corresponding elements of each vector differ on average by some value (3 in this example). This is important when our aim is to ensure that, for example, two different coders give the same ratings to a set of stimuli.\nImportantly, Hedge et al. found that the test-retest reliability of the Stroop effect as measured using equation 1 was \\(ICC(2,1) = .6\\), which is a surprisingly low number given how robust the Stroop effect is at the group level! As Hedge et al. discuss, with such low reliability, any research that correlates individual-level Stroop estimates with other measures (e.g., BOLD fMRI signals, personality measures, etc.) should be correcting the estimated correlations for the high measurement error of the Stroop effect, which substantially increases the …","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"d67976203ff03d456282999ac47087e5","permalink":"http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/","section":"post","summary":"The Reliability Paradox Defining Reliability In 2017, Hedge, Powell, and Sumner (2017) conducted a study to determine the reliability of a variety of of behavioral tasks. Reliability has many different meanings throughout the psychological literature, but what Hedge et al.","tags":["R","Bayesian Statistics"],"title":"Thinking generatively: Why do we use atheoretical statistical models to test substantive psychological theories?","type":"post"},{"authors":null,"categories":["Statistical Modeling"],"content":"        Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:\n\\[y = \\beta_{0}+X\\beta + \\epsilon\\tag{1}\\] where \\(\\epsilon\\) is the error, which is normally distributed as:\n\\[\\epsilon \\sim \\mathcal{N}(0, \\sigma)\\tag{2}\\] Unlike these traditional linear regression models, regularized linear regression models produce biased estimates for the \\(\\beta\\) weights. Specifically, both frequentist and Bayesian regularized linear regression models pool information across \\(\\beta\\) weights, resulting in regression toward a common mean. When the common mean is centered at 0, this pooling of information produces more conservative estimates for each \\(\\beta\\) weight (they are biased toward 0). In contrast, traditional linear regression models assume that \\(\\beta\\) weights share no group-level information (i.e. they are independent), which leads to so-called unbiased estimates.\nSo then, why are these models—which produce biased estimates—becoming increasingly popular throughout the social and behavioral sciences?\nLearning Objectives The current post seeks to show that we actually want biased estimates in many contexts. In doing so, we will also explore associations between frequentist and Bayesian regularization. Therefore, the learning objectives of the current post are to develop an understanding of:\nwhy so-called biased estimates are actually good for science, the difference between traditional and regularized linear regression models, and the correspondence between frequentist regularization and hierarchical Bayesian regression    Data For this post, we will use a college-admissions dataset that is freely available from Kaggle. Make sure to download the .csv file named Admission_Predict_Ver1.1.csv. This dataset contains 500 observations (i.e. rows) and 9 total variables (i.e. columns). 1 of these columns is a subject ID, which we will note use for modeling. Taken directly from the link:\n“The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : 1. GRE Scores ( out of 340 ) 2. TOEFL Scores ( out of 120 ) 3. University Rating ( out of 5 ) 4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) 5. Undergraduate GPA ( out of 10 ) 6. Research Experience ( either 0 or 1 ) 7. Chance of Admit ( ranging from 0 to 1 )”\nOur goal is to predict the likelihood of being admitted to graduate school (Chance of Admit), given the other variables, which we will now refer to as predictors.\nGetting Started First off, let’s load the libraries that we will use:\n# For traditional LASSO/Ridge regression library(glmnet) ## Warning: package \u0026#39;glmnet\u0026#39; was built under R version 4.1.1 # For Bayesian modeling library(rstan) # For data wrangling/plotting library(dplyr) library(tidyr) library(foreach) library(ggplot2) library(bayesplot) # Visualizing posteriors library(akima) # for 3D plotting ## Warning: package \u0026#39;akima\u0026#39; was built under R version 4.1.1 library(plotly) # for 3D plotting ## Warning: package \u0026#39;plotly\u0026#39; was built under R version 4.1.1 Next, we need to read in the data. Assuming that you have already downloaded the data from the link above, we can read it into R as follows:\n# I was lazy and just left this in the downloads folder... grad_dat \u0026lt;- read.csv(\u0026#34;~/Downloads/Admission_Predict_Ver1.1.csv\u0026#34;) # View first few observations tbl_df(grad_dat) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## # A tibble: 500 × 9 ## Serial.No. GRE.Score TOEFL.Score University.Rating SOP LOR CGPA Research ## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 337 118 4 4.5 4.5 9.65 1 ## 2 2 324 107 4 4 4.5 8.87 1 ## 3 3 316 104 3 3 3.5 8 1 ## 4 4 322 110 3 3.5 2.5 8.67 1 ## 5 5 314 103 2 2 3 8.21 0 ## 6 6 330 115 5 4.5 3 9.34 1 ## 7 7 321 109 3 3 4 8.2 1 ## 8 8 308 101 2 3 4 7.9 0 ## 9 9 302 102 1 2 1.5 8 0 ## 10 10 323 108 3 3.5 3 8.6 0 ## # … with 490 more rows, and 1 more variable: Chance.of.Admit \u0026lt;dbl\u0026gt;  Creating Training and Test Sets Before really looking at the data, we need to separate out the training and testing portions. The original competition version of data uploaded to Kaggle only included the first 400 observations, and competitors had to make predictions on the remaining 100 observations before the actual outcomes (i.e. likelihood of getting into graduate school) were released.\nTo show off the benefits of regularized over traditional methods, we will only train our models on the first 20 observations, and make predictions on the remaining 480. In these low data settings—which are common to many areas of social and behavioral science (e.g., psychology, …","date":1557100800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557100800,"objectID":"2993693cf47b8f747b7625e8212fca91","permalink":"http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/","publishdate":"2019-05-06T00:00:00Z","relpermalink":"/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/","section":"post","summary":"Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:","tags":["R","Machine Learning","Regularization","Bayesian Statistics"],"title":"On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression","type":"post"},{"authors":["Hunter Hahn","Samuel Kalnitsky","Nathaniel Haines","Sneha Thamotharan","Theodore Beauchaine"],"categories":null,"content":"","date":1554163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554163200,"objectID":"ae5958ad0716a29db3996e636b514aaa","permalink":"http://haines-lab.com/publication/hahn_2019/","publishdate":"2019-04-02T00:00:00Z","relpermalink":"/publication/hahn_2019/","section":"publication","summary":"We conducted an experiment with a sexual discounting task to investigate differences in willingness to have immediate, unprotected versus delayed, protected sex in people identifying as straight or gay—we also looked at how such effects were modulated by relationship status.","tags":["Source Themes"],"title":"Delay Discounting of Protected Sex: Relationship Type and Sexual Orientation Influence Sexual Risk Behavior","type":"publication"},{"authors":["Theodore Beauchaine","Nathaniel Haines"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"4200202ab9bec62de48b9b6e535b7041","permalink":"http://haines-lab.com/publication/beauchaine_haines_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/beauchaine_haines_2019/","section":"publication","summary":"We review different perspectives on emotion dysregulation, and its relationship to emotion regulation.","tags":["Source Themes"],"title":"Functionalist and Constructionist Perspectives on Emo­ tion Dysregulation","type":"publication"},{"authors":["Nathaniel Haines","Ziv Bell","Sheila Crowell","Hunter Hahn","Dana Kamara","Heather McDonough-Caplan","Tiffany Shader","Theodore Beauchaine"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"e8925a44bafacd1ae774bf7105d402b3","permalink":"http://haines-lab.com/publication/haines_dev_psych_2019/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/haines_dev_psych_2019/","section":"publication","summary":"We review the importance of facial expression recognition and perception acros development, and demonstrate how machine learning can help automate facial expression coding in research settings.","tags":["Source Themes"],"title":"Using automated computer vision and machine learning to code facial expressions of affect and arousal: Implications for emotion dysregulation research","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}   Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://haines-lab.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Nathaniel Haines","Matthew Southward","Jennifer Cheavens","Theodore Beauchaine","Woo-Young Ahn"],"categories":null,"content":"","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"50d5d4d1063895e6102f35183a74ceec","permalink":"http://haines-lab.com/publication/haines_plos_one_2019/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/publication/haines_plos_one_2019/","section":"publication","summary":"We use computer vision and machine learning model to detect positive and negative facial affect intensity with accuracy comparable to trained human coders.","tags":["Source Themes"],"title":"Using computer-vision and machine learning to automate facial coding of positive and negative affect intensity","type":"publication"},{"authors":["Nathaniel Haines","Jasmin Vassileva","Woo-Young Ahn"],"categories":null,"content":"","date":1538697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538697600,"objectID":"6d8fcd9dd597a55f2f966a245c75bbdc","permalink":"http://haines-lab.com/publication/haines_cog_sci_2018/","publishdate":"2018-10-05T00:00:00Z","relpermalink":"/publication/haines_cog_sci_2018/","section":"publication","summary":"We developed a novel computational model for the IGT and used it to investigate differences in learning mechanisms between groups with versus without substance use disorders.","tags":["Source Themes"],"title":"The Outcome‐Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task","type":"publication"},{"authors":null,"categories":["Reinforcement Learning"],"content":"  1. Goals of Paramter Estimation When estimating paramters for a given model, we typically aim to make an inference on an individual’s underlying decision process. We may be inferring a variety of different factors, such as the rate at which someone updates their expectations, the way that someone subjectively values an outcome, or the amount of exploration versus exploitation that someone engages in. Once we estimate an individual’s parameters, we can compare then to other people or even other groups of people. Further, we can compare parameters within subjects after an experimental manipulation (e.g., does drug X affect a person’s learning rate?).\nBelow, we will explore multiple paremter estimation methods. Specifically, we will use: (1) maximum likelihood estimation, (2) maximum a posteriori estimation, (3) and fully Bayesian estimation. First, we will simulate data from models described in the previous post on a simple 2-armed bandit task. Importantly, we will simulate data using known parameter values, which we will then try to recover from the simulated data. We will refer to the known paramters as the true parameters.\n 2. Simulation For our simulation, we will simulate choice from a model using delta-rule learning and softmax choice. To keep things simple, the learning rate will be the only free paramter in the model. Additionally, we will simulate choices in a task where there are two choices, where choice 1 has a mean payoff of 1 and choice 2 has a mean payoff of -1. Therefore, a learning agent should be able to learn that choice 1 is optimal and make selections accordingly. However, we will add noise to each choice payoff (sigma below) to make things more realistic.\nThe following R code simulates 200 trials using the model and task described above:\n# For pretty plots and data manipulation library(ggplot2) library(foreach) library(dplyr) # Simulation parameters set.seed(1) # Random seed for replication mu \u0026lt;- c(1, -1) # Mean payoff for choices 1 and 2 sigma \u0026lt;- 3 # SD of payoff distributions n_tr \u0026lt;- 200 # Number of trials beta \u0026lt;- 0.1 # True learning rate # Initial expected value ev \u0026lt;- c(0, 0) # Softmax choice function logsumexp \u0026lt;- function (x) { y \u0026lt;- max(x) y + log(sum(exp(x - y))) } softmax \u0026lt;- function (x) { exp(x - logsumexp(x)) } # Simulate data sim_dat \u0026lt;- foreach(t=1:n_tr, .combine = \u0026#34;rbind\u0026#34;) %do% { # Generate choice probability with softmax pr \u0026lt;- softmax(ev) # Use choice probability to sample choice choice \u0026lt;- sample(c(1,2), size = 1, prob = pr) # Generate outcome based on choice outcome \u0026lt;- rnorm(1, mean = mu[choice], sd = sigma) # Delta-rule learning ev[choice] \u0026lt;- ev[choice] + beta * (outcome - ev[choice]) # Save data data.frame(Trial = rep(t, 2), EV = ev, Pr = pr, Option = paste(1:2), Choice = rep(choice, 2), Outcome = rep(outcome, 2)) } # Change in expected values across tirals ggplot(sim_dat, aes(x = Trial, y = EV, geom = \u0026#34;line\u0026#34;, color = Option)) + geom_line() + scale_color_manual(values = c(\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;)) + ylab(\u0026#34;Expected Value\u0026#34;) + theme_minimal(base_size = 20) The above graph shows the simulated agent’s expected value (EV) for options 1 and 2 across the 200 trials. We can also view the probability of selecting each option across trials:\n# Change in probability of selecting each option across tirals ggplot(sim_dat, aes(x = Trial, y = Pr, geom = \u0026#34;line\u0026#34;, color = Option)) + geom_line() + scale_color_manual(values = c(\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;)) + ylab(\u0026#34;Pr(Choice)\u0026#34;) + theme_minimal(base_size = 20) Clearly, the agent learns to prefer option 1 over option 2 across trials. Although we know the true learning rate (i.e. \\(\\beta_{true} = 0.1\\)), we will explore the various parameter estimation techniques below to try and recover \\(\\beta_{true}\\) from the simulated data.\n 3. Parameter Estimation Methods 3.1 Maximum Likelihood The goal of maximum likelihood estimation (MLE) is to identify the single, most likely parameter value(s) that could have produced the observed data. For our purposes, MLE will allow us to estimate the learning rate that maximizes the probability of observing the simulated data. We refer to his estimate as \\(\\hat{\\beta}\\) (pronounced beta-hat).\nBefore moving on, it is worth introducing some new notation. If we refer to the observed data as \\(X\\) and the parameters we aim to estimate as \\(\\theta\\), we can refer to the likelihood function as:\n\\[Pr(X|\\theta)\\]\nIn our case, \\(\\theta = \\hat{\\beta}\\), and \\(X\\) is the vector of simulated choices from above. So then, how do we actually compute the probability of choices \\(X\\) given learning rate \\(\\hat{\\beta}\\)? Simple! We use the model that we simulated the data from. Specifically, we:\nMake a guess for \\(\\hat{\\beta}\\) Look into \\(X\\) and find out what choice and outcome the agent made/experienced on trial \\(t\\) Use our guess for \\(\\hat{\\beta}\\) to update the EV for the chosen option accoring to the model Enter the updated EVs into the softmax function to generate the probability of selecting each option for the next trial Store the …","date":1536364800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536364800,"objectID":"c8e05639896d413a88cb46e44e365b28","permalink":"http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3/","publishdate":"2018-09-08T00:00:00Z","relpermalink":"/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3/","section":"post","summary":"1. Goals of Paramter Estimation When estimating paramters for a given model, we typically aim to make an inference on an individual’s underlying decision process. We may be inferring a variety of different factors, such as the rate at which someone updates their expectations, the way that someone subjectively values an outcome, or the amount of exploration versus exploitation that someone engages in.","tags":["R","Modeling","Learning"],"title":"Human Choice and Reinforcement Learning (3)","type":"post"},{"authors":["Woo-Young Ahn","Paul Hendricks","Nathaniel Haines"],"categories":null,"content":"","date":1509408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509408000,"objectID":"c65921c432d83c61aaa4a5de18e1d2c5","permalink":"http://haines-lab.com/publication/ahn_2017/","publishdate":"2017-10-31T00:00:00Z","relpermalink":"/publication/ahn_2017/","section":"publication","summary":"We developed an open-source R/Python toolbox for easily fitting popular supervised learning models.","tags":["Source Themes"],"title":"Easyml: Easily Build And Evaluate Machine Learning Models","type":"publication"},{"authors":["Andrew Rogers","Ilana Seager","Nathaniel Haines","Hunter Hahn","Amelia Aldeo","Woo-Young Ahn"],"categories":null,"content":"","date":1508889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508889600,"objectID":"5ea242bcf2d8eb39867fd23be5a5f38f","permalink":"http://haines-lab.com/publication/rogers_2017/","publishdate":"2017-10-25T00:00:00Z","relpermalink":"/publication/rogers_2017/","section":"publication","summary":"We investigated the role of emotion regulation in how minority stress expresses itself in people identifying as lesbian, gay, or bisexual.","tags":["Source Themes"],"title":"The Indirect Effect of Emotion Regulation on Minority Stress and Problematic Substance Use in Lesbian, Gay, and Bisexual Individuals","type":"publication"},{"authors":null,"categories":["Reinforcement Learning"],"content":"  1. Answer to post 1 In the previous post, I reviewed the Rescorla-Wagner updating (Delta) rule and its contemporary instantiation. At the end, I asked the following question:\n How should you change the learning rate so that the expected win rate is always the average of all past outcomes?  We will go over the answer to this question before progressing to the use of the Delta rule in modeling human choice. To begin, refer back to the Delta rule written in the following form:\n\\[Pr(win)_{t+1} = (1 - \\beta) \\cdot Pr(win)_{t} + \\beta \\cdot \\lambda_{t}\\]\nHere, we see that in the Delta rule the expected win probability for the next trial is equal to the exponentially weighted moving average of the past expectation and the current outcome. It is easy to show this through a visualization of the expectation over time. For example, imagine that we have a vector of outcomes \\(\\lambda = [1,0,0,1,1,1,0,1,1,1]\\), where 0 and 1 represent losing and winning slot machine rolls, respectively. Note that in this example, the placement of these outcomes within the vector \\(\\lambda\\) indicates their temporal order (i.e. \\(\\lambda_{1}=1\\), \\(\\lambda_{2}=0\\), etc.). Now, if we set an arbitrary learning rate such as \\(\\beta = 0.05\\), what is the expected win rate after iterating through outcomes \\(\\lambda\\)? The R code below demonstrates the use of the Delta rule and an alternative exponential weighting scheme–which takes the form of a power series–to determine the expectation on each trial:\n# For pretty plots library(ggplot2) # Assign lambda (lambda[1] == first trial) lambda \u0026lt;- c(1,0,0,1,1,1,0,1,1,1) # Set learning rate beta \u0026lt;- 0.05 ### Iterative prediction error (Delta rule) approach ### # Function that iterates the Rescorla-Wagner rule # This function is slightly modified from the last post # to ensure that that final expectation is stored rw_update \u0026lt;- function(lambda, beta, init) { # To store expected value for each trial Pr_win \u0026lt;- vector(length=length(lambda)+1) # Set initial value Pr_win[1] \u0026lt;- init for (t in 1:(length(lambda))) { Pr_win[t+1] \u0026lt;- Pr_win[t] + beta * (lambda[t] - Pr_win[t]) } return(Pr_win) } # With initial expectation = 0, iterate Delta rule delta_results \u0026lt;- rw_update(lambda = lambda, beta = beta, init = 0)[-1] # ^ # Remove initial value (0) ### Power series approach ### # Direct exponential weighting (saving all expectations) power_ser \u0026lt;- NULL for (i in 1:10) { power_ser[i] \u0026lt;- beta * sum((1-beta)^(0:(i-1))*lambda[i:1]) } ### Comparison of both approaches ### all(round(delta_results, 8) == round(power_ser, 8)) ## [1] TRUE # data.frame for ggplot all_data \u0026lt;- stack(data.frame(delta = delta_results, pow_s = power_ser)) # Add trial all_data[[\u0026#34;trial\u0026#34;]] \u0026lt;- rep(1:10, 2) names(all_data)[2] \u0026lt;- \u0026#34;Approach\u0026#34; # Visually p \u0026lt;- ggplot(all_data, aes(x = trial, y = values, color = Approach)) + geom_line() + facet_grid(facets = \u0026#34;Approach ~ .\u0026#34;) + ggtitle(\u0026#34;Comparison of approaches\u0026#34;) + xlab(\u0026#34;Trial Number\u0026#34;) + ylab(\u0026#34;Expected Win Probability\u0026#34;) p As you can see in the plots, both the Delta rule and the power series approach yield the same exact expectations when iterated for each trial. However, the power series form requires each past observation while the Delta rule only requires the last expectation–this feature makes the Delta rule form of the equation much more plausible as a processes that people may use to estimate the value of a choice. This is because the computational cost does not increase with the number of past observations.\nThrough this example, those familiar with economics or signal processing may find the Delta rule familiar. Essentially, we can think of the Delta rule as a smoothing function or a high- or low-pass filter–albeit in the time as opposed to frequency domain–which effectively attenuates the effect of past or current outcomes, respectively. What makes this specific form interesting is again the fact that it can be iterated (i.e. it is recursive), making it a realistic approximation to the computations performed by the brain when estimating some value.\nWith the above intuitions in mind, we will now get back to the question. How do we change the learning rate to ensure that the current expectation is always the simple average of all past outcomes? Since the above example showed that the Delta rule is really just a moving average where past outcomes are given exponentially decreasing weights, our goal is to make all outcomes equally represented. In other words, we want to weight past and current outcomes equally–this is a cumulative moving average. Using our slot machine example, the cumulative moving average formula (in its most common form) is written as follows:\n\\[Pr(win)_{t+1} = \\frac{\\lambda_{t} + (t-1) \\cdot Pr(win)_{t}}{t}\\]\nWe can re-write the above equation into one that is more familiar to us:\n\\[Pr(win)_{t+1} = (1-\\frac{1}{t}) \\cdot Pr(win)_{t} + \\frac{1}{t} \\cdot \\lambda_{t} \\]\nIn the above form, you can see that the cumulative moving average can be computed using the Delta rule by setting the …","date":1491523200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491523200,"objectID":"3719b8159bedc37663192cc2de4807fe","permalink":"http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/","publishdate":"2017-04-07T00:00:00Z","relpermalink":"/post/2017-04-07-human-choice-and-reinforcement-learning-2/","section":"post","summary":"1. Answer to post 1 In the previous post, I reviewed the Rescorla-Wagner updating (Delta) rule and its contemporary instantiation. At the end, I asked the following question:","tags":["R","Modeling","Learning"],"title":"Human Choice and Reinforcement Learning (2)","type":"post"},{"authors":null,"categories":["Reinforcement Learning"],"content":"  1. Short history In 1972, Robert Rescorla and Allan Wagner developed a formal theory of associative learning, the process through which multiple stimuli are associated with one-another. The most widely used example (Fig. 1) of associative learning comes straight from Psychology 101–Pavlov’s dog.\nFigure 1\n The idea is simple, and it’s something that we experience quite often in everyday life. In the same way that Pavlov’s dog begins to drool after hearing a bell, certain cognitive and/or biological processes are triggered when we are exposed to stimuli that we have been exposed to in the past. But how can this learning process be modeled? That is to say, what sort of equation can we use to describe how an agent learns to associate multiple stimuli? To answer these questions, Rescorla and Wagner developed what is now know as the Rescorla-Wagner updating rule:\n\\[\\Delta V_{A} = \\alpha_{A} \\beta_{1} (\\lambda_{1} - V_{AX})\\]\n\\[\\Delta V_{X} = \\alpha_{X} \\beta_{1} (\\lambda_{1} - V_{AX})\\]\nFirst off, note that the original Rescorla-Wagner rule was developed to explain compound stimuli (e.g. presentation of a bell and light, followed by food). Here, \\(\\Delta V_{A}\\) is the change in associative strength between stimulus \\(A\\) (e.g. the bell) and the response (e.g. food). \\(\\Delta V_{X}\\) has the same interpretation, but refers to stimulus \\(X\\) (e.g. the light).\n\\(0 \\leq \\beta_{1} \\leq 1\\) is a free parameter (i.e. we estimate it from the data) referred to as the learning rate. The learning rate controls how quickly updating takes place, where values near 0 and 1 reflect sluggish and rapid learning, respectively. Above, the learning rate is shared across stimuli.\n\\(0 \\leq \\alpha_{A} \\leq 1\\) is a free parameter which is determined by the salience of stimulus \\(A\\), and \\(0 \\leq \\alpha_{X} \\leq 1\\) for stimulus \\(X\\). Unlike the learning rate, which is shared across stimuli, the salience parameter is specific to each stimulus. Put simply, this just means that learning can occur at different rates depending on the type of stimulus (e.g. I may associate a light with food more quickly than a tone).\n\\(\\lambda_{1}\\) is described as “the asymptote of associative strength”. This is the upper-limit on how strong the association strength can be. In this way, \\(\\lambda_{1}\\) reflects the value being updated toward by the learning rate (\\(\\beta\\)) and stimulus salience (\\(\\alpha\\)).\nLastly, \\(V_{AX}\\) is the total associative strength of the compound stimulus \\(AX\\). Rescorla and Wagner assume that this is a simple sum of both stimuli strengths:\n\\[V_{AX} = V_{A} + V_{X}\\]\nInterpretation of this model is actually quite simple–we update associative strength (\\(V_{*}\\)) for each stimulus by taking steps (\\(\\alpha \\beta\\)) toward the difference between the asymptote of learning (\\(\\lambda_{1}\\)) and the current associative strength of the compund stimulus (\\(V_{AX}\\)). By continually exposing an agent to a tone or bell paired with a reward (or punishment), the agent learns the associative strength of the conditioned and unconditioned stimuli.\nWhile the original Rescorla-Wagner model was successful for explaining associative learning for classical conditioning paradigms, what of operant conditioning? What if we are interested in how people learn to make decisions? In most current research, we are not interested in knowing how people learn to associate lights or tones with some reward. Instead, we would like a model that can describe how people learn to select the best choice among multiple choices. This model would need to explain how people assign values to multiple options as well as how they decide which option to choose. In statistical terms, we want to know how people solve the multi-armed bandit problem. In the following section, we will begin to solve this problem.\n 2. Current Implementations Figure 1\n As a motivating example, we will explore the simple problem of learning the probability that a slot machine will payoff (i.e. that you will win any amount after pulling the lever). To do so, the above equations only need minor modifications. Additionally, we will change the terminology–instead of learning an associative strength, we will now be learning the probability of a winning outcome. To start, we take the first equation above and write it for a single stimulus \\(V\\), but exchange \\(V\\) with the probability of observing a win:\n\\[\\Delta Pr(win) = \\alpha \\beta (\\lambda - Pr(win))\\]\nBecause \\(\\Delta Pr(win) = Pr(win)_{t+1} - Pr(win)_{t}\\) (where \\(t\\) is the current trial), we can re-write the above equation into an iterative form:\n\\[Pr(win)_{t+1} = Pr(win)_{t} + \\alpha \\beta (\\lambda_{t} - Pr(win)_{t})\\]\nSince we are using this model to explain how people learn the probability of a binary outcome, \\(\\lambda_{t} \\in [0, 1]\\) now represents the outcome of slot machine roll on trial \\(t\\). Now, we drop the \\(\\alpha\\) parameter to simplify the model further. Because we have a single choice option, estimating both \\(\\alpha\\) …","date":1491264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491264000,"objectID":"c3ff2688dc3b49b90f28e586a49ccaf6","permalink":"http://haines-lab.com/post/2017-04-04-choice_rl_1/","publishdate":"2017-04-04T00:00:00Z","relpermalink":"/post/2017-04-04-choice_rl_1/","section":"post","summary":"1. Short history In 1972, Robert Rescorla and Allan Wagner developed a formal theory of associative learning, the process through which multiple stimuli are associated with one-another. The most widely used example (Fig.","tags":["R","Modeling","Learning"],"title":"Human Choice and Reinforcement Learning (1)","type":"post"},{"authors":["Woo-Young Ahn","Nathaniel Haines","Lei Zhang"],"categories":null,"content":"","date":1488758400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488758400,"objectID":"b5673ad0683366d7db775c5afd7b3314","permalink":"http://haines-lab.com/publication/ahn_haines_cpsy_2017/","publishdate":"2017-03-06T00:00:00Z","relpermalink":"/publication/ahn_haines_cpsy_2017/","section":"publication","summary":"We developed an open-source R/Python package for fitting reinforcement learning and decision-making models using hierarchical Bayesian analysis.","tags":["Source Themes"],"title":"Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"http://haines-lab.com/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]