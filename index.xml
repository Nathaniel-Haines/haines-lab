<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computational Psychology</title>
    <link>http://haines-lab.com/</link>
      <atom:link href="http://haines-lab.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Computational Psychology</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Nathaniel Haines, PhD</copyright><lastBuildDate>Mon, 24 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://haines-lab.com/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_512x512_fill_lanczos_center_3.png</url>
      <title>Computational Psychology</title>
      <link>http://haines-lab.com/</link>
    </image>
    
    <item>
      <title>Automating Computational Reproducibility in R using renv, Docker, and GitHub Actions</title>
      <link>http://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;Have you ever wanted to run someone else’s R code (or your own—from a distant past), but upon downloading it, you realized that it relied on versions of either R or R libraries that are dramatically different from those on your own computer? This situation can easily spiral into &lt;a href=&#34;https://en.wikipedia.org/wiki/Dependency_hell&#34;&gt;&lt;strong&gt;&lt;em&gt;dependency hell&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;, a place that every researcher or data scientist will (always unintentionally) find themselves when trying to get random code from bygone days to run locally. Oftentimes, it may seem impossible to resolve The Problem, and giving up may seem like the only logical thing to do…&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://imgs.xkcd.com/comics/dependencies.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Occasionally, dependency hell peak’s its head into the real world too&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-naive-attempt-to-resolve-the-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Naive Attempt to Resolve The Problem&lt;/h1&gt;
&lt;p&gt;If you are like me, the first thing you might do when finding yourself in dependency hell is to try installing different versions of the R libraries that R is complaining about. If that doesn’t work, you may even get desperate enough to try installing a different version of R in hopes that The Problem will magically disappear.&lt;/p&gt;
&lt;p&gt;In some cases, the naive attempt &lt;em&gt;can&lt;/em&gt; succeed. But alas, it will often fail, and even when it does succeed—that different version of R (or R libraries) you installed could lead to problems in other ongoing projects. You know, the projects that you briefly forgot about while trying to resolve The Problem for the current project on the to-do list. In this case, dependency hell has evolved into &lt;a href=&#34;https://en.wikipedia.org/wiki/Whac-A-Mole#Colloquial_usage&#34;&gt;&lt;strong&gt;&lt;em&gt;computational whac-a-mole&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;—a rage-inducing game that your therapist told you to stop playing many sessions ago.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/whac-a-mole.jpeg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: You, disregarding your therapist&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-slightly-better-attempt-but-it-requires-foresight&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Slightly Better Attempt, But It Requires Foresight&lt;/h1&gt;
&lt;p&gt;If you truly want to resolve dependency issues, you will need to think ahead and build some form of dependency management into your workflow (I know, very unfortunate…). That said, it is getting easier with each passing year to automate a good deal of it. In the remainder of this post, we will walk through how to create a GitHub repository that takes advantage of &lt;a href=&#34;https://rstudio.github.io/renv/articles/renv.html&#34;&gt;&lt;code&gt;renv&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://www.docker.com/&#34;&gt;&lt;code&gt;Docker&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://github.com/features/actions&#34;&gt;&lt;code&gt;GitHub Actions&lt;/code&gt;&lt;/a&gt; to make your R workflow reproducible into the future.&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;NOTE&lt;/strong&gt;: Throughout this post, you can refer to &lt;a href=&#34;https://github.com/Nathaniel-Haines/computational-reproducibility-in-r&#34;&gt;my accompanying GitHub repo&lt;/a&gt; to see all the code in one place. I would recommend forking it, cloning to a local directory, and testing things out that way. Then, you can try out some of your own R scripts to see how things work!)&lt;/p&gt;
&lt;div id=&#34;an-overview-of-renv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An Overview of &lt;code&gt;renv&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;renv&lt;/code&gt; is an R package that allows for us to install &lt;strong&gt;&lt;em&gt;project-specific packages&lt;/em&gt;&lt;/strong&gt; as opposed to relying on the same set of packages (and their associated versions) across all R projects. Further, we can take a &lt;strong&gt;snapshot&lt;/strong&gt; of the packages we are using in a given project, save the snapshot for later, and then &lt;strong&gt;restore&lt;/strong&gt; (i.e. reinstall) the packages later—even on different computers.&lt;/p&gt;
&lt;div id=&#34;starting-a-new-project-with-renv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Starting a New Project with &lt;code&gt;renv&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Using &lt;code&gt;renv&lt;/code&gt; is very easy. First, while in R, set your working directory to the folder that we want to create our project in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# This is the GitHub repo associated with this project
setwd(&amp;quot;~/computational-reproducibility-in-r&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, install &lt;code&gt;renv&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;renv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, we simply load the package and then initialize the project:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;renv::init()

# * Initializing project ...
# * Discovering package dependencies ... Done!
# * Copying packages into the cache ... Done!
# * Lockfile written to # &amp;#39;~/computational-reproducibility-in-r/renv.lock&amp;#39;.

# Restarting R session...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;installing-packages-with-renv&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Installing Packages with &lt;code&gt;renv&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;With the project initialized, we essentially proceed as normal, except that we will install all the packages we need along the way. For example, if our project uses &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;, we install them as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(c(&amp;quot;dplyr&amp;quot;, &amp;quot;ggplot2&amp;quot;))

# Installing dplyr [1.0.7] ...
#   OK [linked cache]
# Installing ggplot2 [3.3.5] ...
#   OK [linked cache]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, both packages are available to use in our project. Let’s make a plot for proof!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

set.seed(43210)

# Yes, your love grows exponentially
love_for_renv &amp;lt;- rnorm(100, exp(.022*(1:100)), .8)

data.frame(Time = 1:100,
           love_for_renv = love_for_renv) %&amp;gt;%
  ggplot(aes(x = Time, y = love_for_renv)) +
  geom_point() +
  geom_line() +
  ylab(&amp;quot;Love for renv&amp;quot;) +
  scale_y_continuous(breaks = 1:10, limits = c(0,11)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2022-01-23-automating-computational-reproducibility-with-r-using-renv-docker-and-github-actions/index.en_files/figure-html/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now, let’s say that we are wrapping up for the day and would like to push our changes to GitHub. After saving our &lt;code&gt;R&lt;/code&gt; file that produces the beautiful graph above, we need to take a snapshot so that &lt;code&gt;renv&lt;/code&gt; remembers which libraries we have installed for the current project:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;renv::snapshot()

# The following package(s) will be updated in the lockfile:
# 
# # CRAN ===============================
# - MASS           [* -&amp;gt; 7.3-54]
# - Matrix         [* -&amp;gt; 1.3-3]
# - R6             [* -&amp;gt; 2.5.1]
# - RColorBrewer   [* -&amp;gt; 1.1-2]
# - cli            [* -&amp;gt; 3.0.1]
# - colorspace     [* -&amp;gt; 2.0-2]
# - crayon         [* -&amp;gt; 1.4.1]
# - digest         [* -&amp;gt; 0.6.29]
# - dplyr          [* -&amp;gt; 1.0.7]
# - ellipsis       [* -&amp;gt; 0.3.2]
# - fansi          [* -&amp;gt; 0.5.0]
# - farver         [* -&amp;gt; 2.1.0]
# - generics       [* -&amp;gt; 0.1.0]
# - ggplot2        [* -&amp;gt; 3.3.5]
# - glue           [* -&amp;gt; 1.6.0]
# - gtable         [* -&amp;gt; 0.3.0]
# - isoband        [* -&amp;gt; 0.2.5]
# - labeling       [* -&amp;gt; 0.4.2]
# - lattice        [* -&amp;gt; 0.20-44]
# - lifecycle      [* -&amp;gt; 1.0.0]
# - magrittr       [* -&amp;gt; 2.0.1]
# - mgcv           [* -&amp;gt; 1.8-35]
# - munsell        [* -&amp;gt; 0.5.0]
# - nlme           [* -&amp;gt; 3.1-152]
# - pillar         [* -&amp;gt; 1.6.2]
# - pkgconfig      [* -&amp;gt; 2.0.3]
# - purrr          [* -&amp;gt; 0.3.4]
# - rlang          [* -&amp;gt; 0.4.12]
# - scales         [* -&amp;gt; 1.1.1]
# - tibble         [* -&amp;gt; 3.1.4]
# - tidyselect     [* -&amp;gt; 1.1.1]
# - utf8           [* -&amp;gt; 1.2.2]
# - vctrs          [* -&amp;gt; 0.3.8]
# - viridisLite    [* -&amp;gt; 0.4.0]
# - withr          [* -&amp;gt; 2.4.2]
# 
# Do you want to proceed? [y/N]: y
# * Lockfile written to 
# &amp;#39;~/computational-reproducibility-in-r/renv.lock&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s a lot of dependencies for just two &lt;code&gt;R&lt;/code&gt; packages… which is exactly why we want to use something like &lt;code&gt;renv&lt;/code&gt; to automate version tracking for us! Note that &lt;code&gt;renv&lt;/code&gt; searches through the files in our project directory to determine which packages we are using, so make sure that you have a file with your &lt;code&gt;R&lt;/code&gt; code saved somewhere in your project directory before taking the snapshot.&lt;/p&gt;
&lt;p&gt;If you look through your project directory, you will notice that &lt;code&gt;renv&lt;/code&gt; has created a few different files. Of these, the &lt;code&gt;renv.lock&lt;/code&gt; file is what tracks the packages and their associated versions that your project depends on. Also, you will see a line in your &lt;code&gt;.Rprofile&lt;/code&gt; file (which may be hidden, in which case you will need to show the hidden files to see it; e.g., in a terminal, run &lt;code&gt;ls -la&lt;/code&gt;) that looks like: &lt;code&gt;source(&#34;renv/activate.R&#34;)&lt;/code&gt;. If you did not know, this file is run each time &lt;code&gt;R&lt;/code&gt; is started from your project directory. In this case, it activates &lt;code&gt;renv&lt;/code&gt; automatically for you, making all the packages available for the current project. This way, if you were to open your project on another computer, you would have all the correct packages/versions ready to go. Simply running &lt;code&gt;renv::restore()&lt;/code&gt; will re-install all of them, after which you can proceed as normal. Pretty cool, right?&lt;/p&gt;
&lt;p&gt;And that is pretty much it for &lt;code&gt;renv&lt;/code&gt;! However, you may have recognized a problem—even if we have the right package versions, what if we want to open our project on a computer with a different version of &lt;code&gt;R&lt;/code&gt;? Or, what if we share our project with someone on a different operating system? In these cases, just having the correct versions for our packages is not enough—we need the correct &lt;code&gt;R&lt;/code&gt; version too. This is where &lt;code&gt;Docker&lt;/code&gt; comes in…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;packaging-up-your-r-project-with-docker&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Packaging up your R Project with Docker&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Docker&lt;/code&gt; is important because it allows us to package up not only our packages, but also our &lt;code&gt;R&lt;/code&gt; version and even our operating system! In this way, creating a &lt;strong&gt;&lt;em&gt;Docker image&lt;/em&gt;&lt;/strong&gt; will ensure that (almost) any computer with &lt;code&gt;Docker&lt;/code&gt; installed will be able to run our code. This is particularly useful when we want to run computationally intensive code on cloud services such as &lt;a href=&#34;https://aws.amazon.com/&#34;&gt;AWS&lt;/a&gt;, &lt;a href=&#34;https://cloud.google.com/&#34;&gt;GCP&lt;/a&gt;, or others.&lt;/p&gt;
&lt;div id=&#34;the-basic-steps-in-building-a-docker-image&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Basic Steps in Building a Docker Image&lt;/h3&gt;
&lt;p&gt;After installing &lt;code&gt;Docker&lt;/code&gt; (&lt;a href=&#34;https://docs.docker.com/get-docker/&#34;&gt;see instructions here&lt;/a&gt;), building an image is rather straightforward—at least, it is with a bit of practice! There are basically three key steps involved in building a &lt;code&gt;Docker&lt;/code&gt; image:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Specifying the starting image to build off of,&lt;/li&gt;
&lt;li&gt;Installing the necessary dependencies/packages that you want to use, and&lt;/li&gt;
&lt;li&gt;Telling the image how to start when you run it&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For us, we want our image to start using some basic version of &lt;code&gt;R&lt;/code&gt; that matches the version we have on our computer when developing a project. Then, we want to install all the &lt;code&gt;R&lt;/code&gt; packages that are specified in the &lt;code&gt;renv.lock&lt;/code&gt; file described earlier. Finally, we have a choice to either make the image start up &lt;code&gt;R&lt;/code&gt; right away, or we can alternatively start from a terminal. I like to start from the terminal, personally, but this is just preference.&lt;/p&gt;
&lt;p&gt;To follow the above steps, we can first determine our &lt;code&gt;R&lt;/code&gt; version by running &lt;code&gt;R --version&lt;/code&gt; in the terminal. In my case, I currently have version &lt;code&gt;4.1.0&lt;/code&gt; installed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R version 4.1.0 (2021-05-18) -- &amp;quot;Camp Pontanezen&amp;quot;
# Copyright (C) 2021 The R Foundation for Statistical Computing
# Platform: aarch64-apple-darwin20 (64-bit)
# 
# R is free software and comes with ABSOLUTELY NO WARRANTY.
# You are welcome to redistribute it under the terms of the
# GNU General Public License versions 2 or 3.
# For more information about these matters see
# https://www.gnu.org/licenses/.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need to know our version of &lt;code&gt;renv&lt;/code&gt; too. For this, you can just look at the &lt;code&gt;renv.lock&lt;/code&gt; file and search for &lt;code&gt;renv&lt;/code&gt;. With these two pieces of information, we can create a &lt;code&gt;Dockerfile&lt;/code&gt; (a raw text file named “Dockerfile” with no file extensions, located in your project root directory), which is what is used to build a &lt;code&gt;Docker&lt;/code&gt; image. Here is the &lt;code&gt;Dockerfile&lt;/code&gt; that I use for the current example project:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Start with R version 4.1.0
FROM rocker/r-ver:4.1.0

#RUN apt-get update &amp;amp;&amp;amp; apt-get install -y libcurl4-openssl-dev libssl-dev

# Use renv version 0.15.2
ENV RENV_VERSION 0.15.2

# Install renv
RUN Rscript -e &amp;quot;install.packages(&amp;#39;remotes&amp;#39;, repos = c(CRAN = &amp;#39;https://cloud.r-project.org&amp;#39;))&amp;quot;
RUN Rscript -e &amp;quot;remotes::install_github(&amp;#39;rstudio/renv@${RENV_VERSION}&amp;#39;)&amp;quot;

# Create a directory named after our project directory
WORKDIR /computational-reproducibility-in-r

# Install all R packages specified in renv.lock
RUN Rscript -e &amp;#39;renv::restore()&amp;#39;

# Default to bash terminal when running docker image
CMD [&amp;quot;bash&amp;quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The comments above detail what is going on—for you, you can make this work for your project by simply changing the &lt;code&gt;R&lt;/code&gt; and &lt;code&gt;renv&lt;/code&gt; version numbers to the values for your own project.&lt;/p&gt;
&lt;p&gt;To actually build the image so that we can run it, we can do so in the terminal using the following code:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker build -t my-docker-image .

# [+] Building 150.2s (11/11) FINISHED                                           
#  =&amp;gt; [internal] load build definition from Dockerfile                       0.0s
#  =&amp;gt; =&amp;gt; transferring dockerfile: 694B                                       0.0s
#  =&amp;gt; [internal] load .dockerignore                                          0.0s
#  =&amp;gt; =&amp;gt; transferring context: 2B                                            0.0s
#  =&amp;gt; [internal] load metadata for docker.io/rocker/r-ver:4.1.0              0.0s
#  =&amp;gt; [internal] load build context                                          0.0s
#  =&amp;gt; =&amp;gt; transferring context: 9.10kB                                        0.0s
#  =&amp;gt; CACHED [1/6] FROM docker.io/rocker/r-ver:4.1.0                         0.0s
#  =&amp;gt; [2/6] RUN Rscript -e &amp;quot;install.packages(&amp;#39;remotes&amp;#39;, repos = c(CRAN = &amp;#39;h  5.9s
#  =&amp;gt; [3/6] RUN Rscript -e &amp;quot;remotes::install_github(&amp;#39;rstudio/renv@0.15.2&amp;#39;)  12.8s
#  =&amp;gt; [4/6] WORKDIR /computational-reproducibility-in-r                      0.0s 
#  =&amp;gt; [5/6] COPY renv.lock renv.lock                                         0.0s 
#  =&amp;gt; [6/6] RUN Rscript -e &amp;#39;renv::restore()&amp;#39;                               130.7s 
#  =&amp;gt; exporting to image                                                     0.7s 
#  =&amp;gt; =&amp;gt; exporting layers                                                    0.7s 
#  =&amp;gt; =&amp;gt; writing image sha256:348a67e88477b16e01bd38a34e73c1c72c2f12e9ec927  0.0s 
#  =&amp;gt; =&amp;gt; naming to docker.io/library/my-docker-image                         0.0s &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above tells &lt;code&gt;Docker&lt;/code&gt; to build the image and tag it with the name &lt;code&gt;my-docker-image&lt;/code&gt;. Looks like it worked!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-a-docker-image&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running a Docker Image&lt;/h3&gt;
&lt;p&gt;To actually be of any use, we need to be able to run code that is in our project using our &lt;code&gt;Docker&lt;/code&gt; image. To do so, it is important to note that when you run a &lt;code&gt;Docker&lt;/code&gt; image, it is completely self-contained (which is why it is called a &lt;code&gt;Docker&lt;/code&gt; container when you are running it). This means that you will not have access to any of your local files when you are running your container unless you connect the container to your local directory.&lt;/p&gt;
&lt;p&gt;To do this, we can use the &lt;code&gt;-v&lt;/code&gt; flag when running &lt;code&gt;Docker&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;docker run -it -v ~/computational-reproducibility-in-r:/computational-reproducibility-in-r my-docker-image&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Above, &lt;code&gt;docker run&lt;/code&gt; is starting the container, &lt;code&gt;-it&lt;/code&gt; is making it &lt;em&gt;interactive mode&lt;/em&gt; (so that we can type out commands ourselves), and the &lt;code&gt;&amp;lt;path-to-my-local-project-directory&amp;gt;:&amp;lt;docker-directory-to-mount-to&amp;gt;&lt;/code&gt; part tells &lt;code&gt;Docker&lt;/code&gt; to mount our local project directory to the &lt;code&gt;computational-reproducibility-in-r&lt;/code&gt; directory that we created in the &lt;code&gt;Dockerfile&lt;/code&gt; above (see the line that reads &lt;code&gt;WORKDIR /computational-reproducibility-in-r&lt;/code&gt;). When mounted, anything we save to the mounted directory will be saved to our local directory too! So, if we create an R script called &lt;code&gt;make_beautiful_plot.R&lt;/code&gt; that contains the same plotting code from above, but it saves the image to our local directory instead of just showing it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)

set.seed(43210)

love_for_renv &amp;lt;- rnorm(100, exp(.022*(1:100)), .8)

my_plot &amp;lt;- 
  data.frame(Time = 1:100,
             love_for_renv = love_for_renv) %&amp;gt;%
  ggplot(aes(x = Time, y = love_for_renv)) +
  geom_point() +
  geom_line() +
  ylab(&amp;quot;Love for renv&amp;quot;) +
  scale_y_continuous(breaks = 1:10, limits = c(0,11)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())

ggsave(my_plot, file = &amp;quot;saved_plot.pdf&amp;quot;, unit = &amp;quot;in&amp;quot;,
       height = 4, width = 5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;we can run this script within the &lt;code&gt;Docker&lt;/code&gt; container by running:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;Rscript make_beautiful_plot.R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and we will see &lt;code&gt;saved_plot.pdf&lt;/code&gt; appear in our local project directory!
Although we just made a basic plot, what we have achieved is rather cool—theoretically, anyone could use the image that we created to run our code, making our analyses computationally reproducible across time and across different computers.&lt;/p&gt;
&lt;p&gt;But… there is one issue… how do we share the image? Also, building the image manually and then sending it somewhere seems like a big pain if I am working on a project over the course of months or longer. Well, this is where &lt;code&gt;GitHub&lt;/code&gt; and GitHub Actions come to play!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-github-to-build-and-store-docker-images&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using &lt;code&gt;GitHub&lt;/code&gt; to Build and Store Docker Images&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;GitHub&lt;/code&gt; is generally an amazing platform for version control when it comes to code, but it can also play the same role for our Docker images. Further, we can use &lt;a href=&#34;https://github.com/features/actions&#34;&gt;GitHub Actions&lt;/a&gt; to automate a good deal of our workflow—in our case, to build and push our &lt;code&gt;Docker&lt;/code&gt; images to a repository that allows for anyone to access them.&lt;/p&gt;
&lt;p&gt;To start, once you have your local project configured to be linked with a GitHub repo, as long as you have a &lt;code&gt;Dockerfile&lt;/code&gt; in your project directory, you can actually use the template provided by GitHub to create a workflow for building and pushing images to your own GitHub account:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/github_actions.gif&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Well, that looks a bit confusing…&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;That said, the template didn’t make a whole lot of sense to me at first, so I opted to follow &lt;a href=&#34;https://docs.github.com/en/packages/managing-github-packages-using-github-actions-workflows/publishing-and-installing-a-package-with-github-actions#upgrading-a-workflow-that-accesses-ghcrio&#34;&gt;the documentation here&lt;/a&gt; in hopes that I would better understand the final product.&lt;/p&gt;
&lt;p&gt;The basic idea is that we want to build a new image using the current dependencies in our project each time that we push changes to GitHub. I made some changes the the linked code above that make this happen. The resulting code is here (I would recommend checking out the link for more details on how this works):&lt;/p&gt;
&lt;pre eval=&#34;FALSE&#34;&gt;&lt;code&gt;name: Docker

# This workflow uses actions that are not certified by GitHub.
# They are provided by a third-party and are governed by
# separate terms of service, privacy policy, and support
# documentation.

on:
  push:
    branches: [ main ]
    # Publish semver tags as releases.
    tags: [ &amp;#39;v*.*.*&amp;#39; ]

env:
  # Use docker.io for Docker Hub if empty
  REGISTRY: ghcr.io
  # github.repository as &amp;lt;account&amp;gt;/&amp;lt;repo&amp;gt;
  IMAGE_NAME: ${{ github.repository }}


jobs:
  # Push image to GitHub Packages.
  # See also https://docs.docker.com/docker-hub/builds/
  push:
    runs-on: ubuntu-latest
    permissions:
      packages: write
      contents: read

    steps:
      - uses: actions/checkout@v2
        with:
          fetch-depth: 0  # OR &amp;quot;2&amp;quot; -&amp;gt; To retrieve the preceding commit.

      - name: Run file diff checking image
        uses: tj-actions/changed-files@v14.1

      - name: Build image
        if: contains(steps.changed-files.outputs.modified_files, &amp;#39;renv.lock&amp;#39;)
        run: |
          IMAGE_NAME=$(echo $IMAGE_NAME | tr &amp;#39;[A-Z]&amp;#39; &amp;#39;[a-z]&amp;#39;)
          docker build . --file Dockerfile --tag $IMAGE_NAME --label &amp;quot;runnumber=${GITHUB_RUN_ID}&amp;quot;
      - name: Log in to registry
        # This is where you will update the PAT to GITHUB_TOKEN
        run: echo &amp;quot;${{ secrets.GITHUB_TOKEN }}&amp;quot; | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Push image
        if: contains(steps.changed-files.outputs.modified_files, &amp;#39;renv.lock&amp;#39;)
        run: |
          IMAGE_ID=ghcr.io/$IMAGE_NAME
          # Change all uppercase to lowercase
          IMAGE_ID=$(echo $IMAGE_ID | tr &amp;#39;[A-Z]&amp;#39; &amp;#39;[a-z]&amp;#39;)
          IMAGE_NAME=$(echo $IMAGE_NAME | tr &amp;#39;[A-Z]&amp;#39; &amp;#39;[a-z]&amp;#39;)
          # Strip git ref prefix from version
          VERSION=$(echo &amp;quot;${{ github.ref }}&amp;quot; | sed -e &amp;#39;s,.*/\(.*\),\1,&amp;#39;)
          # Strip &amp;quot;v&amp;quot; prefix from tag name
          [[ &amp;quot;${{ github.ref }}&amp;quot; == &amp;quot;refs/tags/&amp;quot;* ]] &amp;amp;&amp;amp; VERSION=$(echo $VERSION | sed -e &amp;#39;s/^v//&amp;#39;)
          # Use Docker `latest` tag convention
          [ &amp;quot;$VERSION&amp;quot; == &amp;quot;main&amp;quot; ] &amp;amp;&amp;amp; VERSION=latest
          echo IMAGE_ID=$IMAGE_ID
          echo VERSION=$VERSION
          docker tag $IMAGE_NAME $IMAGE_ID:$VERSION
          docker push $IMAGE_ID:$VERSION&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that, as in the template shown in the &lt;code&gt;.gif&lt;/code&gt; above, the code chunk above is saved in &lt;code&gt;computational-reproducibility-in-r/.github/workflows/docker-publish.yml&lt;/code&gt;. Once this file is in your GitHub repository, all we need to do is make changes to our code and push them to GitHub, and a new &lt;code&gt;Docker&lt;/code&gt; image will be built and pushed to your &lt;code&gt;GitHub Packages&lt;/code&gt; registry (which we will walk through later) automatically! Let’s test it out.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;testing-out-our-complete-workflow&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Testing Out Our Complete Workflow&lt;/h2&gt;
&lt;p&gt;To test things out, let’s install a new &lt;code&gt;R&lt;/code&gt; package and then push our updated code to GitHub. This will trigger the GitHub Action from above, which will then build and push an image to our GitHub Packages registry. First, let’s install the &lt;code&gt;tidyverse&lt;/code&gt;. After opening an &lt;code&gt;R&lt;/code&gt; terminal in our project directory:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;tidyverse&amp;quot;)

# Retrieving &amp;#39;https://cran.rstudio.com/bin/macosx/big-sur-arm64/contrib/4.1/dtplyr_1.2.1.tgz&amp;#39; ...
#         OK [downloaded 321 Kb in 0.3 secs]
# Retrieving &amp;#39;https://cran.rstudio.com/bin/macosx/big-sur-arm64/contrib/4.1/rvest_1.0.2.tgz&amp;#39; ...
#         OK [downloaded 195 Kb in 0.3 secs]
# Installing dtplyr [1.2.1] ...
#         OK [installed binary]
# Moving dtplyr [1.2.1] into the cache ...
#         OK [moved to cache in 0.43 milliseconds]
# Installing forcats [0.5.1] ...
#         OK [linked cache]
# Installing googledrive [2.0.0] ...
#         OK [linked cache]
# Installing rematch [1.0.1] ...
#         OK [linked cache]
# Installing cellranger [1.1.0] ...
#         OK [linked cache]
# Installing ids [1.0.1] ...
#         OK [linked cache]
# Installing googlesheets4 [1.0.0] ...
#         OK [linked cache]
# Installing haven [2.4.3] ...
#         OK [linked cache]
# Installing modelr [0.1.8] ...
#         OK [linked cache]
# Installing readxl [1.3.1] ...
#         OK [linked cache]
# Installing reprex [2.0.1] ...
#         OK [linked cache]
# Installing selectr [0.4-2] ...
#         OK [linked cache]
# Installing rvest [1.0.2] ...
#         OK [installed binary]
# Moving rvest [1.0.2] into the cache ...
#         OK [moved to cache in 0.47 milliseconds]
# Installing tidyverse [1.3.1] ...
#         OK [linked cache]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will see &lt;code&gt;renv&lt;/code&gt; do its magic, as per usual. Then, we need to modify our code to include the tidyverse so that &lt;code&gt;renv&lt;/code&gt; can detect it when taking a new snapshot. I will just add a new file called &lt;code&gt;new_dependency.R&lt;/code&gt; that only runs &lt;code&gt;library(tidyverse)&lt;/code&gt; as an example. After creating this file, we go back to an R terminal and run:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;renv::snapshot()

# The following package(s) will be updated in the lockfile:
# 
# # CRAN ===============================
# - DBI             [* -&amp;gt; 1.1.1]
# - Rcpp            [* -&amp;gt; 1.0.8]
# - askpass         [* -&amp;gt; 1.1]
# - assertthat      [* -&amp;gt; 0.2.1]
# - backports       [* -&amp;gt; 1.2.1]
# - base64enc       [* -&amp;gt; 0.1-3]
# - bit             [* -&amp;gt; 4.0.4]
# - bit64           [* -&amp;gt; 4.0.5]
# - blob            [* -&amp;gt; 1.2.2]
# - broom           [* -&amp;gt; 0.7.11]
# - callr           [* -&amp;gt; 3.7.0]
# - cellranger      [* -&amp;gt; 1.1.0]
# - clipr           [* -&amp;gt; 0.7.1]
# - cpp11           [* -&amp;gt; 0.4.1]
# - curl            [* -&amp;gt; 4.3.2]
# - data.table      [* -&amp;gt; 1.14.0]
# - dbplyr          [* -&amp;gt; 2.1.1]
# - dtplyr          [* -&amp;gt; 1.2.1]
# - evaluate        [* -&amp;gt; 0.14]
# - fastmap         [* -&amp;gt; 1.1.0]
# - forcats         [* -&amp;gt; 0.5.1]
# - fs              [* -&amp;gt; 1.5.0]
# - gargle          [* -&amp;gt; 1.2.0]
# - googledrive     [* -&amp;gt; 2.0.0]
# - googlesheets4   [* -&amp;gt; 1.0.0]
# - haven           [* -&amp;gt; 2.4.3]
# - highr           [* -&amp;gt; 0.9]
# - hms             [* -&amp;gt; 1.1.0]
# - htmltools       [* -&amp;gt; 0.5.2]
# - httr            [* -&amp;gt; 1.4.2]
# - ids             [* -&amp;gt; 1.0.1]
# - jquerylib       [* -&amp;gt; 0.1.4]
# - jsonlite        [* -&amp;gt; 1.7.3]
# - knitr           [* -&amp;gt; 1.37]
# - lubridate       [* -&amp;gt; 1.8.0]
# - mime            [* -&amp;gt; 0.12]
# - modelr          [* -&amp;gt; 0.1.8]
# - openssl         [* -&amp;gt; 1.4.5]
# - prettyunits     [* -&amp;gt; 1.1.1]
# - processx        [* -&amp;gt; 3.5.2]
# - progress        [* -&amp;gt; 1.2.2]
# - ps              [* -&amp;gt; 1.6.0]
# - rappdirs        [* -&amp;gt; 0.3.3]
# - readr           [* -&amp;gt; 2.0.2]
# - readxl          [* -&amp;gt; 1.3.1]
# - rematch         [* -&amp;gt; 1.0.1]
# - rematch2        [* -&amp;gt; 2.1.2]
# - reprex          [* -&amp;gt; 2.0.1]
# - rmarkdown       [* -&amp;gt; 2.11]
# - rstudioapi      [* -&amp;gt; 0.13]
# - rvest           [* -&amp;gt; 1.0.2]
# - selectr         [* -&amp;gt; 0.4-2]
# - stringi         [* -&amp;gt; 1.7.6]
# - stringr         [* -&amp;gt; 1.4.0]
# - sys             [* -&amp;gt; 3.4]
# - tidyr           [* -&amp;gt; 1.1.3]
# - tidyverse       [* -&amp;gt; 1.3.1]
# - tinytex         [* -&amp;gt; 0.36]
# - tzdb            [* -&amp;gt; 0.2.0]
# - uuid            [* -&amp;gt; 1.0-3]
# - vroom           [* -&amp;gt; 1.5.5]
# - xfun            [* -&amp;gt; 0.29]
# - xml2            [* -&amp;gt; 1.3.2]
# - yaml            [* -&amp;gt; 2.2.1]
# 
# Do you want to proceed? [y/N]: y
# * Lockfile written to &amp;#39;~/computational-reproducibility-in-r/renv.lock&amp;#39;.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, our &lt;code&gt;renv.lock&lt;/code&gt; file is updated! Finally, we can go back to a regular terminal in our project directory and add, commit, and push our changes to GitHub:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git add .
git commit -m &amp;quot;adding new dependencies&amp;quot;
git push&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We should be able to navigate to the &lt;code&gt;Actions&lt;/code&gt; tab in our GitHub repo and see the image being built in real time!&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/action_building.gif&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Well would you look at that, it works!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-use-your-docker-images&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;How to Use Your Docker Images&lt;/h3&gt;
&lt;p&gt;You will now be able to find you image under the &lt;code&gt;Packages&lt;/code&gt; tab on your GitHub profile. For example, you can find the image that &lt;a href=&#34;https://github.com/Nathaniel-Haines/computational-reproducibility-in-r/pkgs/container/computational-reproducibility-in-r&#34;&gt;my own workflow built here&lt;/a&gt;. Here, you will also see a command you can run to download the package so that you (or anyone!) could use the image. In my case, it is: &lt;code&gt;docker pull ghcr.io/nathaniel-haines/computational-reproducibility-in-r:latest&lt;/code&gt;. After pulling it to your local computer, you can use it as normal!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wrapping up&lt;/h1&gt;
&lt;p&gt;Alright! So that was a bit long, but I hope that it was useful in covering computational reproducibility in &lt;code&gt;R&lt;/code&gt;. There are of course many other things you could include in this workflow to make it even better (e.g., only building a new image when the &lt;code&gt;renv.lock&lt;/code&gt; file changes, as opposed to each push, or integrating with &lt;a href=&#34;https://github.com/asdf-community/asdf-r&#34;&gt;&lt;code&gt;asdf&lt;/code&gt;&lt;/a&gt; so that you can use different &lt;code&gt;R&lt;/code&gt; versions across local projects akin to how &lt;code&gt;renv&lt;/code&gt; lets you use different &lt;code&gt;R&lt;/code&gt; package versions), but I will leave that as an exercise for those who are interested :D&lt;/p&gt;
&lt;p&gt;Thanks a lot for stopping by, and I hope you find this useful in your work!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Role of Reward and Punishment Learning in Externalizing Adolescents: A Joint Generative Model of Traits and Behavior</title>
      <link>http://haines-lab.com/talk/the-role-of-reward-and-punishment-learning-in-externalizing-adolescents-a-joint-generative-model-of-traits-and-behavior/</link>
      <pubDate>Thu, 17 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/talk/the-role-of-reward-and-punishment-learning-in-externalizing-adolescents-a-joint-generative-model-of-traits-and-behavior/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Integrating Trait and Neurocognitive Mechanisms of Externalizing Psychopathology: A Joint Modeling Framework for Measuring Impulsive Behavior</title>
      <link>http://haines-lab.com/publication/haines_2021_diss/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/haines_2021_diss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Introduction to Bayesian Data Analysis</title>
      <link>http://haines-lab.com/talk/an-introduction-to-bayesian-data-analysis/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/talk/an-introduction-to-bayesian-data-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Series on Building Formal Models of Classic Psychological Effects: Part 1, the Dunning-Kruger Effect</title>
      <link>http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/</link>
      <pubDate>Sun, 10 Jan 2021 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Dunning-Kruger effect is perhaps one of the most well-known effects in all of social psychology, defined as the phenomenon wherein people with low “objective” skill tend to over-estimate their objective skill, whereas people with high objective skill tend to under-estimate their skill. In the popular media, the Dunning-Kruger effect is often summarized in a figure like the one below (&lt;a href=&#34;https://www.businesstimes.com.sg/brunch/not-so-blissful-ignorance-the-dunning-kruger-effect-at-work&#34;&gt;source&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/dunning-kruger_misinterpretation.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: The Dunning-Kruger Effect in Popular Media&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Despite the widespread use of figures such as the one above, the specific form of the effect is misleading–it suggests that people with low objective skill perceive their skill to be &lt;em&gt;higher than those with the highest skill&lt;/em&gt;, which is not what Dunning and Kruger actually found in their &lt;a href=&#34;https://doi.org/10.1037/0022-3514.77.6.1121&#34;&gt;original 1999 study&lt;/a&gt;. As clarified by many others (e.g., see &lt;a href=&#34;https://www.talyarkoni.org/blog/2010/07/07/what-the-dunning-kruger-effect-is-and-isnt/&#34;&gt;here&lt;/a&gt;), Dunning and Kruger actually found the following:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/orig_dunning-kruger_plots.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Original Data from Dunning &amp;amp; Kruger (1999)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The basic pattern across all these different domains is consistent with our original definition of the effect–that people with low objective skill tend to over-estimate their skill to quite a large extent, yet those with high objective skill under-estimate their skill, but to a lesser extent. However, unlike the popularized interpretation, the relationship between objective and perceived skill appears to be monotonic, such that, on average, people with low skill still perceive themselves to be less skilled compared to high-skilled people.&lt;/p&gt;
&lt;p&gt;Regardless of misinterpretations in the popular media, the notoriety of the Dunning-Kruger effect cannot be understated. As of January 2021, the original study has been cited upwards of 6500 times on Google Scholar alone, and the basic Dunning-Kruger effect has been consistently replicated across samples, domains, and contexts (see &lt;a href=&#34;https://doi.org/10.1016/B978-0-12-385522-0.00005-6&#34;&gt;here&lt;/a&gt;). In many ways, research on the Dunning-Kruger effect theorefore embodies the highest ideals of contemporary psychological science, passing the bar on replicability, generalizability, and robustness that psychological scientists have been striving for since the advent of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Replication_crisis&#34;&gt;replication crisis&lt;/a&gt; (see also &lt;a href=&#34;https://www.psychologytoday.com/us/blog/how-do-you-know/202012/dunning-kruger-isnt-real&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;There is just one small problem…&lt;/p&gt;
&lt;div id=&#34;wait-the-dunning-kruger-effect-is-just-measurement-error&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wait, the Dunning-Kruger Effect is just Measurement Error?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/media/measurement_error_meme.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perhaps surprisingly, the traditional methods used to measure the Dunning-Kruger effect have always been heavily scrutinized–even the original authors acknowledged potential issues with measurement error. Specifically, it is unclear to what extent that the classic effect arises due to an actual psychological bias, versus more mundain statistical measurement error resulting from regression to the mean. In fact, in their 1999 article, Dunning and Kruger explicitly recognize this possibility (p. 1124):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“… the reader may point to the regression effect as an alternative interpretation of our results. … Because perceptions of ability are imperfectly correlated with actual ability, the regression effect virtually guarantees this result. … Despite the inevitability of the regression effect, we believe that the overestimation we observed was more psychological than artifactual. For one, if regression alone were to blame for our results, then the magnitude of miscalibration among the bottom quartile would be comparable with that of the top quartile. A glance at Figure 1 quickly disabuses one of this notion.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Although Dunning and Kruger were willing to state their belief that their results were attributable more so to psychological rather than statistical effects, they did not provide an explicit generative model for their pattern of results. Their reasoning appears sound (i.e. that differences in magnitude of mis-estimation for those under/over average indicates something beyond regression to the mean), but without a generative model, it is unclear how much faith we should place in the authors’ beliefs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-the-dunning-kruger-effect&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulating the Dunning-Kruger Effect&lt;/h2&gt;
&lt;p&gt;Fortunately, a number of simulation studies have since been conducted to show exactly how measurement error can generate data consistent with the Dunning-Kruger effect. &lt;a href=&#34;https://doi.org/10.1016/S0191-8869(01)00174-X&#34;&gt;Ackerman et al. (2002)&lt;/a&gt; was one of the first, which showed how the pattern found in Dunning and Kruger’s original study could arise from plotting any two variables with less than a perfect correlation (i.e. &lt;span class=&#34;math inline&#34;&gt;\(r=1\)&lt;/span&gt;) using the objective skill quantile versus perceived skill percentile convention from the original study. For example, if observed objective skill and perceived skill are correlated at &lt;span class=&#34;math inline&#34;&gt;\(r=.19\)&lt;/span&gt;, using the standard plotting convention, Ackerman et al. (2002) obtained the following pattern:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/ackerman_2002.png&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Caption: Figure 1 from Ackerman et al. (2002)&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The take-away from the above results is that any less-than-perfect correlation between two variables will produce the general pattern found in Dunning and Kruger’s original study, which results from a statistical phenomenon termed &lt;a href=&#34;https://en.wikipedia.org/wiki/Regression_toward_the_mean&#34;&gt;&lt;em&gt;regression to the mean&lt;/em&gt;&lt;/a&gt;. If the concept of regression to the mean seems a bit magical (it certainly was for me when I was first introduced), it is useful to simulate data to get a sense of what is going on. The R code below replicates previous simulation studies, but at three different correlations between objective and perceived skill:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Libraries we will use throughout the post
library(mvtnorm)
library(dplyr)
library(tidyr)
library(ggplot2)
library(gghighlight)
library(patchwork)
library(foreach)
library(rstan)
library(bayesplot)
library(hBayesDM)
library(httr)

# RNG seed for reproducing
set.seed(43202)

# Number of simulated participants
n &amp;lt;- 100

# Group-level means/SDs on &amp;quot;objective&amp;quot; and perceived skill assessments 
mu    &amp;lt;- c(0,0)
sigma &amp;lt;- c(1,1)

# Three different correlations between objective and perceived skill
obj_per_cor &amp;lt;- c(0, .5, 1)

# Simulate data
sim_dk_plots &amp;lt;- foreach(i=obj_per_cor) %do% {
  # Make correlation matrix
  R &amp;lt;- matrix(c(1, i,
                i, 1),
            ncol = 2)

  # Construct covariance matrix
  Epsilon &amp;lt;- diag(sigma)%*%R%*%diag(sigma)
  
  # Simulate correlated data
  sim &amp;lt;- rmvnorm(n, mu, Epsilon)
  
  # Compute quantiles, summarize and save out plot for given correlation
  sim %&amp;gt;%
    as.data.frame() %&amp;gt;%
    rename(obs_obj = V1,
           obs_per = V2) %&amp;gt;%
    mutate(quantile = cut_number(obs_obj, n = 4, labels = F)) %&amp;gt;%
    group_by(quantile) %&amp;gt;%
    summarize(mu_obj = mean(obs_obj),
              mu_per = mean(obs_per)) %&amp;gt;%
    ggplot() +
    geom_point(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;)) +
    geom_line(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;)) +
    geom_point(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;)) +
    geom_line(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;)) +
    annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 2.5, y = -1, 
             color = I(&amp;quot;black&amp;quot;), size = 5) +
    annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.8, y = .5, 
             color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
    ggtitle(paste0(&amp;quot;r = &amp;quot;, i)) +
    xlab(&amp;quot;Quantile&amp;quot;) +
    ylab(&amp;quot;Score&amp;quot;) +
    ylim(-1.7, 1.5) +
    theme_minimal(base_size = 15) +
    theme(panel.grid = element_blank(), 
          plot.title = element_text(hjust=.5))
}

# Plot objective versus perceived quantiles
sim_dk_plots[[1]] | sim_dk_plots[[2]] | sim_dk_plots[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-1-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Very interesting! We are able to replicate the simulation studies from before. More importantly, the interpretation of the Dunning-Kruger effect is not looking so good. As in previous simulation studies, our simulated data looks strikingly similar to that reported by Dunning and Kruger in their original study, yet our model contains no psychological mechanisms that could produce the effect–the results are purely due to the lack of correlation between objective and perceived skill, which could result from a number of sources including measurement error. In fact, a few recent studies (see &lt;a href=&#34;http://dx.doi.org/10.5038/1936-4660.9.1.4&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;http://dx.doi.org/10.5038/1936-4660.10.1.4&#34;&gt;here&lt;/a&gt;) have used this line of reasoning to claim that the traditional psychological interpretation of the Dunning-Kruger effect is likely incorrect, and a recent &lt;a href=&#34;mcgill.ca/oss/article/critical-thinking/dunning-kruger-effect-probably-not-real&#34;&gt;scicomm article&lt;/a&gt; brought the controversy to academic Twitter (&lt;a href=&#34;https://twitter.com/Nate__Haines/status/1345859629444706304?s=20&#34;&gt;which I participated in&lt;/a&gt;, inspiring this post!).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-generative-model-of-the-dunning-kruger-effect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Building a Generative Model of the Dunning-Kruger Effect&lt;/h1&gt;
&lt;div id=&#34;a-simple-noise-bias-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Simple Noise + Bias Model&lt;/h2&gt;
&lt;p&gt;So far, we have found that the group-level Dunning-Kruger effect can be simulated through regression to the mean, which we expect to see anytime we are observing the relationship between any two variables that have less than a perfect correlation. However, there are two problems with relying on regression to the mean &lt;strong&gt;&lt;em&gt;as an explanation&lt;/em&gt;&lt;/strong&gt; for how the Dunning-Kruger effect is generated:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Regression to the mean alone cannot account for the differences in mis-estimation found in real data between those with high versus low objective skill, and&lt;/li&gt;
&lt;li&gt;Regression to the mean results from the correlation between two variables, but what is it exactly that causes a high or low correlation between objective and perceived skill?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Regarding (1), if you compare the simulated data from either Ackerman et al. (2002) or from our own example in the preceeding section to the emprical data in Dunning and Kruger (1999), you will see that our simulated data indeed misses this difference in mis-estimation. In the emprical data, the perceived skill line is shifted upward relative to what we would expect. Therefore, it is clear that regression to the mean alone will not re-produce all aspects of the traditional Dunning-Kruger effect.&lt;/p&gt;
&lt;p&gt;Point (2) is a bit more nuanced, and it is something that we can only really answer by thinking about the data-generating process hollistically. For example, if we assume that there is some latent, underlying skill that leads to both observed performance on an objective measure and perceived performance on a subjective measure, how could this single latent skill produce patterns that are consistent with real-world data? &lt;a href=&#34;https://doi.org/10.1037/0022-3514.90.1.60&#34;&gt;Burson et al. (2006)&lt;/a&gt; took exactly this approach, which &lt;strong&gt;&lt;em&gt;I term the generative approach&lt;/em&gt;&lt;/strong&gt; (see &lt;a href=&#34;https://psyarxiv.com/xr7y3&#34;&gt;here&lt;/a&gt;, &lt;a href=&#34;http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/&#34;&gt;here&lt;/a&gt;, and &lt;a href=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/&#34;&gt;here&lt;/a&gt;). They proposed a noise + bias model (see their model specification in the appendix, on p. 77), which assumes that each participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;’s observed perceived skill rating &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}}\)&lt;/span&gt; results from their objective underlying skill level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;, in addition to some noise &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; and a bias term &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; that captures over-confidence. Conceptually, there are many ways we could implement such a noise + bias model, but the authors chose to implement it as a linear model with normally distributed errors (note that the original model assumes that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; can vary across participants, but I simplified here for illustration):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  y_{i,\text{per}} &amp;amp; = \theta_i+ b + \epsilon \\
  \epsilon &amp;amp; \sim \mathcal{N}(0,\sigma)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;It is useful to take a moment and reflect on the difference between how the problem is formulated here, as opposed to, say, thinking only in terms of the observed correlation between objective and perceived skill. The above formulation puts forth a generative model of how the person-level observed data arise, and the parameters now have direct psychological interpretations. For example, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; is interpreted as a psychological bias that leads to over-confident perceptions if &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; can be thought of as a “perception noise” parameter that controls how far one’s observed perception judgements deviate from their objective underlying skill &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;. Before, when thinking about the problem in terms of the observed correlation, there was not a clear mapping from our psychological theory to the observed data. Our statistical toolbox consisted only of group-level means, standard deviations, and correlations, yet our target for inference was on person-level psychological processes that give rise to observed data. As a result, the theoretical value of our statistical model suffered.&lt;/p&gt;
&lt;p&gt;Now, before using our generative model to explain the Dunning-Kruger effect, we first need to make a small extention. Specifically, Burson et al. (2006) specified the relationship between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; (the underlying “objective” skill) and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}}\)&lt;/span&gt; (the observed perception ratings), but they did not specify how &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; produces observed data on the “objective” skill measure, which we will term &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}}\)&lt;/span&gt;. To do so, we will add a second noise term to the model that is specific to the objective measure, which gives us the following:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  y_{i,\text{obj}} &amp;amp; = \theta_i + \epsilon_{\text{obj}} \\
  y_{i,\text{per}} &amp;amp; = \theta_i + b + \epsilon_{\text{per}} \\
  \epsilon_{\text{obj}} &amp;amp; \sim \mathcal{N}(0,\sigma_{\text{obj}}) \\
  \epsilon_{\text{per}} &amp;amp; \sim \mathcal{N}(0,\sigma_{\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;After this modification, we have a model of how one’s underlying skill (&lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;) generates observed data on both the objective measure (&lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}}\)&lt;/span&gt;) and the subjective measure (&lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}}\)&lt;/span&gt;). Therefore, we can simulate person-level data from the model. Before doing so, however, I prefer to rearrange the terms on the model to clarify the generative structure (see also &lt;a href=&#34;https://drbenvincent.medium.com/the-dunning-kruger-effect-probably-is-real-9c778ffd9d1b&#34;&gt;Ben Vincent’s recent blog&lt;/a&gt;, in which he formulates the same generative model; note also that we have added an assumption about how &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters are distributed):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  y_{i,\text{obj}} &amp;amp; \sim \mathcal{N}(\theta_i, \sigma_{\text{obj}}) \\
  y_{i,\text{per}} &amp;amp; \sim \mathcal{N}(\theta_i + b, \sigma_{\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite being mathematically identical to the first formulation, I believe this new formulation makes our assumptions more clear–that each person’s observed data are generated by a normal distribution with a mean determined by their latent skill (&lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;), which is shifted by a bias term (&lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;) when making perceived jugdements on their skill. Additionally, there is a context-specific &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameter, which controls how much oberved performance on the objective measure and perceived ratings of skill vary around the objective underlying skill. Because the observed data are assumed to follow a normal distribution, I will term this the &lt;strong&gt;&lt;em&gt;normal noise + bias model&lt;/em&gt;&lt;/strong&gt;. However, it is worth emphasizing that this model is only one implementation of the core theoretical constructs of perception noise and bias, and we would need to modify the model–while retaining these core theoretical constructs–to extend it into domains wherein we do not expect continuous data. For example, if participants performed a True/False test to measure skill in some domain, a generative model should respect the structure of the observed data by producing dichotomous responses.&lt;/p&gt;
&lt;p&gt;Alright! Enough on assumptions and limitations (for now :D), here is what happens when we simulate data from the normal noise + bias model, and then summarize the results using the traditional Dunning-Kruger plotting convention:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# RNG seed for reproducing
set.seed(43204)

# Number of simulated participants
n &amp;lt;- 100

# Latent &amp;quot;objective&amp;quot; skill
theta &amp;lt;- rnorm(n, 0, 1)

# Set noise parameters
sigma_obj &amp;lt;- 1
sigma_per &amp;lt;- 1

# Set bias parameter
b &amp;lt;- 1

# Simulate objective and perceived observed data
sim_obj &amp;lt;- rnorm(n, theta, sigma_obj)
sim_per &amp;lt;- rnorm(n, theta + b, sigma_per)
  
# Compute quantiles, summarize and plot 
data.frame(sim_obj = sim_obj,
           sim_per = sim_per) %&amp;gt;%
  mutate(quantile = cut_number(sim_obj, n = 4, labels = F)) %&amp;gt;%
  group_by(quantile) %&amp;gt;%
  summarize(mu_obj = mean(sim_obj),
            mu_per = mean(sim_per)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 1) +
  geom_point(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 1) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 2.2, y = -1, 
           color = I(&amp;quot;black&amp;quot;), size = 5) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.8, y = .5, 
           color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
  xlab(&amp;quot;Quantile&amp;quot;) +
  ylab(&amp;quot;Score&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Like before, we are able to re-produce the Dunning-Kruger effect quite well! Unlike before, our generative model assumes that the effect arises through two psychological mechanisms–a general overconfidence (i.e. &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;), and perception noise (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{obj}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{per}}\)&lt;/span&gt;). Importantly, it is the perception noise parameters that actually lead to a discrepancy between the observed objective and subjective responses. Intuitively, if &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{obj}} = \sigma_{\text{per}} = 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}} = \theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{per}} = \theta_i + b\)&lt;/span&gt;. In this case, the correlation between &lt;span class=&#34;math inline&#34;&gt;\(\text{cor}(\mathbf{y}_{\text{obj}}, \mathbf{y}_{\text{per}}) = 1\)&lt;/span&gt;, and there is subsequently no regression to the mean. Instead, the perception ratings are simply shifted upward according to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;. Conversely, as either &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{obj}}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{per}}\)&lt;/span&gt; increase toward &lt;span class=&#34;math inline&#34;&gt;\(+\infty\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{cor}(\mathbf{y}_{\text{obj}}, \mathbf{y}_{\text{per}}) \to 0\)&lt;/span&gt;, and we get results like those shown in our previous simulations where &lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt; (although the perception ratings line would still be shifted upward by &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;).&lt;/p&gt;
&lt;div id=&#34;extending-the-noise-bias-model-to-dichotomous-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Extending the Noise + Bias Model to Dichotomous Data&lt;/h3&gt;
&lt;p&gt;Before moving on, we need to modify the model for application in settings where the observed data are dichotomous in nature. The primary purpose for this is not theoretical, but instead to prepare ourselves for fitting the model to a real-world dataset in upcoming sections. In other words, we would like to modify only the part of our model that connects the core theoretical constructs of perception noise and bias to the observed data. We will get into specific details regarding the dataset later on, but for now, it is sufficient to say that we want a model that could generate data in a setting where:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;“Objective” skill is assessed on an “objective” multiple item test, where each item is scored correct or incorrect, and&lt;/li&gt;
&lt;li&gt;Perceived skill is assessed by having participants estimate how many questions they answered correctly on the objective test.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are many possible ways to approach this general problem, but for our purposes, we will begin with the simplest model: the bernoulli model. As described in a &lt;a href=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/&#34;&gt;previous post&lt;/a&gt;, the bernoulli distribution describes data that arise through a “biased coin flipping” process. For example, imagine that we have a biased coin, such that it lands “heads” 70% of the time, and “tails” 30% of the time. But suppose that we do not know the actual bias of the coin–how might we figure it out? Well, clearly we should flip the coin a few times! But how would we then estimate the underlying “success probability” (if we arbitrarily define success as landing “heads”)? This is exactly the same problem we encounter when aiming to estimate the underlying “objective skill” of a person from an objective test where each item is scored as correct or incorrect! As with a coin flip, if we assume that each item response &lt;span class=&#34;math inline&#34;&gt;\(y_t\)&lt;/span&gt; is independent and equally informative with respect to the underlying objective probability of answering correctly (which directly corresponds to the “objective skill”), we can use a bernoulli model, defined as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Pr}(y_{t}=1) = p = 1 - \text{Pr}(y_{t}=0) = 1 - q\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(y_{t} = 1\)&lt;/span&gt; indicates that item &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; was answered correctly (a “success” trial), and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,t} = 0\)&lt;/span&gt; indicates that it was answered incorrectly (a “failure” trial). The likelihood of observing a correct response is simply &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, and the likelihood of observing an incorrect response is then &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;. &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34;&gt;Per Wikipedia&lt;/a&gt;, the likelihood of a single response–termed a &lt;em&gt;bernoulli trial&lt;/em&gt;–can also be written more compactly as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p^{k}(1-p)^{1-{k}}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is indicates a correct (&lt;span class=&#34;math inline&#34;&gt;\(k=1\)&lt;/span&gt;) or incorrect (&lt;span class=&#34;math inline&#34;&gt;\(k=0\)&lt;/span&gt;) response. The re-writing is useful when we want to define how likely we are to observe, say, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; successes within &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; trials. In our case, for a given participant, &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; would be the number of of questions they answered correctly, and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; the total number of questions on the test. In this case, we have &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; bernoulli trials, and the likelihood of observing their sum follows a &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34;&gt;binomial distribution&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{n!}{k!(n-k)!}p^{k}(1-p)^{n-k}\]&lt;/span&gt;
This equation may appear a bit daunting at first glance, but it is actually quite straightforward! The first term is the binomial coefficient, and it probably looks familiar. Specifically, if you had to take the GRE for grad shcool applications, you have likely been exposed to this equation, also sometimes called an “n choose k formula”. Far from being arbitrary, the binomial coefficient is a useful, compact representation of how many different ways you could receive &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; successes within a given &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; trials. For intuition, imagine that we have a test with 3 items (&lt;span class=&#34;math inline&#34;&gt;\(n=3\)&lt;/span&gt;), and our participant answers 2 items correctly (&lt;span class=&#34;math inline&#34;&gt;\(k=2\)&lt;/span&gt;). If we somehow know that their objective underlying probability of responding correctly is .8 (&lt;span class=&#34;math inline&#34;&gt;\(p=.8\)&lt;/span&gt;), how likely are our observed data? To get the answer, we need to first consider how many different ways in which we could observe 2 correct responses in any 3 trials. For example, we could have observed any of the following sequences of correct/incorrect responses: &lt;span class=&#34;math inline&#34;&gt;\(\{1,1,0\}, \{1,0,1\}, \{0,1,1\}\)&lt;/span&gt;. Of course, it is a pain to write out all these different possibilities just to find out how many ways observed data could have arised–and herein lies the power of the binomial coefficient! Working through &lt;span class=&#34;math inline&#34;&gt;\(\frac{3!}{2!(3-2)!}\)&lt;/span&gt;, we get &lt;span class=&#34;math inline&#34;&gt;\(\frac{3 \times2 \times 1}{2 \times 1 \times 1} = 3\)&lt;/span&gt;–the same answer. Knowing that we could get 2 correct responses in 3 trials in 3 different ways, we then need to determine how likely the actual responses are. Since &lt;span class=&#34;math inline&#34;&gt;\(p=.8\)&lt;/span&gt; and trials are assumed indpendent, we can get the joint probability by taking the product as &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(k=1) \times \text{Pr}(k=1) \times \text{Pr}(k=0) = .8 \times .8 \times .2 = .128\)&lt;/span&gt;. Given that these particular responses could have been generated in 3 different ways, we then multiply everything together to get &lt;span class=&#34;math inline&#34;&gt;\(3 \times .128= .384\)&lt;/span&gt;. What this means is that, if our participant has a objective &lt;span class=&#34;math inline&#34;&gt;\(p=.8\)&lt;/span&gt;, and they were to take the same 3 item test an infinite number of times (without remembering the questions from before–indeed a nightmarish situation, but let’s roll with it anyway), we would expect them to get 2 of the 3 question correct 38.4% of the time.&lt;/p&gt;
&lt;p&gt;To confirm, we can do a very simple simulation in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Draw 3 samples from a binomial distribution where n=1 and p=.8 
# (equivalent to bernoulli trials), and then take the sum 
sum(rbinom(3,1,.8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Replicate the example above 10000 times, and compute the proportion of
# times that the sum is equal to 2
mean(replicate(10000, sum(rbinom(3,1,.8)))==2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3796&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simulation is pretty close to the analytical solution! With the binomial likelihood covered, we are ready to move on to the next step–incorporating the theoretical constructs of perception noise and bias into a model that generates data as described by (1) and (2) above.&lt;/p&gt;
&lt;p&gt;Now that we are assuming a binomial generative model, we need to think of our problem in terms of how to model the succuss probability, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; as opposed to the mean of a normal distribution, as in the normal noise + bias model in the previous section. The most intuitive way to do this is to take our underlying objective skill parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; from before, and apply a transformation to them such that &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \theta_i &amp;lt; 1\)&lt;/span&gt; as opposed to &lt;span class=&#34;math inline&#34;&gt;\(-\infty &amp;lt; \theta_i &amp;lt; +\infty\)&lt;/span&gt;. Retaining the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\theta_i \sim \mathcal{N}(0,1)\)&lt;/span&gt;, a good option is the cumultative distribution function of the standard normal distribution, or the probit transformation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \Phi(\theta_i)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The probit transtormation has a useful interpretation—we can now think of the underlying &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters as z-scores, and the resulting success probabilities &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; as the area under the curve of the standard normal distribution up to the z-score &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;. We can represent this graphically in R as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Visualize cumulative distribution function of normal distribution

# For the first plot, we look at the area under \theta = -1
p1 &amp;lt;- data.frame(x = seq(-5, 5, length.out = 100)) %&amp;gt;% 
  mutate(y = dnorm(x)) %&amp;gt;%
  ggplot(aes(x, y)) + 
  geom_area(fill = &amp;quot;#8F2727&amp;quot;) + 
  gghighlight(x &amp;lt; -1) +
  ggtitle(expression(theta~&amp;quot; = -1&amp;quot;)) +
  xlab(expression(theta)) +
  ylab(&amp;quot;density&amp;quot;) +
  annotate(geom=&amp;quot;text&amp;quot;, x = -3, y = .05, label = &amp;quot;.16&amp;quot;, 
           color = I(&amp;quot;#8F2727&amp;quot;), size = 8) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(hjust=.5))

# For the second plot, we look at the area under \theta = +1
p2 &amp;lt;- data.frame(x = seq(-5, 5, length.out = 100)) %&amp;gt;% 
  mutate(y = dnorm(x)) %&amp;gt;%
  ggplot(aes(x, y)) + 
  geom_area(fill = &amp;quot;#8F2727&amp;quot;) + 
  gghighlight(x &amp;lt; 1) +
  ggtitle(expression(theta~&amp;quot; = +1&amp;quot;)) +
  xlab(expression(theta)) +
  annotate(geom=&amp;quot;text&amp;quot;, x = -3, y = .05, label = &amp;quot;.84&amp;quot;, 
           color = I(&amp;quot;#8F2727&amp;quot;), size = 8) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        plot.title = element_text(hjust=.5))

# Plot together for comparison
p1 | p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the area under the curve up to the given value of &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; is highlighted in red, and the ratio of red area to total area is shown next to the highlighted portion–these are the values for &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; that result from the probit transformation. If we make the simplifying assumption that observed responses on the objective skill test are directly related to &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; through the binomial generative model (i.e. no perception noise or bias), we can begin constructing our model as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i)\\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Despite the fact that we do not have a noise term in the model, the relationship between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_{i,\text{obj}}\)&lt;/span&gt; is still probabilistic, as in the normal noise + bias model. This is because the binomial model is inherently probabilistic–with a set number of trials &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{obj}}\)&lt;/span&gt; and a given underlying correct response probability &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;, the model will produce different observed responses if we generate data from it repeatedly. We can do this in R, looking at 50 different realizations from each of the example participants illustrated in the previous figure, where we demonstrated the probit transformation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of trials to simulate
n_trials &amp;lt;- 10

# Binomial sums of trials
# For participant where \theta = -1
subj1_obj &amp;lt;- replicate(50, sum(rbinom(n_trials, 1, pnorm(-1))))
# For participant where \theta = +1
subj2_obj &amp;lt;- replicate(50, sum(rbinom(n_trials, 1, pnorm(1))))

# Plotting binomial counts
p1_obj &amp;lt;- ggplot() +
  geom_bar(aes(x = as.factor(subj1_obj)), 
           fill = &amp;quot;#8F2727&amp;quot;, stat = &amp;quot;count&amp;quot;) +
  geom_vline(xintercept = 6, linetype = 2) +
  ggtitle(expression(theta~&amp;quot; = -1&amp;quot;)) +
  xlab(&amp;quot;Number of Correct Responses&amp;quot;) +
  scale_x_discrete(limit = as.factor(seq(0,10,1))) +
  coord_cartesian(ylim=c(0,22)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(), 
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))
p2_obj &amp;lt;- ggplot() +
  geom_bar(aes(x = as.factor(subj2_obj)), 
           fill = &amp;quot;#8F2727&amp;quot;, stat = &amp;quot;count&amp;quot;) +
  geom_vline(xintercept = 6, linetype = 2) +
  ggtitle(expression(theta~&amp;quot; = +1&amp;quot;)) +
  xlab(&amp;quot;Number of Correct Responses&amp;quot;) +
  scale_x_discrete(limit = as.factor(seq(0,10,1))) +
  coord_cartesian(ylim=c(0,22)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(), 
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))

# plot together, as before
p1_obj | p2_obj&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the black dotted line indicates a sum of 5, which I include simply as a reference point to compare the two “participants”. As anticipated, across the 50 different simulations of the model, each with &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{obj}}=10\)&lt;/span&gt;, the observed sum of correct responses varies around the underlying generating probability for each participant, &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;. It is also worth recognizing that these distributions are quite skewed, such that they cannot be fully appreciated with a single summary statistic as is often done in studies (e.g., using the proportion of correct responses as an independent/dependent variable in a model).&lt;/p&gt;
&lt;!-- We can start with a conceptual question. What is the function of noise? Per the normal noise + bias model, perception noise is a form of uncertainty or general lack of information that manifests as participants having imperfect knowledge of their objective underlying skill, $\theta_i$. Time for a thought experiment! Imagine you were in a scenario where you took a 10 item test. Upon finishing, for some reason, you were highly uncertain regarding your level of skill in the tested domain. If this were the case (really try to imagine it!), and you were subsequently asked to estimate how many items you answered correctly, what would be your best guess? You may be tempted to answer &#34;none&#34;, but wouldn&#39;t that imply that you thought the test was difficult, which can only be in reference to your skill level. So, what is your answer? Now, imagine the opposite case--that after having taken the test, you were completely certain regarding your skill level. If you were again asked to estimate how many items you answered correctly, what would you say? For even better intuition, imagine that I have a completed, 10 item test from an individual, but I will tell you nothing at all about them or the test--the test could be anything from an experiement administered to a rat to my 3rd grade history exam, and I will leave you with utter, complete uncertainty. Now, how many questions were correct on this test?  --&gt;
&lt;p&gt;The next step is to extend our model so that it can also account for perceived judgements. In our case, the perceived judgements take the form of asking participants to estimate how many items that they got correct on the objective skill assessment. This is nice, because it means that we can still use the binomial distribution as a generative model, although we do need to somehow incorporate the perception noise and bias terms. To do so, we need to think more deeply about the underlying decision process. For perception bias, the simplest implementation is to add a term, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, to &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; before the probit transformation, which will lead to decrease or increase the resulting perceived probability of responding correctly to each item correctly (&lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt;) depending on whether &lt;span class=&#34;math inline&#34;&gt;\(b&amp;lt;0\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;Perception noise will be a bit more tricky to deal with. Per the normal noise + bias model, perception noise is a form of uncertainty or general lack of information that manifests as participants having imperfect knowledge of their objective underlying skill, &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;, and subsequently their objective probability of getting items correct, &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;. One way to capture this idea is to take inspiration from signal detection theory (which is a reasonable starting place, as SDT has been used to model exactly these types of subjective, uncertain judgements; see &lt;a href=&#34;https://doi.org/10.1016/0030-5073(80)90045-8&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1037/0033-295X.101.3.490&#34;&gt;here&lt;/a&gt;). In this framework, we can model noise as the standard deviation of a normal distribution, which scales the difference between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}} = \Phi(\frac{\theta_i + b}{\sigma})\)&lt;/span&gt;, which gives us the following full model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i)\\
  p_{i,\text{per}} &amp;amp; = \Phi(\frac{\theta_i + b}{\sigma}) \\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}}) \\
  y_{i,\text{per}} &amp;amp; \sim \text{Binomial}(n_{\text{per}}, p_{i,\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can gain a better intuition of the noise and bias terms by visualizing how they work. First, a look at how the noise term influences resulting &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt; values when &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set parameters
theta &amp;lt;- 1
b     &amp;lt;- 0           # no bias
sigma &amp;lt;- c(.5, 1, 2) # three values for sigma

# Loop over different sigma values
sdt_plots &amp;lt;- foreach(sig=sigma) %do% {
  # Plot the &amp;quot;noise distribution&amp;quot;, centered at \theta+b
  data.frame(x = seq(-5.5, 5.5, length.out = 100)) %&amp;gt;% 
      mutate(y = dnorm(x, theta+b, sig)) %&amp;gt;%
      ggplot(aes(x, y)) + 
      geom_area(fill = &amp;quot;#8F2727&amp;quot;) + 
      gghighlight(x &amp;gt; 0) +
      geom_vline(xintercept = 0, linetype = 2) +
      ggtitle(paste0(&amp;quot;sigma = &amp;quot;, sig)) +
      xlab(expression(theta+b)) +
      ylab(&amp;quot;density&amp;quot;) +
      ylim(0,.8) +
      annotate(geom=&amp;quot;text&amp;quot;, x = -3.2, y = .25, 
               label = round(pnorm((theta+b)/sig), 2), 
               color = I(&amp;quot;#8F2727&amp;quot;), size = 8) +
      theme_minimal(base_size = 15) +
      theme(panel.grid = element_blank(),
            axis.text.y = element_blank(),
            plot.title = element_text(hjust=.5))
}

# Plot examples
sdt_plots[[1]] | sdt_plots[[2]] | sdt_plots[[3]]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-6-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With this function, as &lt;span class=&#34;math inline&#34;&gt;\(\sigma \to +\infty\)&lt;/span&gt;, perception noise increases indefinitely, and &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}} \to .5\)&lt;/span&gt;. As &lt;span class=&#34;math inline&#34;&gt;\(\sigma \to 0\)&lt;/span&gt;, perception noise becomes non-existent, and we get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{i,\text{per}} \to
\begin{cases} 
  0, ~\text{ if } \theta+b &amp;lt; 0 \\ 
  1, ~\text{ if } \theta+b &amp;gt; 0 \\ 
  .5, ~\text{if } \theta+b = 0 
\end{cases}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Intuitively, when perception noise is very low, people with above average objective skill (indicated by &lt;span class=&#34;math inline&#34;&gt;\(\theta_i = 0\)&lt;/span&gt;) will perceive their performance to be top-notch, whereas those below average will perceive their performance to be absolutely terrible. Conversely, if noise is very high, the opposite pattern emerges, where those with very low objective skill will perceive their performance to be better than it truly is, and those with high skill will perceive their performance to be worse than it truly is. Finally, when combined with a general bias, &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, the amount of over- or under-confidence can be shifted. Therefore, if &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;1\)&lt;/span&gt;, bias and noise work together to produce an effect where those with low skill overestimate their skill to a large extend, whereas those with high skill underestimate their skill, but to a lesser extent–the classic Dunning-Kruger effect! To see this, it is useful to plot out &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt; as a function of &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obs}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set parameters
thetas &amp;lt;- seq(-5, 5, length.out = 100) # whole range of theta
bs     &amp;lt;- c(0, 1)     # two levels of bias
sigmas &amp;lt;- c(.5, 1, 2) # three levels of noise

# Loop over different b and sigma values
noise_bias_dat &amp;lt;- foreach(b=bs, .combine = &amp;quot;rbind&amp;quot;) %do% {
  foreach(sig=sigma, .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Generate p_obj and p_per from theta, sigma, and b
    data.frame(theta = thetas) %&amp;gt;% 
        mutate(b     = b,
               sigma = sig, 
               p_obj = pnorm(theta),
               p_per = pnorm((theta+b)/sig))
  }
}

# Plot different combinations of noise and bias
noise_bias_dat %&amp;gt;%
  ggplot(aes(p_obj, p_per)) + 
  geom_line(color = I(&amp;quot;#8F2727&amp;quot;), size = 1.5) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  facet_grid(b ~ sigma, labeller = labeller(.rows = label_both, 
                                            .cols = label_both)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-7-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I don’t know about you, but I think this is really cool. What we see is that two simple mechanisms of perception noise and bias can generate a diverse range of patterns, of which the Dunning-Kruger effect is a special case (see the bottom right panel, where &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;1\)&lt;/span&gt;). Additionally, there is a special case where participants are expected to be perfectly callibrated to their underlying “objective” skill level, which occurs when &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; (see the top middle panel). Not only can this model give us an explanation for the Dunning-Kruger effect, but it also facilitates making novel predictions. For example, if we design an experiment to manipulate uncertainty in people’s confidence judgements, we would expect this to influence perception noise, which then produces specific patterns of behavior depending on whether we increase or decrease uncertainty.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;a-perception-distortion-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Perception Distortion Model&lt;/h2&gt;
&lt;p&gt;Before moving on to fit our model to real-world data, I thought it would be worth considering a different model inspired by the confidence judgement literature (in particular, see &lt;a href=&#34;https://doi.org/10.1016/j.jmp.2010.08.011&#34;&gt;here&lt;/a&gt;). This model does not include noise per se, but instead it relies on a function describing how people’s judgements become distorted when they are accessed. This distortion could arise from various different sources unrelated to uncertainty, but as we will see, it generates data analagous to the binomial noise + bias model in the previous section.&lt;/p&gt;
&lt;p&gt;To begin, we will assume that the objective performance component for our new model is the same as in the binomial noise + bias model, giveing us:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i)\\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So far, so good–nothing different from before. Next, we will use a similar function as described by &lt;a href=&#34;https://doi.org/10.1016/j.jmp.2010.08.011&#34;&gt;Merkle et al. (2011)&lt;/a&gt;, which is grounded in the probability estimation literature:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_{i,\text{per}} = \frac{\delta p_{i,\text{obj}}^{\gamma}}{\delta p_{i,\text{obj}}^{\gamma} + (1-p_{i,\text{obj}})^{\gamma}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; is a bias parameters akin to &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; in the binomial noise + bias model, and &lt;span class=&#34;math inline&#34;&gt;\(\gamma\)&lt;/span&gt; is a &lt;strong&gt;&lt;em&gt;perception distortion&lt;/em&gt;&lt;/strong&gt; parameter, which controls how strongly participants over- or under-weight below- and above-average levels of skill, as indexed by &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt;. Per usual, it is best to visualize the function to better interpret the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Perception distortion function
per_dist &amp;lt;- function(p_obj, gamma, delta) {
  delta*(p_obj^gamma) / (delta*(p_obj^gamma) + (1-p_obj)^gamma)
}

# Set parameters
thetas &amp;lt;- seq(-5, 5, length.out = 100) # whole range of theta
deltas &amp;lt;- c(1, 2)     # two levels of bias
gammas &amp;lt;- c(2, 1, .3) # three levels of noise

# Loop over different delta and gamma values
per_dist_dat &amp;lt;- foreach(delta=deltas, .combine = &amp;quot;rbind&amp;quot;) %do% {
  foreach(gamma=gammas, .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Generate p_obj and p_per from theta, gamma, and delta
    data.frame(theta = thetas) %&amp;gt;% 
        mutate(delta = delta,
               gamma = gamma, 
               p_obj = pnorm(theta),
               p_per = per_dist(pnorm(theta), gamma, delta))
  }
}

# Plot different combinations of perception distortion and bias
per_dist_dat %&amp;gt;%
  mutate(gamma = factor(gamma, 
                        levels = c(2, 1, .3),
                        labels = c(2, 1, .3))) %&amp;gt;%
  ggplot(aes(p_obj, p_per)) + 
  geom_line(color = I(&amp;quot;#8F2727&amp;quot;), size = 1.5) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  facet_grid(delta ~ gamma, labeller = labeller(.rows = label_both, 
                                               .cols = label_both)) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x = element_text(angle = 45),
        plot.title = element_text(hjust=.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks familiar, doesn’t it? We get the same basic pattern using this perception distortion model as we do with the noise + bias model. In many ways, it appears that these models may be statistically indistinguishable without a very specific experimental design. Alternatively, one could make the arguement that the noise + bias model is more theoretically informative, as it provides a plausible psychological explanation for how perception distortion may arise, whereas the perception distortion model itself does not have an obvious psychological interpretation . Still, it is useful to show the correspondence between these models, because this perception distortion function is widely used in the decision-making literature. In fact, it makes up a key component of &lt;a href=&#34;https://doi.org/10.1007/BF00122574&#34;&gt;cumulative prospect theory&lt;/a&gt;, wherein it is used to model how people assign subjective weights to objective probabilities, thereby producing patterns of risk preferences that we observe in real data.&lt;/p&gt;
&lt;p&gt;The full binomial perception distortion model can then be written as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align} 
  \theta_i &amp;amp; \sim \mathcal{N}(0,1) \\
  p_{i,\text{obj}} &amp;amp; = \Phi(\theta_i) \\
  p_{i,\text{per}} &amp;amp; = \frac{\delta p_{i,\text{obj}}^{\gamma}}{\delta p_{i,\text{obj}}^{\gamma} + (1-p_{i,\text{obj}})^{\gamma}} \\
  y_{i,\text{obj}} &amp;amp; \sim \text{Binomial}(n_{\text{obj}}, p_{i,\text{obj}}) \\
  y_{i,\text{per}} &amp;amp; \sim \text{Binomial}(n_{\text{per}}, p_{i,\text{per}})
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-our-models-to-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting Our Models to Data&lt;/h1&gt;
&lt;p&gt;We have almost made it! Thanks for sticking around for this long :D. We are now ready to try fitting our models to real-world data to determine whether our models can adequately account for people’s actual perception judgements. Further, the parameter estimates we obtain will allow us to determine if there is evidence for the traditional interpretation of the Dunning-Kruger effect in our data.&lt;/p&gt;
&lt;p&gt;First, we need to find a dataset. Fortunately, with the recent increase in open science practices in psychology, I was able to locate a perfect dataset for us. The data we will use are from Study 1 of &lt;a href=&#34;https://doi.org/10.3758/s13423-017-1242-7&#34;&gt;Pennycook et al. (2017)&lt;/a&gt;, and can be found at the following &lt;a href=&#34;https://osf.io/3kndg/&#34;&gt;Open Science Foundation Repo&lt;/a&gt; (huge shout-out for making your data openly available!). In this study, participants engaged in a cognitive reflection task with 8 items, where questions were designed to ellicit an “intuitive” answer that was nevertheless incorrect. For example, one item was as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If you’re running a race and you pass the person in second place, what place are you in? (intuitive answer: first; correct answer: second).”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The other items were similar, and overall they were quite fun to think through! Participants answered each of the 8 items, were not given feedback on their answers, and were then asked to give an estimate of the numberof items that they think they got correct. Therefore, our data are formatted such that we have two “counts” for each participant, one reflecting the number of items they answered correctly, and the other reflecting the perceived number of items they answered correctly. In both cases, there are 8 items in total, meaning that &lt;span class=&#34;math inline&#34;&gt;\(n_{\text{obj}} = n_{\text{per}} = 8\)&lt;/span&gt;. This experiment is perfect for the binomial models that we developed!&lt;/p&gt;
&lt;p&gt;The code below downloads the data from the OSF repo into R, and then makes a scatterplot of the objective number of correct items versus perceived number of correct itmes across participants:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download data first
osf_url &amp;lt;- &amp;quot;https://osf.io/3kndg/download&amp;quot; # url for Study 1 csv file
filename &amp;lt;- &amp;#39;pennycook_2017.csv&amp;#39;
GET(osf_url, write_disk(filename, overwrite = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Response [https://files.osf.io/v1/resources/p8xjs/providers/osfstorage/589ddf839ad5a1020acb69c8?action=download&amp;amp;direct&amp;amp;version=1]
##   Date: 2022-01-26 04:07
##   Status: 200
##   Content-Type: text/csv
##   Size: 88.2 kB
## &amp;lt;ON DISK&amp;gt;  /Users/nathanielhaines/Dropbox/Building/my_website/haines-lab_dev/content/post/2021-01-10-modeling-classic-effects-dunning-kruger/pennycook_2017.csv&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;osf_dat &amp;lt;- read.csv(filename, header=TRUE)

# Actual versus perceived number correct
obs_scatter &amp;lt;- qplot(osf_dat$CRT_sum, osf_dat$Estimate, 
      color = I(&amp;quot;#8F2727&amp;quot;), size = 1, alpha = .01) +
  ggtitle(paste0(&amp;quot;N = &amp;quot;, nrow(osf_dat), 
                 &amp;quot;; r = &amp;quot;, round(cor(osf_dat$CRT_sum, 
                                     osf_dat$Estimate), 2))) +
  xlab(&amp;quot;Objective Number Correct&amp;quot;) +
  ylab(&amp;quot;Perceived Number Correct&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;, 
        plot.title = element_text(hjust=.5))
obs_scatter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the scatterplot alone, it looks like there is not much going on–there is a modest positive correlation between the objective and perceived number of correct items, but how should we interpret this? We know from our initial analyses that such a low correlation should produce the Dunning-Kruger effect when plotted using the traditional quantile visualization by way of regression to the mean, although regression to the mean alone should not produce any systematic biases (i.e. over- or under-confidence). Let’s see what the quantile plot looks like:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Make quantile plot with observed data
data.frame(obs_obj = osf_dat$CRT_sum,
           obs_per = osf_dat$Estimate) %&amp;gt;%
  mutate(quantile = cut_number(obs_obj, n = 4, labels = F)) %&amp;gt;%
  group_by(quantile) %&amp;gt;%
  summarize(mu_obj = mean(obs_obj),
            mu_per = mean(obs_per)) %&amp;gt;%
  ggplot() +
  geom_point(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_obj), color = I(&amp;quot;black&amp;quot;), size = 1) +
  geom_point(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 2) +
  geom_line(aes(x = 1:4, y = mu_per), color = I(&amp;quot;#8F2727&amp;quot;), size = 1) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 1.5, y = 1, 
             color = I(&amp;quot;black&amp;quot;), size = 5) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.5, y = 4, 
             color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
  ylim(0,8) +
  xlab(&amp;quot;Quantile&amp;quot;) +
  ylab(&amp;quot;Average Sum Within Quantile&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-10-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, we see the general Dunning-Kruger pattern, where participants with low objective scores over-estimate their scores, and those with high objective scores under-estimate their scores, but to a lesser extent. Knowing that the effect is there in the data, it is time to fit our models to see what insights they reveal!&lt;/p&gt;
&lt;div id=&#34;fitting-the-binomial-noise-bias-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting the Binomial Noise + Bias Model&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; code below implements the binomial noise + bias model:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
  int&amp;lt;lower=1&amp;gt; N;     // Number of participants
  int&amp;lt;lower=1&amp;gt; n_obj; // Number of items on objective measure
  int&amp;lt;lower=1&amp;gt; n_per; // Number of items on perception measure
  int y_obj[N];       // Objective number of items correct
  int y_per[N];       // Perceived number of items correct
}

parameters {
  vector[N] theta;     // Underlying skill
  real&amp;lt;lower=0&amp;gt; sigma; // Perception noise 
  real b;              // Bias
}

transformed parameters {
  vector[N] p_obj;
  vector[N] p_per;

  // Probit transforming our parameters
  for (i in 1:N) {
    p_obj[i] = Phi_approx(theta[i]);
    p_per[i] = Phi_approx((theta[i]+b)/sigma);
  }
}

model {
  // Prior distributions
  theta ~ normal(0,1);
  sigma ~ lognormal(0,1);
  b     ~ normal(0,1);
  
  // Likelihood for both objective and perceived number of correct items
  y_obj ~ binomial(n_obj, p_obj);
  y_per ~ binomial(n_per, p_per);
}

generated quantities {
  int y_obj_pred[N];
  int y_per_pred[N];
  
  // Generate posterior predictions to compare against observed data
  y_obj_pred = binomial_rng(n_obj, p_obj);
  y_per_pred = binomial_rng(n_per, p_per);
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I have written the code above to be as close to the formal definitions described throughout this post as possible, in an effort tomake the Stan code easier to understand. Overall, the model is rather simple–the most complex part is the noise component, but hopefully the visualizations we made above make the meaning clear. Next, we just need to format our data and fit the model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note that you will need to compile the model first. Mine is
# already compiled within the Rmarkdown file used to make the post:
# binomial_noise_bias &amp;lt;- stan_model(&amp;quot;path_to_model/file_name.stan&amp;quot;)

# Format data for stan
stan_dat &amp;lt;- list(N = nrow(osf_dat),
                 n_obj = 8,
                 n_per = 8,
                 y_obj = osf_dat$CRT_sum,
                 y_per = osf_dat$Estimate)

# Fit the model!
fit_noise_bias &amp;lt;- sampling(binomial_noise_bias, 
                           data   = stan_dat, 
                           iter   = 1000, # 1000 MCMC samples
                           warmup = 300,  # 300 used for warm-up
                           chains = 3,    # 3 MCMC chains
                           cores  = 3,    # parallel over 3 cores
                           seed   = 43210)

# Once finished, check convergence using traceplots 
traceplot(fit_noise_bias)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;
Simple as that! The model runs very quickly, and we can see from the traceplots above that the model seems to have converged well (the traceplots should look like “furry caterpillars”). We would normally check traceplots for more parameters than just the 10 participant &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters above, but for now, we will just check the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; statistics, which should be close to 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot R-hat statistics
stan_rhat(fit_noise_bias)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-13-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;They look great! Given that these diagnostics look good, and that our model is relatively simple, I am confident that the model has converged, and we can now look at our parameter estimates.&lt;/p&gt;
&lt;p&gt;Remember, the binomial noise + bias model produces the Dunning-Kruger effect when both &lt;span class=&#34;math inline&#34;&gt;\(\sigma &amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;, so we are looking for evidence that our parameters meet these criteria. For our purposes, we can just visually check the parameters:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract posterior samples from model fit object
pars_noise_bias &amp;lt;- rstan::extract(fit_noise_bias)

# Prior versus posterior distribution for sigma
post_sigma &amp;lt;- ggplot() +
  geom_vline(xintercept = 1, linetype = 2) + 
  geom_density(aes(x = pars_noise_bias$sigma), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dlnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(sigma)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(0,5) +
  ylim(0,1.75) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(size = 15))
post_b &amp;lt;- ggplot() +
  geom_vline(xintercept = 0, linetype = 2) + 
  geom_density(aes(x = pars_noise_bias$b), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(b)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(-3,3) +
  ylim(0,1.75) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 15))
# Plot together
post_sigma | post_b&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-14-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What you are seeing above is the posterior distribution for both parameters (in dard red), compared to each parameter’s prior distribution (dotted gray line). Further, the black dotted lines indicate &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt; (note that I chose priors that placed 50% of the prior mass over and under the “critical” values of &lt;span class=&#34;math inline&#34;&gt;\(\sigma=1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b=0\)&lt;/span&gt;). Importantly, there is very strong evidence that both &lt;span class=&#34;math inline&#34;&gt;\(\sigma&amp;gt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b&amp;gt;0\)&lt;/span&gt;–indeed, the entire posterior distribution for each parameter is above the critical value, suggesting strong evidence of perception noise and bias, which together produce the classic Dunning-Kruger effect.&lt;/p&gt;
&lt;p&gt;Of course, before we get too excited, we should confirm that the model can reproduce theoretically relevant patterns in the observed data. First, we can check how well the model can predict both the objective and perceived number of items correct for each participant:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set the color scheme for maximum vibes
color_scheme_set(&amp;quot;red&amp;quot;)

# We will order the plots by the objective number correct
ord &amp;lt;- order(stan_dat$y_obj)

# Posterior predictions for objective number correct
pp_obj &amp;lt;- ppc_intervals(y = stan_dat$y_obj[ord],
                        yrep = pars_noise_bias$y_obj_pred[,ord],
                        prob = .5, 
                        prob_outer = .95) +
  xlab(&amp;quot;Participant&amp;quot;) +
  ylab(&amp;quot;Objective Number Correct&amp;quot;) +
  ylim(0,8) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)

# Posterior predictions for perceived number correct
pp_per &amp;lt;- ppc_intervals(y = stan_dat$y_per[ord],
                        yrep = pars_noise_bias$y_per_pred[,ord],
                        prob = .5, 
                        prob_outer = .95) +
  xlab(&amp;quot;Participant&amp;quot;) +
  ylab(&amp;quot;Perceived Number Correct&amp;quot;) +
  ylim(0,8) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank())

# Plot together
pp_obj | pp_per&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-15-1.svg&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the dark red points indicate the observed sums, and the lighter red points and intervals indicate the posterior model predictions and 50% and 95% highest density intervals, respectively (i.e. the darker intervals are 50% HDIs, and lighter intervals 95% HDIs). We want to ensure that our uncertainty intervals are not biased in any systematic way, and that the observed data are reasonably well described by the model predictions. In our case, it is pretty clear that the objective number correct is captured quite well, although there is still some uncertainty due to the fact that we only have 8 items to work with. Conversely, there is much more uncertainty for the perceived number of correct, although the model does seem to have reasonably well callibrated uncertainty intervals. For example, across participants, it is pretty clear that at least 95% of the observed points are contained within the 95% HDIs. Although, 1 participant in particular is apparently VERY underconfident… :’(. Overall, the model performs as we should expect it to–with the additional perception noise component, the predictions should naturally be more uncertain relative to the objective number correct results.&lt;/p&gt;
&lt;p&gt;We should also check to see if the model can re-produce the main effect of interest–the Dunning-Kruger effect using a traditional quantile plot! The code below draws 50 samples from the posterior distribution, each time summarizing the data and plotting according to a traditional Dunning-Kruger style plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Pick 50 random draws from our 2100 total posterior samples
samps &amp;lt;- sample(1:2100, size = 50, replace = F)

# Posterior predictions, summarized with quantile plot
foreach(i=samps, .combine = &amp;quot;rbind&amp;quot;) %do% {
  data.frame(obj = pars_noise_bias$y_obj_pred[i,],
             per = pars_noise_bias$y_per_pred[i,]) %&amp;gt;%
    mutate(quantile = cut_number(obj, n = 4, labels = F)) %&amp;gt;%
    pivot_longer(cols = c(&amp;quot;obj&amp;quot;, &amp;quot;per&amp;quot;), names_to = &amp;quot;type&amp;quot;) %&amp;gt;%
    group_by(type, quantile) %&amp;gt;%
    summarize(mu = mean(value)) %&amp;gt;%
    mutate(iter = i)
} %&amp;gt;% 
  ggplot(aes(x = quantile, y = mu, color = type, 
             group = interaction(type, iter))) +
  geom_line(alpha = .1, size = 1) +
  geom_point(alpha = .1, size = 2) +
  scale_color_manual(values = c(&amp;quot;black&amp;quot;, &amp;quot;#8F2727&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Objective&amp;quot;, x = 1.5, y = 1, 
             color = I(&amp;quot;black&amp;quot;), size = 5) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Perceived&amp;quot;, x = 1.5, y = 4, 
             color = I(&amp;quot;#8F2727&amp;quot;), size = 5) +
  ylim(0,8) +
  xlab(&amp;quot;Quantile&amp;quot;) +
  ylab(&amp;quot;Average Sum Within Quantile&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-16-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see here that the model indeed captures the traditional Dunning-Kruger effect, although now we have the added insight of being able to interpret the effect through the lens of the model. It is also worth noting that, by plotting samples from the posterior, we get an idea of what the variability in the data look like, and by extention we know what to expect if we were to run the same experiment again.&lt;/p&gt;
&lt;p&gt;Finally, we can reporoduce the scatterplot we made of the original data to see if the correlation looks as expected (this time, using only one sample from the posterior for clarity):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Draw random sample from posterior
samp &amp;lt;- sample(1:2100, size = 1, replace = F)

# Plotting 
pred_scatter &amp;lt;- qplot(pars_noise_bias$y_obj_pred[samp,], 
                      pars_noise_bias$y_per_pred[samp,], 
      color = I(&amp;quot;#8F2727&amp;quot;), size = 1, alpha = .01) +
  ggtitle(paste0(&amp;quot;r = &amp;quot;, 
                 round(cor(pars_noise_bias$y_obj_pred[samp,],
                           pars_noise_bias$y_per_pred[samp,]), 2))) +
  xlab(&amp;quot;Objective Number Correct&amp;quot;) +
  ylab(&amp;quot;Perceived Number Correct&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;, 
        axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        plot.title = element_text(hjust=.5))

# Plot along with observed data to compare
obs_scatter | pred_scatter&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-17-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks pretty good! You can get a better sense of the expected variability by taking different samples from the posterior and plotting those. You will find that the correlation will vary a decent amount, which is in part due to the uncertainty in the underlying parameters, but also partly due to the random nature of the binomial generative model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-the-binomial-perception-distortion-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting the Binomial Perception Distortion Model&lt;/h1&gt;
&lt;p&gt;We can now turn our attention toward the binomial perception distortion model to see if the results come back similar. We will not go into as much detail in this section, but we will at least fit the model and take a look at the parameters to see if they come out in the expected directions. The Stan code for our next model is below:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;functions {
  // Perception distortion function
  real per_dist(real p_obj, real gamma, real delta) {
    real p_per;
    if (p_obj == 0) {
      p_per = 0;
    } else {
      p_per = delta*pow(p_obj, gamma) / (delta*pow(p_obj, gamma) + pow((1-p_obj), gamma));
    }
    return p_per;
  }
}

data {
  int&amp;lt;lower=1&amp;gt; N;     // Number of participants
  int&amp;lt;lower=1&amp;gt; n_obj; // Number of items on objective measure
  int&amp;lt;lower=1&amp;gt; n_per; // Number of items on perception measure
  int y_obj[N];       // Objective number of items correct
  int y_per[N];       // Perceived number of items correct
}

parameters {
  vector[N] theta;     // Underlying skill
  real&amp;lt;lower=0&amp;gt; gamma; // Perception distortion 
  real&amp;lt;lower=0&amp;gt; delta; // Bias
}

transformed parameters {
  vector[N] p_obj;
  vector[N] p_per;

  // Probit transforming our parameters
  for (i in 1:N) {
    p_obj[i] = Phi_approx(theta[i]);
    p_per[i] = per_dist(p_obj[i], gamma, delta);
  }
}

model {
  // Prior distributions
  theta ~ normal(0,1);
  gamma ~ lognormal(0,1);
  delta ~ lognormal(0,1);
  
  // Likelihood for both objective and perceived number of correct items
  y_obj ~ binomial(n_obj, p_obj);
  y_per ~ binomial(n_per, p_per);
}

generated quantities {
  int y_obj_pred[N];
  int y_per_pred[N];
  
  // Generate posterior predictions to compare against observed data
  y_obj_pred = binomial_rng(n_obj, p_obj);
  y_per_pred = binomial_rng(n_per, p_per);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code is very similar to before, but we have added the perception distortion function, which we will use instead of the noise and bias terms from the previous model. Time fit the model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Note that you will need to compile the model first. Mine is
# already compiled within the Rmarkdown file used to make the post:
# binomial_per_dist &amp;lt;- stan_model(&amp;quot;path_to_model/file_name.stan&amp;quot;)

# Fit the model!
fit_per_dist &amp;lt;- sampling(binomial_per_dist, 
                         data   = stan_dat, 
                         iter   = 1000, # 1000 MCMC samples
                         warmup = 300,  # 300 used for warm-up
                         chains = 3,    # 3 MCMC chains
                         cores  = 3,    # parallel over 3 cores
                         seed   = 43210)

# Once finished, check convergence using traceplots 
traceplot(fit_per_dist)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-19-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Convergence looks good! We will skip the &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; plot for brevity, but it also came back looking good.&lt;/p&gt;
&lt;p&gt;Next, let’s plot the parameters! Remember, for this model, a pattern where &lt;span class=&#34;math inline&#34;&gt;\(\gamma&amp;lt;1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta&amp;gt;1\)&lt;/span&gt; produces a Dunning-Kruger effect, so we are looking to see of the posterior distributions meet these criteria:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract posterior samples from model fit object
pars_per_dist &amp;lt;- rstan::extract(fit_per_dist)

# Prior versus posterior distribution for gamma
post_gamma &amp;lt;- ggplot() +
  geom_vline(xintercept = 1, linetype = 2) + 
  geom_density(aes(x = pars_per_dist$gamma), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dlnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(gamma)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(0,4) +
  ylim(0,7.0) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.title.x = element_text(size = 15))
# For delta
post_delta &amp;lt;- ggplot() +
  geom_vline(xintercept = 1, linetype = 2) + 
  geom_density(aes(x = pars_per_dist$delta), fill = I(&amp;quot;#8F2727&amp;quot;)) + 
  geom_line(aes(x = seq(-5,5, length.out = 300), 
                y = dlnorm(seq(-5,5, length.out = 300))),
            linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  xlab(expression(delta)) +
  ylab(&amp;quot;Density&amp;quot;) +
  xlim(0,4) +
  ylim(0,7.0) +
  theme_minimal(base_size = 15) + 
  theme(panel.grid = element_blank(),
        axis.text.y = element_blank(),
        axis.title.y = element_blank(),
        axis.title.x = element_text(size = 15))
# Plot together
post_gamma | post_delta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-20-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Once again, pretty clear evidence for the Dunning-Kruger effect! And once more, the parameters have a psychological interpretation that directly corresponds to the hypothesis underlying the Dunning-Kruger effect, which makes us more confident that the effect is indeed psychological, rather than an artefact of an arbitrary statistical modeling decision.&lt;/p&gt;
&lt;div id=&#34;a-psychological-dunning-kruger-effect-is-present-in-both-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Psychological Dunning-Kruger Effect is Present in Both Models&lt;/h2&gt;
&lt;p&gt;For one final plot, it is insightful to show what the estimated parameters for each model imply regarding the relationship beetween &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt;, which directly correspond to participants’ objective skill and perceived skill, respectively. This will be similar to previous plots that we made, but conditional on the observed data. Here we go:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot different combinations of perception distortion and bias
p_noise_bias &amp;lt;- qplot(x = colMeans(pars_noise_bias$p_obj),
                      y = colMeans(pars_noise_bias$p_per),
                      color = I(&amp;quot;#8F2727&amp;quot;), alpha = .1, size = 1) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  ggtitle(&amp;quot;Noise + Bias&amp;quot;) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust=.5),
        legend.position = &amp;quot;none&amp;quot;)
# Same, for other model
p_per_dist &amp;lt;- qplot(x = colMeans(pars_per_dist$p_obj),
                    y = colMeans(pars_per_dist$p_per),
                    color = I(&amp;quot;#8F2727&amp;quot;), alpha = .1, size = 1) +
  geom_abline(intercept = 0, slope = 1, color = I(&amp;quot;black&amp;quot;), 
              linetype = 2, size = 1) +
  ggtitle(&amp;quot;Perception Distortion&amp;quot;) +
  xlab(expression(p[&amp;quot;obj&amp;quot;])) +
  ylab(expression(p[&amp;quot;per&amp;quot;])) +
  xlim(0,1) +
  ylim(0,1) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        plot.title = element_text(hjust=.5),
        legend.position = &amp;quot;none&amp;quot;)
# Plot together
p_noise_bias | p_per_dist&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger_files/figure-html/unnamed-chunk-21-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;What you are seeing above are the posterior expectations (i.e. averages across the posterior distribution) for the &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{per}}\)&lt;/span&gt; terms for each of the participants in the dataset, and for both models separately. As noted before, the models are practically indistinguishable! More importantly, though, is that this is a “psychological space” of sorts, where we can interpret these patterns as psychological effects. That said, the Dunning-Kruger effect is clear as day here!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wait-the-dunning-kruger-effect-is-real-after-all&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Wait, the Dunning-Kruger Effect is Real After All?&lt;/h1&gt;
&lt;p&gt;Our results suggest that the Dunning-Kruger effect is indeed a real psychological phenomenon. Using two diferrent generative models with psychologically interpretable parameters, we found not only strong evidence for the effect, but revealed potential mechanisms through which the effect may arise. The first is through the concept of “perception noise”, as shown with our noise + bias model. The second is through a general “perception distortion” mechanism, captured in the perception distortion model (however, this distortion may itself be attributable to noise, so one could argue that we have identified a single mechanism). When either of these mechanisms is combined with a general over-estimation bias, the Dunning-Kruger effect appears in observed data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The Dunning-Kruger effect is saved!… perhaps… This was quite an interesting journey for me. It revealed to me, once again, just how much great work exists in the fields of mathematical psychology and cognitive science, which I took inspiration from when developing the models presented here. Moreover, it shows how powerful generative modeling can be, even for problems that may traditionally be viewed as outside the scope of mathematical modeling.&lt;/p&gt;
&lt;p&gt;Regarding the models we developed, there are some clear limitations, which I think future work could readily address. The most obvious extention, to me, would be to collect enough data from each participant to estimate a noise (or perception distortion) and bias parameters for each participant. Right now, our model assumes that these parameters are constant across participants, but this is likely untrue. A study that sampled each participant across the whole range of &lt;span class=&#34;math inline&#34;&gt;\(p_{i,\text{obj}}\)&lt;/span&gt; would be perfectly suited for this. Another interesting extention would be to test the models for &lt;a href=&#34;https://doi.org/10.1037/a0018435&#34;&gt;selective parameter influence&lt;/a&gt;, to determine if manipulating uncertainty/noise really does influence behavior in ways predicted by the models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.0 (2021-05-18)
## Platform: aarch64-apple-darwin20 (64-bit)
## Running under: macOS 12.0.1
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] httr_1.4.2           hBayesDM_1.1.1       Rcpp_1.0.8          
##  [4] bayesplot_1.8.1      rstan_2.21.2         StanHeaders_2.21.0-7
##  [7] foreach_1.5.1        patchwork_1.1.1      gghighlight_0.3.2   
## [10] ggplot2_3.3.5        tidyr_1.1.3          dplyr_1.0.7         
## [13] mvtnorm_1.1-2       
## 
## loaded via a namespace (and not attached):
##  [1] prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.29     
##  [5] utf8_1.2.2         V8_3.4.2           R6_2.5.1           plyr_1.8.6        
##  [9] ggridges_0.5.3     stats4_4.1.0       evaluate_0.14      highr_0.9         
## [13] blogdown_1.7.3     pillar_1.6.2       rlang_0.4.12       curl_4.3.2        
## [17] rstudioapi_0.13    data.table_1.14.0  callr_3.7.0        rmarkdown_2.11    
## [21] labeling_0.4.2     stringr_1.4.0      loo_2.4.1          munsell_0.5.0     
## [25] compiler_4.1.0     xfun_0.29          pkgconfig_2.0.3    pkgbuild_1.2.0    
## [29] htmltools_0.5.2    tidyselect_1.1.1   tibble_3.1.4       gridExtra_2.3     
## [33] bookdown_0.24      codetools_0.2-18   matrixStats_0.60.1 fansi_0.5.0       
## [37] crayon_1.4.1       withr_2.4.2        grid_4.1.0         jsonlite_1.7.3    
## [41] gtable_0.3.0       lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1    
## [45] formatR_1.11       scales_1.1.1       RcppParallel_5.1.4 cli_3.0.1         
## [49] stringi_1.7.6      reshape2_1.4.4     farver_2.1.0       ellipsis_0.3.2    
## [53] generics_0.1.0     vctrs_0.3.8        iterators_1.0.13   tools_4.1.0       
## [57] glue_1.6.0         purrr_0.3.4        processx_3.5.2     parallel_4.1.0    
## [61] fastmap_1.1.0      yaml_2.2.1         inline_0.3.19      colorspace_2.0-2  
## [65] knitr_1.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Generative Models Can Advance the Social, Behavioral, and Brain Sciences</title>
      <link>http://haines-lab.com/talk/generative-models-can-advance-the-social-behavioral-and-brain-sciences/</link>
      <pubDate>Wed, 14 Oct 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/talk/generative-models-can-advance-the-social-behavioral-and-brain-sciences/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rapid, precise, and reliable measurement of delay discounting using a Bayesian learning algorithm</title>
      <link>http://haines-lab.com/publication/ahn_2020/</link>
      <pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/ahn_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using Decision Theory to Model Anxiety-Impulsivity Interactions and Impulsive Behavior</title>
      <link>http://haines-lab.com/talk/using-decision-theory-to-model-anxiety-impulsivity-interactions-and-impulsive-behavior/</link>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/talk/using-decision-theory-to-model-anxiety-impulsivity-interactions-and-impulsive-behavior/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Moving beyond Ordinary Factor Analysis in Studies of Personality and Personality Disorder: A Computational Modeling Perspective</title>
      <link>http://haines-lab.com/publication/haines_psychopathology_2020/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/haines_psychopathology_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On Curbing Your Measurement Error: From Classical Corrections to Generative Models</title>
      <link>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post, we will explore how measurement error arising from imprecise parameter estimation can be corrected for. Specifically, we will explore the case where our goal is to estimate the correlation between a self-report and behavioral measure–a common situation throughout the social and behavioral sciences.&lt;/p&gt;
&lt;p&gt;For example, as someone who studies impulsivity and externalizing psychopathology, I am often interested in whether self-reports of trait impulsivity (e.g., the &lt;a href=&#34;http://www.impulsivity.org/measurement/bis11&#34;&gt;Barratt Impulsiveness Scale&lt;/a&gt;) correlate with performance on tasks designed to measure impulsive behavior (e.g., the &lt;a href=&#34;http://www.impulsivity.org/measurement/BART&#34;&gt;Balloon Analogue Risk task&lt;/a&gt;). In these cases, it is common for researchers to compute summed or averaged scores on the self-report measure (e.g., summing item responses and divding by the number of items) and use summary statistics of behavioral performance (e.g., percent risky choices) for each subject’s behavioral measure, wherein the resulting estimates are then entered into a secondary statistical model to make inference (e.g., correlation between self-reported trait and behavioral measures). Importantly, this two-stage approach to inference assumes that our summary measures both contain no measurement error, or alternatively that we have estimated these summary mesaures with perfect precision–a very strong assumption that is surely not met in practice.&lt;/p&gt;
&lt;p&gt;Here, we will explore how such assumptions can bias our statistical inferences on individual differences. As we will show, this bias arises because these two-stage approaches ignore important sources of measurement error. We will begin with an exploration of traditional methods developed within the context of classical test theory, and we will then transition to the use of more contemporary generative models. Throughout, we will explore relationships between classical and generative approaches, which are actually more similar than they are different in many ways.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;classical-corrections&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classical Corrections&lt;/h1&gt;
&lt;div id=&#34;imprecision-and-reliability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Imprecision and Reliability&lt;/h2&gt;
&lt;p&gt;At the level of a single measure, we can think of reliability as directly corresponding to precision–or how close our estimates are to the underlying “true score”. As our estimates become more precise at the individual-level, we should be able to better infer differences between individuals.&lt;/p&gt;
&lt;p&gt;For example, assume that we are interested estimating a trait score for an individual, which we measure using a 10 item self-report questionnaire. For simplicity, let’s also assume that each item requires a yes/no endorse/not endorse response (coded 1 and 0, respectively), no items are reverse scored, and all items share the same difficulty. A typical approach to score this questionnaire would be to sum up the individual items &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and divide the sum by the number of items &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; (i.e. taking the average). So, the vector of responses &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}\)&lt;/span&gt; for a single subject may look something like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\textbf{x} = [1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We then compute our observed score like so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = \frac{1}{T}\sum^{T}_{t=1}\textbf{x}_{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The resulting observed score &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is simply the proportion of yes/endorsed responses, which is &lt;span class=&#34;math inline&#34;&gt;\(X = .7\)&lt;/span&gt; in this example. We then interpret &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; as a quantitative measure of the underlying contruct. However, there are a few important, related questions worth asking regarding this estimate:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;How close is our observed score measure &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; to the “true score” we aim to measure?&lt;/li&gt;
&lt;li&gt;Can we use this measurement approach to make inference on individual differences?&lt;/li&gt;
&lt;li&gt;If so, how can we best estimate the “true score” for each individual?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;From the perspective of classical test theory, we can answer these questions by determining the reliability of our measure. Below, we will define reliability and discuss how it can be used to answer the questions above.&lt;/p&gt;
&lt;div id=&#34;defining-reliability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Defining Reliability&lt;/h3&gt;
&lt;p&gt;In psychology and education, reliability is often discussed within the context of classical test theory, which assumes that our estimates reflect some combination of the unobserved “true score” plus some error:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[X = \theta + \epsilon\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is measurement error that contaminates the “true score” &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. Note that there are different interpretations of what the truth actually is, but here we will use the following definition: &lt;strong&gt;the true score is the expected value of the observed score over many independent realizations&lt;/strong&gt; (for alternative definitions of the true score, see &lt;a href=&#34;https://psycnet.apa.org/record/1968-35040-000&#34;&gt;Lord, Novick, &amp;amp; Birnbaum, 1968&lt;/a&gt;). Mathematically, we can represent this as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta = \mathbb{E}[X]\]&lt;/span&gt;
Following this definition, the error for a single realization is then defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\epsilon = X - \mathbb{E}[X]\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{E}[\epsilon] = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\epsilon, \theta) = 0\)&lt;/span&gt;. In english, the expected error is 0, and the correlation/covariance between the error and true score is 0.&lt;/p&gt;
&lt;p&gt;Given these assumptions, reliability is then defined as the squared correlation between the true score and observed score, or &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt;. Inuitively, &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta} \rightarrow 1\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(X \rightarrow \theta\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; is attenuated to the extent that &lt;span class=&#34;math inline&#34;&gt;\(X \ne \theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As a consequence of the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\epsilon, \theta) = 0\)&lt;/span&gt;, the observed, true, and error variances have the following relationship:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_X = \sigma^{2}_\theta + \sigma^{2}_\epsilon\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; can also be thought of as the proportion of variance that the true score accounts for in the observed score (similar to &lt;span class=&#34;math inline&#34;&gt;\(R^2\)&lt;/span&gt; in regression), or alternatively as 1 minus the ratio of error variance relative to observed score variance:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho^{2}_{X, \theta} = \frac{\sigma^{2}_\theta}{\sigma^{2}_{X}} = 1 - \frac{\sigma^{2}_{\epsilon}}{\sigma^{2}_{X}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now have a few different ways to think about reliability. Now, it is time to estimate it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-reliability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Estimating Reliability&lt;/h3&gt;
&lt;p&gt;To estimate &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt;, we need to somehow estimate either the true score or error variance, neither of which are directly observed. How do we do this? The answer is actually quite nuanced, and it all comes down to how we want to conceptualize the error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If we are simply interested in how precisely we can estimate the true score from observed data, it may be best to think of &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt; as the uncertainty in our estimate across administrations of an identical mesaure (termed &lt;strong&gt;precision&lt;/strong&gt; or &lt;strong&gt;parallel forms reliability&lt;/strong&gt;). If we are interested in whether or not multiple items on a scale capture the same underlying construct, we may use something like Chronbach’s &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; (termed &lt;strong&gt;internal consistency&lt;/strong&gt;). If we are interested in consistency of a measure over time, then &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt; may best be thought of as uncommon variance across different administrations of the same measure over an arbitrary period of time (termed &lt;strong&gt;termporal consistency&lt;/strong&gt; or &lt;strong&gt;test-retest reliability&lt;/strong&gt;). Crucially, the equations above underlie all these different forms of reliability–the big difference being how we conceptualize and compute &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given our focus here on precision, the current post will explore the first form of reliability mentioned above–parallel forms reliability. Typically, parallel forms reliability is estimated by having a group people take two &lt;em&gt;identical versions&lt;/em&gt; of the same measure and then estimating the correlation between measures across individuals. Here, &lt;em&gt;identical&lt;/em&gt; means that the measures tap into the same underlying trait/construct (i.e. the underlying true score is the same), share the same item characteristics, and have the same error variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_\epsilon\)&lt;/span&gt;. Under these assumptions, the correlation between the observed scores across measure 1 and 2 (&lt;span class=&#34;math inline&#34;&gt;\(r_{X,X&amp;#39;}\)&lt;/span&gt;) is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{X,X&amp;#39;} = \frac{\sigma^{2}_\theta}{\sigma^{2}_{X}} = \rho^{2}_{X, \theta}\]&lt;/span&gt;
This relationship allows us to think of reliability in two different ways: (1) as the correlation between observed scores of identical measures, or (2) as the ratio of true score variance to observed score variance.&lt;/p&gt;
&lt;p&gt;However, what if we do not have parallel forms? Obviously, having two identical measures is infeasible in many settings. Assuming that there are no practice effects, we could always have our subjects re-take the measure and then estimate &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; as the correlation between their scores across administrations, although this would still demand more time on the part of participants.&lt;/p&gt;
&lt;div id=&#34;reliability-as-measurement-precision&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reliability as Measurement Precision&lt;/h4&gt;
&lt;p&gt;One of the key of framing reliability in terms of precision is that we can estimate &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X, \theta}\)&lt;/span&gt; from a single administration of our measure. In this section, we will employ this method and compare it to the parallel forms/test-retest method where reliability is characterized as the correlation between observed scores of identical measures.&lt;/p&gt;
&lt;p&gt;Using precision to estimate the error variance of our measure is actually pretty straightforward. Returning to the example in the introduction, say we have a vector of yes/no responses for a single indivdual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; who responded to 10 items: &lt;span class=&#34;math inline&#34;&gt;\(\textbf{x}_i = [1, 0, 1, 0, 1, 1, 1, 0, 1, 1]\)&lt;/span&gt;. As before, if we take the mean of this vector as the observed score, we have &lt;span class=&#34;math inline&#34;&gt;\(X_i = \frac{1}{T}\sum^{T}_{t=1}\textbf{x}_{i,t} = .7\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because we have binary observed data, we can view each response as a bernoulli trial, allowing us to use the bernoulli distribution to estimate the error variance and &lt;a href=&#34;https://stats.stackexchange.com/questions/29641/standard-error-for-the-mean-of-a-sample-of-binomial-random-variables&#34;&gt;standard error&lt;/a&gt;. According to the wisdom of &lt;a href=&#34;https://en.wikipedia.org/wiki/Bernoulli_distribution&#34;&gt;Wikipedia&lt;/a&gt;, the mean of a bernoulli distribution is &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the success probability (i.e. probability of observing 1 rather than 0). In our case, &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is estimated by the observed mean &lt;span class=&#34;math inline&#34;&gt;\(X_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Again taking from Wikipedia, the variance of a sum of bernoulli trials is given by &lt;span class=&#34;math inline&#34;&gt;\(pq\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is as defined above and &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the failure probability, &lt;span class=&#34;math inline&#34;&gt;\(1-p\)&lt;/span&gt;. We can then compute the standard error of measurement for this individual &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\epsilon,i}\)&lt;/span&gt; (which we can square to get &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{\epsilon,i}\)&lt;/span&gt;) by dividing the variance by &lt;span class=&#34;math inline&#34;&gt;\(T-1\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; is the number of items:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_{\epsilon,i} = \frac{pq}{T-1} = \frac{.7 \times (1-.7)}{10-1} = 0.021\]&lt;/span&gt;
The standard error of measurement corresponds to the strandard deviation of the &lt;em&gt;sampling distribution&lt;/em&gt; of our observed score. The standard error of measurement thus gives us useful information about how close our observed score is to the true score. In particular, as we increase the number of items on our measure, the sampling distribution of the mean becomes more concentrated. In turn, on average, our observed score becomes closer to the true score.&lt;/p&gt;
&lt;p&gt;If these ideas are unfamiliar to you, it is useful to visualize what happens to the sampling distribution as we increase the number of items on the questionnaire. The R code below simulates the sampling distribution for each of a set of item sizes ranging from 10 to 100, using a success probability of .7:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First load some packages we will use
library(dplyr)
library(foreach)
library(ggplot2)
library(ggridges)
library(truncnorm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# num of items and success probability
n_items    &amp;lt;- seq(10, 100, by = 5)
success_pr &amp;lt;- .7

# number of samples from sampling distribution
n_reps &amp;lt;- 10000

# Generate binomial sampling distribution
results &amp;lt;- foreach(i=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
  data.frame(T = rep(n_items[i], n_reps),
             X = replicate(n_reps, 
                           mean(rbinom(n_items[i], 1, prob = success_pr))))
}

# Compute standard deviation of the sampling distribution 
# (aka the standard error of the mean)
sem &amp;lt;- results %&amp;gt;%
  mutate(T = factor(T, levels = rev(unique(sort(T))))) %&amp;gt;%
  group_by(T) %&amp;gt;%
  summarize(se_lo = mean(X) - sd(X), # mean - 1 SEM
            se_hi = mean(X) + sd(X)) # mean + 1 SEM

# Plot some cool ridges
results %&amp;gt;% 
  mutate(T = factor(T, levels = rev(unique(sort(T))))) %&amp;gt;%
  ggplot(aes(x = X, y = T, group = T)) +
  geom_density_ridges(aes(fill = I(&amp;quot;#DCBCBC&amp;quot;), color = I(&amp;quot;white&amp;quot;)), 
                      stat = &amp;quot;binline&amp;quot;, binwidth = .01, scale = 3) +
  geom_segment(aes(x = se_lo, xend = se_lo, y = as.numeric(T),
                   yend = as.numeric(T) + 1),
               data = sem, size = 1) +
  geom_segment(aes(x = se_hi, xend = se_hi, y = as.numeric(T),
                   yend = as.numeric(T) + 1),
               data = sem, size = 1) +
  coord_cartesian(xlim = c(.3, 1)) +
  ggtitle(&amp;quot;The Bernoulli Sampling Distribution&amp;quot;) +
  xlab(&amp;quot;Sample Mean (X)&amp;quot;) +
  ylab(&amp;quot;# Items (T)&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-2-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the black bars on either side of each distribution represent the mean &lt;span class=&#34;math inline&#34;&gt;\(\pm ~1\)&lt;/span&gt; standard deviation of the sampling distribution. As you can see, with an increasing number of items, the sampling distribution becomes more concentrated and more normal (i.e. Gaussian). For us, this means that the range of possible observed scores decreases as we use more items–therefore, an obsvered score computed with a measure containing many items is probably closer to the true score than a measure with a low number of items. In other words, our uncertainty becomes lower, and our estimate more precise.&lt;/p&gt;
&lt;p&gt;Importantly, the standard error of measurement has a direct relationship to the reliability of our measure. As described by &lt;a href=&#34;https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1955.tb00054.x&#34;&gt;Lord (1955)&lt;/a&gt; (see also &lt;a href=&#34;https://doi.org/10.1177/001316445701700407&#34;&gt;Lord, 1957&lt;/a&gt;), if we calculate &lt;span class=&#34;math inline&#34;&gt;\(\sigma^{2}_{\epsilon,i}\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; individuals, the average of the resulting squared standard errors of measurement corresponds to the error variance we use to estimate reliability:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_\epsilon = \frac{1}{N}\sum_{i=1}^{N}\sigma^{2}_{\epsilon,i}\]&lt;/span&gt;
Then, if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the vector of observed scores for each individual, the total or observed variance is simply:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sigma^{2}_X = \text{Var}(X)\]&lt;/span&gt;
Remember, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; here is a vector of observed scores across all individuals. Reliability is then computed using the same formula as always:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\rho^{2}_{X,\theta} = 1 - \frac{\sigma^{2}_\epsilon}{\sigma^{2}_X}\]&lt;/span&gt;
The implication here is that we can actually use what we know about the standard error/precision of our estimates for each individual to estimate reliability pretty easily from just a single measure. In fact, assuming that the underlying true score is equivalent across measures/timepoints, this approach is equivalent to the parallel forms or test-retest reliability estimate that we discussed above. We can check this correspondence with a quick simulation, wherein we can look at the reliability estimates for each approach as a function of the number of items:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(43202)

# Number of subjects and items
n_subjs &amp;lt;- 30
n_items &amp;lt;- seq(10, 500, length.out = 30)

# Random sample of &amp;quot;true&amp;quot; scores around success prob. p = .7
theta &amp;lt;- rnorm(n_subjs, .7, .1)

# Estimate standard error of measurement (squared)
est_se2 &amp;lt;- function(x) {
  # Success and failure probability
  n &amp;lt;- length(x)
  p &amp;lt;- mean(x)
  q &amp;lt;- 1 - p

  sig2_ep_i &amp;lt;- (p*q)/(n-1)

  return(sig2_ep_i)
}

# Estimate observed and true score
rel_dat &amp;lt;- foreach(t=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Parallel form 1 (or timepoint 1 administration)
    X_all_f1 &amp;lt;- foreach(i=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
      # Simulate from binomial distribution
      rbinom(n_items[t], 1, prob = theta[i])
    }
    # Parallel form 2 (or timepoint 2 administration)
    X_all_f2 &amp;lt;- foreach(i=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
      # note theta here is equivalent to above (i.e. true scores are the same)
      rbinom(n_items[t], 1, prob = theta[i])  
    }
    
    # Computing X_i (p) for each individual
    X_f1 &amp;lt;- rowMeans(X_all_f1)
    X_f2 &amp;lt;- rowMeans(X_all_f2)
    
    # Standard arror of measurement approach (just using form/timepoint 1)
    sig2_ep &amp;lt;- mean(apply(X_all_f1, 1, est_se2)) 
    sig2_X   &amp;lt;- var(X_f1)
    
    data.frame(n_items  = n_items[t],
               rho2_pf  = cor(X_f1, X_f2), # Parallel form/test-retest approach
               rho2_sem = 1 - (sig2_ep/sig2_X))
}
# ooooooh nice :D
rel_dat %&amp;gt;%
  ggplot() +
  geom_line(aes(x = n_items, y = rho2_pf), color = I(&amp;quot;#DCBCBC&amp;quot;)) +
  geom_line(aes(x = n_items, y = rho2_sem), color = I(&amp;quot;#8F2727&amp;quot;)) +
  ggtitle(&amp;quot;Approaches to estimating reliability&amp;quot;) +
  xlab(&amp;quot;Number of Items&amp;quot;) +
  ylab(bquote(&amp;quot;Reliability (&amp;quot; ~rho[X~~&amp;quot;,&amp;quot;~theta]^2~ &amp;quot;)&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, x = 220, y = 1, label = &amp;quot;Parallel Forms&amp;quot;, 
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 320, y = .8, label = &amp;quot;Standard Error of\nMeasurement&amp;quot;, 
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-3-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It is clear from the above figure that both approaches give the same reliability estimates on average, although there is some noise due to the probabilistic nature of the simulation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-reliability-to-improve-individual-level-inference&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Using Reliability to Improve Individual-level Inference&lt;/h4&gt;
&lt;p&gt;Importantly, we can use our reliability estimate to improve our inference on individual-level true scores.&lt;/p&gt;
&lt;p&gt;Specifically, Kelley demonstrated–in as early as 1920 (see &lt;a href=&#34;https://psycnet.apa.org/record/1951-05849-000&#34;&gt;Kelley, 1947&lt;/a&gt;)–that we can estimate the true scores for each individual, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_i\)&lt;/span&gt;, by regressing the observed scores on the reliability estimate:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\theta}_i = (1-\rho^{2}_{X,\theta})\bar{X} + \rho^{2}_{X,\theta}X_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, &lt;span class=&#34;math inline&#34;&gt;\(\bar{X}\)&lt;/span&gt; is the mean of the observed scores across all subjects, given by &lt;span class=&#34;math inline&#34;&gt;\(\bar{X} = \frac{1}{N}\sum_{i=1}^{N}X_i\)&lt;/span&gt;. Intuitively, the true score for each subject is estimated by pooling the their observed score toward the group-level mean score in proportion to the reliability of the individual-level estimate. If &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X,\theta} = 1\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(X_i = \hat{\theta}_i\)&lt;/span&gt; for all individuals–there is no pooling. Conversely, as &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{X,\theta} \rightarrow 0\)&lt;/span&gt;, the individual-level observed scores have no weight at all, and the estimated true score is simply the group-level mean for each individual.&lt;/p&gt;
&lt;p&gt;In addition to estimating the true scores as observed scores pooled toward the group mean, Kelley also showed that the standard error of true scores is reduced in the following manner (for a derivation see this &lt;a href=&#34;https://education.uiowa.edu/sites/education.uiowa.edu/files/documents/centers/casma/publications/casma-technical-report-5.pdf&#34;&gt;technical report by Brennan, 2012&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\sigma}_{\epsilon} = \sigma_{X}\sqrt{1-\rho^{2}_{X,\theta}}\sqrt{\rho^{2}_{X,\theta}}\]&lt;/span&gt;
This standard error of the true score estimates is lower than that of the observed scores, which is given by &lt;span class=&#34;math inline&#34;&gt;\(\hat{\sigma}_{\epsilon} = \sigma_{X}\sqrt{1-\rho^{2}_{X,\theta}}\)&lt;/span&gt;. Comparing the equations, we see that the true score standard error incorporates an additional term, &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\rho^{2}_{X,\theta}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These equations are rather interesting for a few reasons. First, they were discovered in 1920 (100 years ago!), long before &lt;a href=&#34;https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator&#34;&gt;James-Stein estimators&lt;/a&gt; (which similarly pool individual-level estimates toward the group-level mean) made their way into statistics (check out &lt;a href=&#34;http://cda.psych.uiuc.edu/web_407_spring_2014/prediction_week5.pdf&#34;&gt;these slides&lt;/a&gt; for some cool historical notes on this).&lt;/p&gt;
&lt;p&gt;Second, they have a Bayesian interpretation! In particular, if we assume that the “true scores” have a normal prior distribution, and that true scores are distributed normally with respect to the observed scores, empirical Bayesian estimation produces posterior means that are equivalent to the Kelley true score point estimates described above (see page 22 of &lt;a href=&#34;https://books.google.com/books?id=dMnoX8YnYgsC&amp;amp;lpg=PA22&amp;amp;ots=reS41nEOre&amp;amp;dq=kelley%201947%20regress&amp;amp;pg=PP11#v=onepage&amp;amp;q=kelley%201947%20regress&amp;amp;f=false&#34;&gt;de Gruijter &amp;amp; van der Kamp, 2008&lt;/a&gt;). It is also worth noting that many popular multilevel/hierarchical modeling software packages work using something akin to empirical Bayesian estimation (e.g., &lt;a href=&#34;https://mc-stan.org/users/documentation/case-studies/tutorial_rstanarm.html&#34;&gt;&lt;code&gt;lmer&lt;/code&gt; in R&lt;/a&gt;). It goes without saying that using group-level information can be exceedingly useful to improve individual-level inference.&lt;/p&gt;
&lt;p&gt;OK, so this has all been a bit abstract. To make things more concrete, we can run a simulation to observe how the true score estimation/pooling described above works. The R code below simulates data from a binomial distribution for 20 “subjects” with success probabilities centered around &lt;span class=&#34;math inline&#34;&gt;\(.7\)&lt;/span&gt;. Additionally, we will use item sizes of 10, 30, and 100 to demonstrate how pooling effects changes as a function of the number of items in a measure (due to the effect of number of items on resulting reliability).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(43202)

# Number of subjects and items
n_subj  &amp;lt;- 20
n_items &amp;lt;- c(10, 30, 100)

# Random sample of &amp;quot;true&amp;quot; scores around .7
theta  &amp;lt;- rnorm(n_subj, .7, .1)

# Estimate observed and true score
dis_dat &amp;lt;- foreach(i=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Generate observed data for each subject using &amp;quot;true&amp;quot; score
  X_all &amp;lt;- foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
   rbinom(n_items[i], 1, prob = theta[t]) 
  }
  
  # group average observed score
  X_bar &amp;lt;- mean(rowMeans(X_all))
  
  # Reliability
  X &amp;lt;- rowMeans(X_all)
  
  # Standard arror of measurement approach
  sig2_ep  &amp;lt;- mean(apply(X_all, 1, est_se2)) 
  sig2_X   &amp;lt;- var(X)
  rho      &amp;lt;- 1 - (sig2_ep/sig2_X)

  foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Using observed scores from parallel form 1
    X_obs &amp;lt;- X_all[t,]
    X_i   &amp;lt;- mean(X_obs)
    
    data.frame(subj_num  = t,
               n_items   = n_items[i],
               theta     = theta[t],
               rho       = rho,
               X         = X_i,
               se_obs    = sd(X)*sqrt(1-rho),
               se_hat    = sd(X)*sqrt(1-rho)*sqrt(rho),
               theta_hat = (1-rho)*X_bar + rho*X_i)
  }
}

# Plot true, observed, and estimated true scores
dis_dat %&amp;gt;%
  mutate(subj_num = reorder(subj_num, theta)) %&amp;gt;%
  ggplot(aes(x = subj_num, y = theta)) + 
  geom_point(color = I(&amp;quot;black&amp;quot;)) +
  geom_point(aes(x = subj_num, y = X),
             color = I(&amp;quot;#DCBCBC&amp;quot;),
             position = position_jitter(width=.2, height=0, seed = 1)) +
  geom_linerange(aes(x = subj_num,
                    ymin = X - 1.96*se_obs,
                    ymax = X + 1.96*se_obs),
                color = I(&amp;quot;#DCBCBC&amp;quot;),
                position = position_jitter(width=.2, height=0, seed = 1)) +
  geom_point(aes(x = subj_num, y = theta_hat), 
             color = I(&amp;quot;#8F2727&amp;quot;),
             position = position_jitter(width=.2, height=0, seed = 2)) +
  geom_linerange(aes(x = subj_num, 
                    ymin = theta_hat - 1.96*se_hat, 
                    ymax = theta_hat + 1.96*se_hat), 
                color = I(&amp;quot;#8F2727&amp;quot;), 
                position = position_jitter(width=.2, height=0, seed = 2)) +
  geom_hline(yintercept = X_bar, linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .4, label = expression(&amp;quot;True&amp;quot;~theta[i]), 
           color = &amp;quot;black&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .3, label = expression(&amp;quot;Obs&amp;quot;~X[i]),
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .2, label = expression(&amp;quot;Est&amp;quot;~hat(theta)[i]),
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  facet_wrap(c(&amp;quot;n_items&amp;quot;), nrow = 1) +
  ggtitle(&amp;quot;Regression-Based True Score Estimates&amp;quot;) +
  xlab(&amp;quot;Subject&amp;quot;) +
  ylab(&amp;quot;Value&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x.bottom = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-4-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I don’t know about you, but I think this is pretty cool for a technique developed in 1920 :D. What we see is that the Kelley regression-based point estimates are pooled toward the group-level average (represented by the horizontal gray dotted line) in proportion to the reliability of the measure, which increases as a function of the number of items. Further, the confidence intervals for the estimated true scores &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_i\)&lt;/span&gt; are much narrower than those of the observed scores–yet they still exhibit good coverage properties of the underlying true scores &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-brief-note-on-assumptions&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A Brief Note on Assumptions&lt;/h4&gt;
&lt;p&gt;It is worth noting that the models we have discussed so far make many assumptions that might not be met in practice (i.e. normality of the sampling distribution, which is clearly not true when the number of items is low). Much of applied frequentist modeling relies on similar assumptions regarding normality when computing standard errors and p-values. Additionally, our model assumes that responses to each item are random with respect the the underlying true score, which is not true if items are of different difficulties. The need to relax these latter assumptions was the impetus for the development of what is now called &lt;em&gt;Generalizability Theory&lt;/em&gt;, or &lt;em&gt;G-Theory&lt;/em&gt;, which seeks to tease apart these various sources of error.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;using-reliability-to-disattenuate-a-correlation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using Reliability to Disattenuate a Correlation&lt;/h2&gt;
&lt;p&gt;So, it is now clear that correcting for low reliability can improve inference on individual-level true scores. However, how do these ideas then translate to measuring the correlation between true scores of two different measures? When we are interested in such quantities, we need to take into account measurement error–else we obtain an attenuted, or biased, correlation estimate, and we may falsely infer that a relationship does not exist when it does at the level of true scores.&lt;/p&gt;
&lt;p&gt;The equation for attenutation is actually quite straightforward, and it is similar to Kelley’s regression-based true score estimation equation above. If we have two measures, say measures &lt;span class=&#34;math inline&#34;&gt;\(M_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M_2\)&lt;/span&gt;, each with corresponding reliability estimates of &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{M_1,\theta_1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{M_2,\theta_2}\)&lt;/span&gt;, the correlation between the osberved scores of the measures &lt;span class=&#34;math inline&#34;&gt;\(r_{M_1,M_2}\)&lt;/span&gt; is &lt;em&gt;attenuated&lt;/em&gt; as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[r_{M_1,M_2} = \hat{r}_{\theta_1,\theta_2}\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\hat{r}_{\theta_1,\theta_2}\)&lt;/span&gt; is our estimate for what the correlation between true scores should be, but our observed correlation is &lt;span class=&#34;math inline&#34;&gt;\(r_{M_1,M_2}\)&lt;/span&gt; is attenuated by the reliability of each measure. Inuitively, if &lt;span class=&#34;math inline&#34;&gt;\(\rho^{2}_{M_1,\theta_1} = \rho^{2}_{M_2,\theta_2} = 1\)&lt;/span&gt;, then the observed scores on each measure are equal to the true scores (i.e. all &lt;span class=&#34;math inline&#34;&gt;\(\theta_1 = M_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\theta_2 = M_2\)&lt;/span&gt;), and there is no attenuation. Otherwise, the true correlation becomes attenuated in proportion to the square root of the product of each measure’s reliability.&lt;/p&gt;
&lt;p&gt;To disattenuate the observed correlation, we can just use some algebra to rearrange the equation to get:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{r}_{\theta_1,\theta_2} = \frac{r_{M_1,M_2}}{\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Although this disattenuation formula is widely accepted, how to best estimate the corresponding confidence interval–which is necessary for making statistical inference–is much more controversial. For our purposes, we will use a simplified approach, which applies the disattenuation formula above to the lower and upper bounds of the observed correlation confidence interval (see &lt;a href=&#34;https://www.jstor.org/stable/1434905?seq=1#metadata_info_tab_contents&#34;&gt;Winne &amp;amp; Belfry, 1982&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\frac{Lower}{\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}} &amp;lt; \hat{r}_{\theta_1,\theta_2} &amp;lt; \frac{Upper}{\sqrt{\rho^{2}_{M_1,\theta_1}\rho^{2}_{M_2,\theta_2}}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We will explore these disattenuation corrections in more detail after we cover generative models below.&lt;/p&gt;
&lt;div id=&#34;a-brief-note-on-disattenuation-and-the-meaning-of-test-retest-reliability&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;A Brief Note on Disattenuation and the Meaning of Test-retest Reliability&lt;/h4&gt;
&lt;p&gt;Before moving on, it is worth emphasizing the implications of the above attenuation equation for research on changes in individual differences over time. For example, if I am interested in how a psychological construct changes over the span of 1 month, I may have a group of participants take the same measure at two separate timepoints, followed by computing a correlation to determine if individual differences at time 1 are consistent at time 2. In this case, we would actually need to correct this correlation by the reliability of each measure if we wanted to infer whether or not observed changes result from actual changes in the underlying, supposedly trait-like construct (i.e. true score), versus measurement error resulting from imprecision. This is an important distinction, because confusion between these different sources of variation (i.e. actual change versus low precision) can result in strong conclusions regarding the utility of certain measures for individual differences research. If you are interested in more on this topic, I cover it in detail in &lt;a href=&#34;http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/&#34;&gt;a previous blog post&lt;/a&gt;, where I dive into the so-called “Reliability Paradox”. In fact, it isn’t so paradoxical after all.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generative-modeling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Generative Modeling&lt;/h1&gt;
&lt;p&gt;Generative models can accomplish all of what we described above pertaining to classical test theory, but in a more intuitive and easy to implement way–or at least that is what I hope to convince you of by the end of this post.&lt;/p&gt;
&lt;p&gt;When building generative models, as opposed to thinking about “true” and “observed” scores, we will think about “true” and “estimated” data-generating parameters, respectively. As opposed to referring to “error variance” or “standard errors of the mean”, we will discuss “uncertainty”–or precision–in our parameter estimates. Finally, as opposed to relying on the asymptotic assumptions necessary for us to estimate error variances using normal approximations, we will use hierarchical Bayesian modeling to jointly account for our uncertainty across all parameters of interest.&lt;/p&gt;
&lt;div id=&#34;the-goal-of-generative-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Goal of Generative Modeling&lt;/h2&gt;
&lt;p&gt;From the perspective of generative modeling, our goal is to specify a joint probability distribution that describes both the relationships between parameters within our model and the relationship between model parameters and observed data. By specifiying a generative model, all of our assumptions are made explicit, which is a useful practice conducive to theory development and testing.&lt;/p&gt;
&lt;p&gt;For the current application, we are interested in the relationship between a trait and behavioral measure, which are captured using a questionnaire and response time task, respectively. Our goal is to determine if individual differences in one measure relate to individual differences in the other. Therefore, we need to develop models of the data-generating process underlying both: (1) responding to questionnaire items, (2) response time distributions, and (3) the relationship between parameters of 1 and 2. Additionally, we need to ensure that our model accounts for the uncertainty in our individual-level parameter estimates (analagous to dissociating error from observed scores to get true scores within classical test theory). Finally, we will compare how well the generative model can uncover individual differences compared to the disattentuation approach used within the context of classical test theory.&lt;/p&gt;
&lt;div id=&#34;generative-model-of-questionnaire-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generative Model of Questionnaire Data&lt;/h3&gt;
&lt;p&gt;For the questionnaire data, we will use the bernoulli distribution, where the response to each item can be thought of as a bernoulli trial. Recall that we used the bernoulli distribution within the classical test theory example to estimate the standard error for our reliability calculation.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;bernoulli model&lt;/em&gt;, as we will now refer to it, is given by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Pr}(x_{i,t}=1) = p_{i} = 1 - \text{Pr}(x_{i,t}=0) = 1 - q_i\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Pretty simple! As in the classical test theory example, &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is the success probability, or the probability that the response of individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is 1. Then, &lt;span class=&#34;math inline&#34;&gt;\(q_i\)&lt;/span&gt; is the failure probability, or the probability that the response is 0 (which is simply &lt;span class=&#34;math inline&#34;&gt;\(1-p_i\)&lt;/span&gt;). We will then define &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; such that:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[p_i = \frac{1}{1+\text{exp}(-\theta_i)}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\theta_i ~(-\infty &amp;lt; \theta_i &amp;lt; +\infty)\)&lt;/span&gt; is unbounded, and the logistic function transforms &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; to ensure that &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is between 0 and 1. Note that we use this transformation because &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is a probability, and therefore must be between 0 and 1. We could use other types of tranformations, but I chose to use the logistic function because it may be more familiar to readers (logistic regression!) compared to other functions. In fact, this model can be thought of as a very simple Item Response Theory model, with only an “ability” parameter for each person (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;), and item difficulties and discriminabilities set to 1.&lt;/p&gt;
&lt;p&gt;Given the above generative specification, the parameter that we actually need to estimate for each individual is &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; is then determined by the logistic function.&lt;/p&gt;
&lt;p&gt;However, in addition to specifying how the observed responses are generated, we also need to specify how the individual-level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters are generated. Similar to how the classical test theory approach requires us to assume a population-level sampling distribution, the generative modeling approach requires us to assume a group-level generative distribution. For simplicity, we may assume that the individual-level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters are generated by a standard normal distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_i \sim \mathcal{N}(0,1)\]&lt;/span&gt;
In Bayesian terminology, such a group-level distribution is often referred to as a &lt;em&gt;prior distribution&lt;/em&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, but I actually do not like this terminology–I instead prefer to think of is as our generative model of the parameters themselves. My reasoning is simple–we do not actually need to specify the parameters of the group-level normal distribution like we did above. Instead, we could estimate these group-level parameters from the data itself:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\theta_i \sim \mathcal{N}(\mu_{\theta},\sigma_{\theta})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then, we need to make a generative assumption regarding the group level mean and standard deviation parameters (or a prior on the group-level parameters in typical Bayes speak). In our case, we will assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\theta} \sim \mathcal{N}(0,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\theta} \sim \text{half-}\mathcal{N}(0,1)\)&lt;/span&gt;. Here, &lt;span class=&#34;math inline&#34;&gt;\(\text{half-}\mathcal{N}\)&lt;/span&gt; indicates a half-normal distribution, which only places probability mass on values greater than 0 (given that standard deviations must be positive). Note that the generative model is now considered hierarchical–as we fit the model, the the individual-level parameters will influence the group-level parameters, which will in turn influence the all individual-level parameters. Much like for the regression-based true score estimates in classical test theory, our individual-level parameters will become pooled toward the group-level mean, which will also shrink the uncertainty intervals at the individual-level.&lt;/p&gt;
&lt;p&gt;Because it is difficult to get a sense of how to interpret these generative assumptions with respect to the outcome, we can generate samples from the model to observe the distribution of &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; and ensure that it aligns with our expectations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of samples we will draw from the priors on group-level dist
n_draw &amp;lt;- 12
# Number of individual-level theta paramaeter drawn from group dist 
n_subj &amp;lt;- 30

# Loop through and plot
foreach(i=1:n_draw, .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Sample group-level parameters from priors 
  mu_theta &amp;lt;- rnorm(1,0,1)
  sig_theta &amp;lt;- rtruncnorm(1,0,1,a=0,b=Inf) # normal dist truncated at 0
  
  # Sample individual level parameters from group distribution
  theta &amp;lt;- rnorm(n_subj, mu_theta, sig_theta)
  # Logistic transform to ensure 0 &amp;lt; p &amp;lt; 1
  p &amp;lt;- 1/(1+exp(theta))
  data.frame(n_draw = rep(i, n_subj),
             p      = p)
} %&amp;gt;%
  ggplot(aes(x = p)) +
  geom_histogram(fill = I(&amp;quot;#8F2727&amp;quot;)) +
  xlab(expression(p[i])) +
  facet_wrap(&amp;quot;n_draw&amp;quot;, ncol = 3, scales = &amp;quot;free_y&amp;quot;) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-5-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Note that each of these 12 panels are separate samples from the priors on the group-level distribution (i.e. separate samples from &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\theta} \sim \mathcal{N}(0,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\theta} \sim \text{half-}\mathcal{N}(0,1)\)&lt;/span&gt;). Within each panel, 30 individual-level &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; parameters have been sampled, which we then transformed to the bernoulli success probability &lt;span class=&#34;math inline&#34;&gt;\(p_i\)&lt;/span&gt; plotted above.&lt;/p&gt;
&lt;p&gt;From the panels, we can see that our generative specification can produce many different distributions of possible success probabilities–in other words, our model is relatively uninformative with respect to what the individual-level success probabilities will be. For our application, this is just fine! Because this is such a simple model, I feel comfortable looking at only the success probability generated from our model. When models are more complex, it is useful to do more rigorous prior predictive simulations, which simulate to the level of observed data.&lt;/p&gt;
&lt;div id=&#34;generative-model-parameters-vs-classical-test-theory-true-scores&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Generative Model Parameters vs Classical Test Theory True Scores&lt;/h4&gt;
&lt;p&gt;Before moving on, we can take a look at how the generative model estimates compare to the classical test theory regression-based true score estimates that we computed above. As a refresher, the regression-based true score estimates were pooled toward the group-level mean, and they were also more precise (i.e. smaller confidence intervals) compared to the observed score counterparts.&lt;/p&gt;
&lt;p&gt;First, here is the Stan code we will use to fit the generative bernoulli model (note that the model is saved in the R envirnoment to the variable &lt;code&gt;m_bernoulli&lt;/code&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load in rstan first
library(rstan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N;      // # of subjects
    int N_items; // # of timepoints
    int Y[N, N_items]; // Responses for each subject and item
}
parameters {
  // Group-level parameters
  // SDs
  real&amp;lt;lower=0&amp;gt; sigma_theta;
  // means
  real mu_theta;
  
  // Individual-level parameters
    vector[N] theta_pr;
}
transformed parameters {
  // Individual-level parameters 
  vector[N] theta;
  
  // Compute individual-level parameters from non-centered parameterization
  theta = mu_theta + sigma_theta * theta_pr;
}
model {
  // Priors on group-level means
  mu_theta ~ normal(0, 1);
  
  // Priors on group-level SDs
  sigma_theta ~ normal(0, 1);
  
  // Priors on individual-level parameters
  theta_pr ~ normal(0, 1);
    
  // For each subject
  for (i in 1:N) {
    // self-report model
    Y[i,:] ~ bernoulli_logit(theta[i]);
  }
}
generated quantities {
  vector[N] p;
  // Success probability estimate for each individual
  p = inv_logit(theta);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The Stan code above uses a &lt;a href=&#34;https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html&#34;&gt;non-centered parameterization&lt;/a&gt; of the group-level portion of the model &lt;a href=&#34;http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/&#34;&gt;for reasons described in a previous post&lt;/a&gt;, but otherwise is mathematically equivalent to the generative model as described by the equations above.&lt;/p&gt;
&lt;p&gt;The R code below fits the generative bernoulli model to the same data we used above when estimating true scores, with item sizes of 10, 30, and 100 to demonstrate the effects of pooling:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# seeded to reproduce same theta&amp;#39;s and data as above for comparison
set.seed(43202) 

# Number of subjects and items
n_subj  &amp;lt;- 20
n_items &amp;lt;- c(10, 30, 100)

# Random sample of &amp;quot;true&amp;quot; scores around .7
theta  &amp;lt;- rnorm(n_subj, .7, .1)

# Estimate observed and true score
gen_dat &amp;lt;- foreach(i=seq_along(n_items), .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Generate observed data for each subject using &amp;quot;true&amp;quot; score
  X_all &amp;lt;- foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
   rbinom(n_items[i], 1, prob = theta[t]) # theta same as in above example
  }
  # Fit generative model
  fit_bernoulli &amp;lt;- sampling(m_bernoulli,
                            data   = list(N = n_subj,
                                          N_items = n_items[i],
                                          Y = X_all),
                            iter   = 3000,
                            warmup = 500,
                            chains = 4,
                            cores  = 4,
                            seed  = 43202) 
  pars &amp;lt;- rstan::extract(fit_bernoulli)
  foreach(t=seq_along(theta), .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Using observed scores from parallel form 1
    bayes_est &amp;lt;- pars$p[,t]
    hdi &amp;lt;- hBayesDM::HDIofMCMC(bayes_est)
    
    data.frame(subj_num  = t,
               n_items   = n_items[i],
               theta     = theta[t],
               bayes_theta = mean(bayes_est),
               bayes_lo    = hdi[1],
               bayes_hi    = hdi[2])
  }
}

# Plot true, observed, and estimated true scores
dis_dat %&amp;gt;%
  full_join(gen_dat) %&amp;gt;%
  mutate(subj_num = reorder(subj_num, theta)) %&amp;gt;%
  ggplot(aes(x = subj_num, y = theta)) + 
  geom_point(color = I(&amp;quot;black&amp;quot;)) +
  geom_point(aes(x = subj_num, y = theta_hat), 
             color = I(&amp;quot;#DCBCBC&amp;quot;),
             position = position_jitter(width=.3, height=0, seed = 2)) +
  geom_linerange(aes(x = subj_num, 
                    ymin = theta_hat - 1.96*se_hat, 
                    ymax = theta_hat + 1.96*se_hat), 
                color = I(&amp;quot;#DCBCBC&amp;quot;), 
                position = position_jitter(width=.3, height=0, seed = 2)) +
  geom_point(aes(x = subj_num, y = bayes_theta), 
             color = I(&amp;quot;#8F2727&amp;quot;),
             position = position_jitter(width=.3, height=0, seed = 1)) +
  geom_linerange(aes(x = subj_num, 
                    ymin = bayes_lo, 
                    ymax = bayes_hi), 
                color = I(&amp;quot;#8F2727&amp;quot;),
                position = position_jitter(width=.3, height=0, seed = 1)) +
  geom_hline(yintercept = X_bar, linetype = 2, color = I(&amp;quot;gray&amp;quot;)) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .4, label = expression(&amp;quot;True&amp;quot;~theta[i]), 
           color = &amp;quot;black&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .3, label = expression(&amp;quot;Est&amp;quot;~hat(theta)[i]),
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = 15, y = .2, label = expression(&amp;quot;Gen&amp;quot;~hat(theta)[i]), 
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  facet_wrap(c(&amp;quot;n_items&amp;quot;), nrow = 1) +
  ggtitle(&amp;quot;Regression-Based True Scores vs\nGenerative Model Estimates&amp;quot;) +
  xlab(&amp;quot;Subject&amp;quot;) +
  ylab(&amp;quot;Value&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        axis.text.x.bottom = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-8-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, I think this is super cool! What we see is that the generative model expectations (i.e. the posterior means) and uncertainty intervals (i.e. the 95% highest density intervals) are &lt;em&gt;very&lt;/em&gt; similar to the corresponding regression-based true score estimates and 95% confidence intervals. In fact, the point esitmates for both approaches are almost indistinguishable. Given that the regression-based true scores have a Bayesian interpretation, this should not be too surprising, yet it is still nice to see it work out empirically.&lt;/p&gt;
&lt;p&gt;More generally, this example demonstrates that generative models can give us the same “true scores” that classical test theory does, yet we do not have to compute reliability, etc., to get there. Instead, we made generative, distributional assumptions regarding how our model parameters related to each other (e.g., the group-level generative model) and to the observed data. Then, we took the posterior expectation (i.e. the posterior mean) of the individual-level parameters to get our best estimates of the “true” underlying data-generating parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;generative-model-of-response-time-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Generative Model of Response Time Data&lt;/h3&gt;
&lt;p&gt;For the response time model, we will assume that response times for each individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and for each trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; arise from a normal distribution, which we will term the &lt;em&gt;normal model&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{RT}_{i,t} \sim \mathcal{N}(\delta_i, \sigma_i)\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_i\)&lt;/span&gt; indicate the mean and standard deviation, respectively, of the response time distribution for individual &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Of course, response times are not typically normally distributed in many beahvioral tasks, but we will ignore that for this demonstration.&lt;/p&gt;
&lt;p&gt;Like the bernoulli model for the questionnaire data, we can then assume that the
individual-level normal model parameters are themselves generated by normal distributions such that &lt;span class=&#34;math inline&#34;&gt;\(\delta_i \sim \mathcal{N}(\mu_{\delta},\sigma_{\delta})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{log}(\sigma_i) \sim \mathcal{N}(\mu_{\sigma},\sigma_{\sigma})\)&lt;/span&gt;. To specify the group-level parameters, it is important to remember that response times are positive valued, and typically greater than 200 milliseconds. We could encode this into the model in the prior distribution. However, for the sake of brevity, we will not focus too much on this.&lt;/p&gt;
&lt;p&gt;For the &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; parameters, we can assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\delta} \sim \mathcal{N}(1,1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\delta} \sim \text{half-}\mathcal{N}(0,0.5)\)&lt;/span&gt;. Next, for the &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; parameters, we can assume that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\sigma} \sim \mathcal{N}(-1,0.2)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\sigma} \sim \text{half-}\mathcal{N}(0,0.5)\)&lt;/span&gt;. This parameterization whould ensure that response times are generally above 0, but there is still a good amount of variation. It is much harder to form an intuition for what types of response time distributions these priors would produce. Therefore, let us simulate!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of samples we will draw from the priors on group-level dist
n_draw &amp;lt;- 10
# Number of individual-level theta paramaeter drawn from group dist 
n_subj &amp;lt;- 5
# Number of trials to simulate from individual-level normal distributions
n_trials &amp;lt;- 200

# Loop through and plot
foreach(d=1:n_draw, .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Sample group-level parameters from priors 
  mu_delta  &amp;lt;- rnorm(1,1,1)
  sig_delta &amp;lt;- rtruncnorm(1,0,.5,a=0,b=Inf) # normal dist truncated at 0
  mu_sigma  &amp;lt;- rnorm(1,-1,.2)
  sig_sigma &amp;lt;- rtruncnorm(1,0,.5,a=0,b=Inf)
  
  # Sample individual-level parameters from group dist
  delta &amp;lt;- rnorm(n_subj, mu_delta, sig_delta)
  sigma &amp;lt;- exp(rnorm(n_subj, mu_sigma, sig_sigma))
    
  foreach(i=1:n_subj, .combine = &amp;quot;rbind&amp;quot;) %do% {
    # Sample individual level response times from indiv dist
    RT &amp;lt;- rnorm(n_trials, delta[i], sigma[i])
    
    data.frame(n_draw = rep(d, n_trials),
               n_subj = rep(i, n_trials),
               RT     = RT)
  }
} %&amp;gt;%
  ggplot(aes(x = RT)) +
  geom_histogram(fill = I(&amp;quot;#8F2727&amp;quot;), binwidth = .5) +
  xlab(&amp;quot;Response Time&amp;quot;) +
  facet_grid(n_draw ~ n_subj, scales = &amp;quot;free&amp;quot;) +
  coord_cartesian(xlim = c(-2, 2)) +
  theme_minimal(base_size = 12) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, the rows represent different draws for the group-level distribution parameters, where the columns are the response time distributions generated by 5 different “subjects” drawn from the given group-level parameters. Note also that I have zoomed in so that the x-axis is between -2 and 2. There are clearly some aspects that we could improve on. For example, many response times are below 0, which is not possible (although this is not relevant for our example given our use of simulated data). That said, feel free to modify the priors to get a sense of what happens!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-joint-generative-model-of-questionnaire-and-response-time-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Joint Generative Model of Questionnaire and Response Time Data&lt;/h3&gt;
&lt;p&gt;We have now demonstrated how generative models relate to classical test theory through the use of regression-based true score estimates, and we have developed generative models for both the questionnaire and response time data that we would like to use to make inference on individual differences. In the current application, this amounts to estimating a correlation between the generative parameters for each task, or the correlation between &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Although there are many ways to estimate such a correlation, one straightforward method is to assume that &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; are drawn from a multivariate normal distribution as opposed to from independent normal distributions. Mathematically, we can make this change by modifying &lt;span class=&#34;math inline&#34;&gt;\(\theta_i \sim \mathcal{N}(\mu_{\theta},\sigma_{\theta})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i \sim \mathcal{N}(\mu_{\delta},\sigma_{\delta})\)&lt;/span&gt; to instead be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{bmatrix} \theta_{i} \\ \delta_{i} \end{bmatrix} \sim \text{MVNormal} \bigg (\begin{bmatrix} \mu_{\theta} \\ \mu_{\delta} \end{bmatrix}, \mathbf{S}  \bigg )\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf S\)&lt;/span&gt; is a covariance matrix, which can be decomposed into the group-level standard deviations and a 2 by 2 correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R\)&lt;/span&gt; that captures the correlation between the individual-level generative parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\mathbf S &amp;amp; = \begin{pmatrix} \sigma_{\theta} &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\delta} \end{pmatrix} \mathbf R \begin{pmatrix} \sigma_{\theta} &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\delta} \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, the correlation matrix contains one free parameter on the off-diagonal, &lt;span class=&#34;math inline&#34;&gt;\(\rho_{\theta,\delta}\)&lt;/span&gt;, which is the correlation of interest:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*}
\mathbf R &amp;amp; = \begin{pmatrix} 1 &amp;amp; \rho_{\theta,\delta} \\ \rho_{\theta,\delta} &amp;amp; 1 \end{pmatrix}
\end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The only other change we need to make is to set a prior distribution on the correlation matrix–or just &lt;span class=&#34;math inline&#34;&gt;\(\rho_{\theta,\delta}\)&lt;/span&gt; in our case. For our purposes, we will assume a non-informative prior, given by &lt;span class=&#34;math inline&#34;&gt;\(\mathbf R \sim \text{LKJcorr} (1)\)&lt;/span&gt;. The &lt;a href=&#34;http://bois.caltech.edu/distribution_explorer/multivariate_continuous/lkj.html&#34;&gt;LKJ distribution&lt;/a&gt; assumes that the correlations on the off-diagonal are &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta-distributed&lt;/a&gt;, ensuring that our correlation is between 0 and 1. In our case, the distribution is uniform across -1 to 1.&lt;/p&gt;
&lt;p&gt;We are now ready to fit the model! The Stan code is given by the following script, which is assigned to the &lt;code&gt;m_joint_RT_Bern&lt;/code&gt; variable in the R environment:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N;             // # of subjects
    int N_items;       // # of items
    int T;             // max # of RT trials across subjects
    real RT[N, T];     // Reaction times for each subject and trial
    int Y[N, N_items]; // Responses for each subject and item
}
parameters {
  // Group-level parameters
  // correlation matrix (cholesky factor for faster computation)
  cholesky_factor_corr[2] R_chol;  
  // SDs
  vector&amp;lt;lower=0&amp;gt;[2] pars_sigma; 
  real&amp;lt;lower=0&amp;gt; sigma_SD;
  // means
  vector[2] pars_mu;
  real sigma_mu;
  
  // Individual-level parameters
    matrix[2,N] pars_pr; 
    vector[N] sigma_pr;
}
transformed parameters {
  // Individual-level parameter off-sets (for non-centered parameterization)
  matrix[2,N] pars_tilde;
  
  // Individual-level parameters 
  vector[N] theta;
  vector[N] delta;
  vector[N] sigma;
  
  // Construct inidividual offsets (for non-centered parameterization)
  pars_tilde = diag_pre_multiply(pars_sigma, R_chol) * pars_pr;
  
  // Compute individual-level parameters from non-centered parameterization
  for (i in 1:N) {
    theta[i] = pars_mu[1] + pars_tilde[1, i];
    delta[i] = pars_mu[2] + pars_tilde[2, i];
    sigma[i] = exp(sigma_mu + sigma_SD * sigma_pr[i]);
  }
}
model {
  // Prior on cholesky factor of correlation matrix
  R_chol ~ lkj_corr_cholesky(1);
  
  // Priors on group-level means
  pars_mu[1]  ~ normal(0, 1);
  pars_mu[2]  ~ normal(1, 1);
  sigma_mu ~ normal(-1, .2);
  
  // Priors on group-level SDs
  pars_sigma[1] ~ normal(0, 1);
  pars_sigma[2] ~ normal(0, .5);
  sigma_SD   ~ normal(0, .5);
  
  // Priors on individual-level parameters
  to_vector(pars_pr)  ~ normal(0, 1);
  to_vector(sigma_pr) ~ normal(0, 1);
    
  // For each subject
  for (i in 1:N) {
    // response time model
    RT[i,1:T] ~ normal(delta[i], sigma[i]);
    
    // self-report model
    Y[i,:] ~ bernoulli_logit(theta[i]);
  }
}
generated quantities { 
  corr_matrix[2] R;
    // Reconstruct correlation matrix from cholesky factor
  R = R_chol * R_chol&amp;#39;;
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Like the Stan code for the bernoulli model, the code above is using a non-centered parameterization for the group-level portion of the model. Additionally, we are using a Cholesky decomposition to better estimate the correlation matrix. These are not important for the results, and for our current purposes we can think of these modification as simply ways to more efficiently fit the models. Regardless, the model in the Stan code above is mathematically equivalent to the model described by the equations in the text.&lt;/p&gt;
&lt;p&gt;Time to fit! First, we will load some relevant R packages:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mvtnorm)
library(doParallel)
library(rstan)
library(hBayesDM)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, the R code below will simulate data where the correlation between the individual-level generative parameters &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; (or the “true scores”) varies from 0 to 1. Then, we will use the classical, reliability-based disattenuation formula along with the joint generative model to recover the true correlation.&lt;/p&gt;
&lt;p&gt;The simulation uses 100 “participants”. Additionally, the questionnaire has 10 items, and the response time task has 50 trials. To the code!:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(43201)

# Number of subjects and items/trials for questionnaire/task
n_subj   &amp;lt;- 100
n_items  &amp;lt;- 10
n_trials &amp;lt;- 50

# Create grid of true generating correlations in (0,1)
true_r &amp;lt;- seq(0,1,length.out = 20)

# Parallel cluster
cl &amp;lt;- makeCluster(4)
registerDoParallel(cl)

# Parallelize across grid of true correlations
joint_dat &amp;lt;- foreach(r=seq_along(true_r), .combine = &amp;quot;rbind&amp;quot;, 
                   .packages = c(&amp;quot;mvtnorm&amp;quot;, &amp;quot;dplyr&amp;quot;, 
                                 &amp;quot;foreach&amp;quot;, &amp;quot;rstan&amp;quot;, &amp;quot;hBayesDM&amp;quot;)) %dopar% {
  
  # Contruct correlation and covariance matrices
  M  &amp;lt;- c(0, .8) # group-level means for theta and delta
  SD &amp;lt;- c(1, .1) # group-level standard deviations for theta and delta
  R  &amp;lt;- matrix(c(1, true_r[r], true_r[r], 1), nrow = 2)
  S  &amp;lt;- diag(SD) %*% R %*% diag(SD)
    
  # Draw individual-level parameters from multivariate normal
  pars &amp;lt;- rmvnorm(n_subj, M, S) %&amp;gt;%
    as.data.frame()
  theta &amp;lt;- pars[,1] # for bernoulli model
  delta &amp;lt;- pars[,2] # for normal model
  sigma &amp;lt;- rnorm(n_subj, .4, .05) # for normal model
  
  # Simulate questionnaire data (i.e. bernoulli generative model)
  X_Q_all &amp;lt;- foreach(i=1:n_subj, .combine = &amp;quot;rbind&amp;quot;) %do% {
    p  &amp;lt;- 1/(1 + exp(-theta[i])) # logistic transform
    rbinom(n_items, 1, prob = p)
  }
  # Simulate resposne time data (i.e. normal generative model)
  X_RT_all &amp;lt;- foreach(i=1:n_subj, .combine = &amp;quot;rbind&amp;quot;) %do% {
    rnorm(n_trials, delta[i], sigma[i])
  }
  
  # group averages of observed scores
  X_bar_Q  &amp;lt;- mean(rowMeans(X_Q_all))
  X_bar_RT &amp;lt;- mean(rowMeans(X_RT_all))
  
  # individual-level observed scores
  X_Q  &amp;lt;- rowMeans(X_Q_all)
  X_RT &amp;lt;- rowMeans(X_RT_all)
  
  # Average of individual-level error variances 
  sig2_ep_Q  &amp;lt;- mean(apply(X_Q_all, 1, est_se2)) # same SE function from earlier 
  sig2_ep_RT &amp;lt;- mean(apply(X_RT_all, 1, function(x) var(x)/(length(x)-1)))
  
  # Group-level observed score variance
  sig2_X_Q  &amp;lt;- var(X_Q)
  sig2_X_RT &amp;lt;- var(X_RT)
  
  # Compute reliability using SEM approach
  rho_Q  &amp;lt;- 1 - (sig2_ep_Q/sig2_X_Q)
  rho_RT &amp;lt;- 1 - (sig2_ep_RT/sig2_X_RT)

  # Observed correlation and 50% confidence interval
  obs_cor &amp;lt;- cor.test(X_Q, X_RT, conf.level = .5)
  obs_r   &amp;lt;- obs_cor$estimate
  obs_ci  &amp;lt;- obs_cor$conf.int
  
  # Disattenuated correlation and 50% confidence interval
  dis_r  &amp;lt;- obs_r / sqrt(rho_Q*rho_RT)
  dis_ci &amp;lt;- obs_ci / sqrt(rho_Q*rho_RT)
    
  # Stan data for joint generative model
  stan_data &amp;lt;- list(N       = n_subj,
                    N_items = n_items,
                    T       = n_trials,
                    Y       = X_Q_all,
                    RT      = X_RT_all)
  
  # Fit joint generative model
  fit_joint &amp;lt;- sampling(m_joint_RT_Bern, 
                        data   = stan_data,
                        iter   = 1000,
                        warmup = 200, 
                        chains = 1, 
                        cores  = 1,
                        seed   = 43201)
  pars &amp;lt;- rstan::extract(fit_joint) # extract parameters
  
  # Generative model correlation and 50% highest density interval
  bayes_r &amp;lt;- mean(pars$R[,1,2])
  hdi &amp;lt;- hBayesDM::HDIofMCMC(pars$R[,1,2], credMass = .5)
  
  # Save out data
  data.frame(true_r    = true_r[r],
             obs_r     = obs_r,
             obs_lo    = obs_ci[1],
             obs_hi    = obs_ci[2],
             dis_r     = dis_r,
             dis_lo    = dis_ci[1],
             dis_hi    = dis_ci[2],
             bayes_r   = bayes_r,
             bayes_lo  = hdi[1],
             bayes_hi  = hdi[2])
}
# Stop the parallel boi
stopCluster(cl)

# Hacky way to get some ggplot
qplot() +
  geom_line(aes(x = true_r, y = true_r), col = I(&amp;quot;black&amp;quot;),
            linetype = 2, size = 1) +
  geom_ribbon(aes(x = true_r,
                  ymin = joint_dat$obs_lo,
                  ymax = joint_dat$obs_hi,
                  fill = I(&amp;quot;gray&amp;quot;)), alpha = .2) +
  geom_ribbon(aes(x = true_r,
                  ymin = joint_dat$bayes_lo,
                  ymax = joint_dat$bayes_hi,
                  fill = I(&amp;quot;#8F2727&amp;quot;)), alpha = .2) +
  geom_ribbon(aes(x = true_r,
                  ymin = joint_dat$dis_lo,
                  ymax = joint_dat$dis_hi,
                  fill = I(&amp;quot;#DCBCBC&amp;quot;)), alpha = .2) +
  geom_line(aes(x = true_r, y = joint_dat$obs_r, col = I(&amp;quot;gray&amp;quot;))) +
  geom_line(aes(x = true_r, y = joint_dat$dis_r, col = I(&amp;quot;#DCBCBC&amp;quot;))) +
  geom_line(aes(x = true_r, y = joint_dat$bayes_r, col = I(&amp;quot;#8F2727&amp;quot;))) +
  annotate(&amp;quot;text&amp;quot;, x = .8, y = .45, label = &amp;quot;Observed&amp;quot;, 
           color = &amp;quot;gray&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = .35, y = .72, label = &amp;quot;Disattenuated&amp;quot;, 
           color = &amp;quot;#DCBCBC&amp;quot;, size = 5) +
  annotate(&amp;quot;text&amp;quot;, x = .85, y = .75, label = &amp;quot;Generative&amp;quot;, 
           color = &amp;quot;#8F2727&amp;quot;, size = 5) +
  coord_cartesian(xlim = c(0,1), ylim = c(0,1)) +
  ggtitle(&amp;quot;Self-report &amp;amp; Behavioral Task Convergence&amp;quot;) +
  xlab(&amp;quot;True Generating Correlation&amp;quot;) +
  ylab(&amp;quot;Estimated Correlation&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And there we have it! The generative and classical disattenuation approaches produce almost identical correlation estimates and 50% uncertainty intervals. In fact, if you are having trouble identifying the classical disattenutation line and intervals, it is because they are just that hard to distinguish from those of the generative model.&lt;/p&gt;
&lt;p&gt;Note that there is some noise across the range of “true” (or generating) correlations, which arises from the probabilistic nature of the simulated data. We could get a better sense of what the approaches produce in expectation by running many iterations for each true/generating correlation, but I prefer the above approach to get a sense of how each may work in a single study.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;The current post demonstrated that generative models are well-suited to make inference on individual differences, and in fact they can give us results similar to approaches developed within the framework of classical test theory (i.e. the correction for attenuation, true score estimation, etc.). Regardless of the approach you take, the results here make it clear that accounting for uncertainty (or reliability) is very important when our goal is to make inference on individual differences. Otherwise, our statistical inferences will be biased and overconfident. Moving forward, I hope that you consider accounting for such uncertainty in your models, regardless of the approach you decide to take.&lt;/p&gt;
&lt;p&gt;More generally, the generative modeling approach is easily extendable. Unlike the classical test theory approach, generative models do not require us to work out a new sampling distribution each time we modify the assumed data-generating model. Instead, we specify the model in a way that is consistent with our theory, and the joint estimation of all parameters allows us to account for uncertainty (or the lack of precision of parameter estimates) across all levels of analysis. Therefore, when doing generative modeling, we do not necessarily need to think about the reliability of our measure–instead we can think about uncertanity in our model parameters. The model can then be refined, extended, and even simulated forward to generate predictions for observed data, and once we are happy with our generative model, we can use Bayesian estimation to condition on the data. The result is a joint probability distribution that contains all the information we need to make subsequent inferences and decisions–in our case, a self-report to behavioral task correlation. If our uncertainty intervals are too wide to make a good decision, we can collect more data. Further, we can conduct formal decision analyses (e.g., see &lt;a href=&#34;https://twiecki.io/blog/2019/01/14/supply_chain/&#34;&gt;Wiecki &amp;amp; Kumar, 2019&lt;/a&gt;). Altogether, generative modeling provides a flexible framework for developing and testing theories.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-final-note&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Final Note&lt;/h1&gt;
&lt;p&gt;I hope that this post was useful for you! I sure learned a lot throughout, and it was great to finally delve into the relationships between classical test theory and generative modeling. Perhaps you will consider generative modeling for your next research project :D.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sessionInfo()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.1.0 (2021-05-18)
## Platform: aarch64-apple-darwin20 (64-bit)
## Running under: macOS 12.0.1
## 
## Matrix products: default
## BLAS:   /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRblas.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/4.1-arm64/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] parallel  stats     graphics  grDevices utils     datasets  methods  
## [8] base     
## 
## other attached packages:
##  [1] hBayesDM_1.1.1       Rcpp_1.0.8           doParallel_1.0.16   
##  [4] iterators_1.0.13     mvtnorm_1.1-2        rstan_2.21.2        
##  [7] StanHeaders_2.21.0-7 truncnorm_1.0-8      ggridges_0.5.3      
## [10] ggplot2_3.3.5        foreach_1.5.1        dplyr_1.0.7         
## 
## loaded via a namespace (and not attached):
##  [1] prettyunits_1.1.1  ps_1.6.0           assertthat_0.2.1   digest_0.6.29     
##  [5] utf8_1.2.2         V8_3.4.2           R6_2.5.1           plyr_1.8.6        
##  [9] stats4_4.1.0       evaluate_0.14      highr_0.9          blogdown_1.7.3    
## [13] pillar_1.6.2       rlang_0.4.12       curl_4.3.2         rstudioapi_0.13   
## [17] data.table_1.14.0  callr_3.7.0        rmarkdown_2.11     labeling_0.4.2    
## [21] stringr_1.4.0      loo_2.4.1          munsell_0.5.0      compiler_4.1.0    
## [25] xfun_0.29          pkgconfig_2.0.3    pkgbuild_1.2.0     htmltools_0.5.2   
## [29] tidyselect_1.1.1   tibble_3.1.4       gridExtra_2.3      bookdown_0.24     
## [33] codetools_0.2-18   matrixStats_0.60.1 fansi_0.5.0        crayon_1.4.1      
## [37] withr_2.4.2        grid_4.1.0         jsonlite_1.7.3     gtable_0.3.0      
## [41] lifecycle_1.0.0    DBI_1.1.1          magrittr_2.0.1     formatR_1.11      
## [45] scales_1.1.1       RcppParallel_5.1.4 cli_3.0.1          stringi_1.7.6     
## [49] farver_2.1.0       ellipsis_0.3.2     generics_0.1.0     vctrs_0.3.8       
## [53] tools_4.1.0        glue_1.6.0         purrr_0.3.4        processx_3.5.2    
## [57] fastmap_1.1.0      yaml_2.2.1         inline_0.3.19      colorspace_2.0-2  
## [61] knitr_1.37&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Anxiety Modulates Preference for Immediate Rewards among Trait-Impulsive Individuals: A Hierarchical Bayesian Analysis</title>
      <link>http://haines-lab.com/publication/haines_cps_2020/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/haines_cps_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A computational model of the Cambridge gambling task with applications to substance use disorders</title>
      <link>http://haines-lab.com/publication/romeu_haines_dad_2019/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/romeu_haines_dad_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Thinking generatively: Why do we use atheoretical statistical models to test substantive psychological theories?</title>
      <link>http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-reliability-paradox&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Reliability Paradox&lt;/h1&gt;
&lt;div id=&#34;defining-reliability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Defining &lt;em&gt;Reliability&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;In 2017, &lt;a href=&#34;https://link.springer.com/article/10.3758/s13428-017-0935-1&#34;&gt;Hedge, Powell, and Sumner (2017)&lt;/a&gt; conducted a study to determine the &lt;strong&gt;&lt;em&gt;reliability&lt;/em&gt;&lt;/strong&gt; of a variety of of behavioral tasks. Reliability has many different meanings throughout the psychological literature, but what Hedge et al. were interested in was how well a behavioral measure &lt;em&gt;consistently ranks individuals&lt;/em&gt;. In other words, when I have people perform a task and then measure their performance, does the measure that I use to summarize their behavior show high &lt;em&gt;test-retest reliability&lt;/em&gt;? Those studying individual differences—including but not limited to personality psychologists, clinical psychologists, and many developmental psychologists—are likley to be familiar with test-retest reliability, as we often desire measures that help us understand differences between people.&lt;/p&gt;
&lt;p&gt;Despite what many of us may first think when we hear the word &lt;em&gt;reliable&lt;/em&gt;, as Hedge et al. note, test-retest reliability is only one of many different definitions of reliable used throughout the psychological literature. For example, when someone states that an effect is reliable, they often mean that the effect is &lt;em&gt;easily replicable&lt;/em&gt;.&lt;/p&gt;
&lt;div id=&#34;replicable-behavioral-effects-arent-reliabile&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Replicable Behavioral Effects Aren’t Reliabile?&lt;/h3&gt;
&lt;p&gt;A commonly cited example of a &lt;em&gt;replicable effect&lt;/em&gt; is the &lt;a href=&#34;https://en.wikipedia.org/wiki/Stroop_effect&#34;&gt;Stroop effect&lt;/a&gt;, which is often measured as the mean difference in response time between incongruent and congruent word-color pairs. For example, when tasked with responding to the color of a word, people tend to respond much more quickly when the word is consistent with its color (“Red” colored red) compared to when it is not (“Red” colored blue). Since the &lt;a href=&#34;http://psychclassics.yorku.ca/Stroop/&#34;&gt;original study in 1935&lt;/a&gt;, this basic effect of an average difference in response times between congruent and incongruent trials has been replicated countless times (see &lt;a href=&#34;https://psycnet.apa.org/fulltext/1991-14380-001.html&#34;&gt;MacLeod, 1991&lt;/a&gt;), thereby becoming one of the most well-known effects in all of psychology. In fact, at the time of writing this post, the original 1935 paper has &lt;em&gt;almost 20,000 citations&lt;/em&gt; on Google Scholar alone.&lt;/p&gt;
&lt;p&gt;Despite how replicable it is, Hedge et al. show that the Stroop effect is &lt;em&gt;unreliable&lt;/em&gt;, in that it shows low test-retest reliability. While Hedge et al. assess many examples of such effects, we will focus on the Stroop effect throughout this post for brevity. Using their first experiment as an example, they had a a group of 50 college students complete a Stroop task two separate times separated by a period of three weeks. The Stroop task consisted of three conditions containing 240 trials each: (1) incongruent, (2) neutral, and (3) congruent conditions (for 720 trials total). On each trial, participants responded to the color of a word presented on a computer monitor which could be colored either red, blue, green, and yellow. Responses were collected from key presses. The word could be the same as the font color (congruent condition; e.g., “Red” colored red), unrelated to the font color (neutral; e.g., “lot” colored red), or conflicting with the font color (incongruent; e.g., “Blue” colored red).&lt;/p&gt;
&lt;p&gt;After subjects completed the above Stroop task at both timepoints, Hedge et al. used the following &lt;em&gt;behavioral model&lt;/em&gt; to estimate each individual’s Stroop effect at both timepoints:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
  Stroop_{i,\text{time}} &amp;amp; = \overline{RT}_{i,\text{time},\text{incongruent}} - \overline{RT}_{i,\text{time}, \text{congruent}}
\end{aligned}\tag{1}
\]&lt;/span&gt;
In plain English, for each person &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; at timepoint &lt;span class=&#34;math inline&#34;&gt;\(\text{time}\)&lt;/span&gt;, the Stroop effect is equal to the average response time across incongruent trials (&lt;span class=&#34;math inline&#34;&gt;\(\overline{RT}_{i,\text{time},\text{incongruent}}\)&lt;/span&gt;) minus the average response time across congruent trials (&lt;span class=&#34;math inline&#34;&gt;\(\overline{RT}_{i,\text{time},\text{congruent}}\)&lt;/span&gt;). Then, to estimate test-retest reliability, Hedge et al. use an Intraclass Correlation Coefficient (ICC) using a two-way random effects model for absolute agreement (&lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt;). The &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt; is similar to a traditional Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, except that it is also sensitive to differences in the mean rather than just the variance of our observations. For example, if &lt;span class=&#34;math inline&#34;&gt;\(A = \{1,2,3\}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B = \{4,5,6\}\)&lt;/span&gt;, the Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; between &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt;. However, the ICC(2,1) between these vectors is attenuated to &lt;span class=&#34;math inline&#34;&gt;\(.18\)&lt;/span&gt; because the corresponding elements of each vector differ on average by some value (3 in this example). This is important when our aim is to ensure that, for example, two different coders give the same ratings to a set of stimuli.&lt;/p&gt;
&lt;p&gt;Importantly, Hedge et al. found that the test-retest reliability of the Stroop effect as measured using equation 1 was &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1) = .6\)&lt;/span&gt;, which is a surprisingly low number given how robust the Stroop effect is at the group level! As Hedge et al. discuss, with such low reliability, any research that correlates individual-level Stroop estimates with other measures (e.g., BOLD fMRI signals, personality measures, etc.) should be correcting the estimated correlations for the high measurement error of the Stroop effect, which substantially increases the sample size required to make reasonably-powered statistical inferences. Because Hedge et al. replicated this result of low test-retest reliability across both multiple groups of participants and multiple different tasks (e.g., Flanker, Navon, Go/No-go, Stop-signal), their general conclusion naturally follows that &lt;strong&gt;behavioral tasks such as the Stroop task are severely limited in their ability to distinguish between individuals, thereby calling into question their utility for individual-differences research.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;revisiting-the-reliability-paradox&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Revisiting the Reliability Paradox&lt;/h1&gt;
&lt;div id=&#34;thinking-generatively-improves-reliability&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thinking Generatively Improves Reliability&lt;/h2&gt;
&lt;p&gt;This post will show that the conclusions drawn by Hedge et al. are highly dependent on the implict assumptions of their methodology. Specifically, we will develop a statistical model that is more consistent with the &lt;strong&gt;&lt;em&gt;data-generating mechanism&lt;/em&gt;&lt;/strong&gt; underlying the group-level Stroop effect, which will allow us to estimate more precise individual-level effects. In turn, we achieve higher test-retest reliability estimates. This work is similar in kind to that of &lt;a href=&#34;https://link.springer.com/article/10.3758/s13423-018-1558-y&#34;&gt;Rouder &amp;amp; Haaf (2019)&lt;/a&gt;, although we will have a more specific focus on estimating test-retest reliability and go into a bit more depth when discussing the implications of our results in reference to the current standard practices in psychology (&lt;em&gt;I was also unfortunately not aware of this fantastic work until shortly after completing this project!&lt;/em&gt;). Additionally, we will use a model parameterization that is more familiar to those who regularly work with R with packages such as &lt;code&gt;brms&lt;/code&gt; (e.g., compare the model we develop below to this &lt;code&gt;brms&lt;/code&gt; &lt;a href=&#34;https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/adventures-in-covariance.html&#34;&gt;write-up&lt;/a&gt;), which I hope will help readers more readily generalize this material to their own work.&lt;/p&gt;
&lt;p&gt;In the following sections, we will download the Stroop data used by Hedge et al., walk through a new behavioral model of the Stroop effect, fit the new model to the data from Hedge et al., extract test-retest reliability estimates from our new model, and compare them to those that are estimated using the traditional model (equation 1). We will end by discussing the implications of our findings for individual-differences research, including suggestions for future research.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;preprocessing-and-summarizing-the-stroop-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preprocessing and Summarizing the Stroop Data&lt;/h2&gt;
&lt;p&gt;To start, we can download the Stroop data from experiment 1 of Hedge et al., which is hosted on the Open Science Foundation webpage &lt;a href=&#34;https://osf.io/cwzds/&#34;&gt;linked here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then, we use the following R code to read in and preprocess the Stroop data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For easy data manipulation
library(foreach)
library(dplyr)
library(tidyr)
library(broom)
# For pretty plots
library(ggplot2)
# For nice tables
library(knitr)
# For intra-class correlations
library(irr)
# For Bayesian modeling
library(rstan)
# For some useful utility functions
library(hBayesDM)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data path and individual subject file names
data_path &amp;lt;- &amp;quot;~/Downloads/Study1-Stroop/&amp;quot;
files_t1 &amp;lt;- list.files(data_path, pattern = &amp;quot;*1.csv&amp;quot;)
files_t2 &amp;lt;- list.files(data_path, pattern = &amp;quot;*2.csv&amp;quot;)

# Create long-format stroop task data including all subjects
long_stroop &amp;lt;- foreach(i=seq_along(files_t1), .combine = &amp;quot;rbind&amp;quot;) %do% {
  # For time 1
  tmp_t1 &amp;lt;- read.csv(file.path(data_path, files_t1[i]), header = F) %&amp;gt;%
    mutate(subj_num = i,
           time = 1)
  # For time 2 (about 3 weeks apart)
  tmp_t2 &amp;lt;- read.csv(file.path(data_path, files_t2[i]), header = F) %&amp;gt;%
    mutate(subj_num = i,
           time = 2)
  # Condition (0 = congruent, 1=neutral, 2=incongruent), 
  # Correct (1) or incorrect (0), 
  # Reaction time is in seconds
  names(tmp_t1)[1:6] &amp;lt;- names(tmp_t2)[1:6] &amp;lt;- c(&amp;quot;Block&amp;quot;, &amp;quot;Trial&amp;quot;, &amp;quot;Unused&amp;quot;, 
                                                &amp;quot;Condition&amp;quot;, &amp;quot;Correct&amp;quot;, &amp;quot;RT&amp;quot;)
  rbind(tmp_t1, tmp_t2)
}

# Compute Stroop effect for each person at each time (equation 1)
sum_stroop &amp;lt;- long_stroop %&amp;gt;%
  group_by(subj_num, time) %&amp;gt;%
  summarize(stroop_eff = mean(RT[Condition==2]) - mean(RT[Condition==0]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` has grouped output by &amp;#39;subj_num&amp;#39;. You can override using the `.groups` argument.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Peak at the data
kable(head(sum_stroop), digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;subj_num&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;time&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;stroop_eff&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.068&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.025&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.053&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.012&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.073&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Now, we have a Stroop effect estimate for each subject at each timepoint, which we can use for further analyses. One thing to note is that unlike Hedge et al., we are not using any heiristic data cleaning rules. Specifically, Hedge et al. did not include response times (RTs) that were below 100 ms or above 3 times an indivual’s absolute median deviation of RTs in their computation of the Stroop effect. One other note is that the RTs in the raw data are in seconds (not ms). Here are the basic descriptives of our data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Means and SDs of Stroop effect at each timepoint
sum_stroop %&amp;gt;%
  ungroup() %&amp;gt;%
  group_by(time) %&amp;gt;%
  summarize(N = n(),
            Mean = round(mean(stroop_eff), 3),
            SD = round(sd(stroop_eff), 3)) %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;time&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;N&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SD&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.081&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.036&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.030&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;traditional-analyses&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Traditional Analyses&lt;/h2&gt;
&lt;div id=&#34;testing-for-a-group-level-stroop-effect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Testing for a Group-level Stroop Effect&lt;/h3&gt;
&lt;p&gt;It is pretty clear from the descriptives above that there is a group-level Stroop effect, but we can conduct a &lt;em&gt;t&lt;/em&gt;-test at each timepoint regardless:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test for group-level Stroop effect at time 1
sum_stroop %&amp;gt;%
  filter(time==1) %&amp;gt;%
  {t.test(.$stroop_eff)} %&amp;gt;%
  tidy() %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.high&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;method&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;alternative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.081&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;15.418&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.071&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.092&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;One Sample t-test&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;two.sided&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, you can see that the Stroop effect is apparent at time 1, with a mean increase in RT during the incongruent trials of about .08 seconds (80 ms). We can also check time 2:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test for group-level Stroop effect at time 2
sum_stroop %&amp;gt;%
  filter(time==2) %&amp;gt;%
  {t.test(.$stroop_eff)} %&amp;gt;%
  tidy() %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.high&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;method&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;alternative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.633&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.05&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.068&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;One Sample t-test&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;two.sided&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Looks good! The Stroop effect is slightly lower at time 2, but still comes out significant at the group level. From these analyses, it is clear that the Stroop effect is indeed replicable using equation 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;test-retest-reliability&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Test-retest Reliability&lt;/h3&gt;
&lt;p&gt;To compute test-retest reliability as in Hedge et al., we will first reformat the data so that it can be used with the &lt;code&gt;irr&lt;/code&gt; package, which allows us to estimate the &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Format for test-retest analysis
stroop_unpooled &amp;lt;- sum_stroop %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(time = ifelse(time==1, &amp;quot;Stroop_T1&amp;quot;, &amp;quot;Stroop_T2&amp;quot;),
         pooled = &amp;quot;No&amp;quot;, 
         Replication = &amp;quot;Sample Mean&amp;quot;) %&amp;gt;% # these &amp;quot;pooled&amp;quot; and Replication variables will come in later
  spread(key = time, value = stroop_eff)

# Intraclass correlation of stroop effect at time 1 and 2
stroop_unpooled %&amp;gt;%
  select(Stroop_T1, Stroop_T2) %&amp;gt;%
  {icc(., model = &amp;quot;twoway&amp;quot;, type = &amp;quot;agreement&amp;quot;, unit = &amp;quot;average&amp;quot;)[c(1, 7:15)]} %&amp;gt;%
  as.data.frame() %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;subjects&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;r0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Fvalue&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df1&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.level&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;lbound&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ubound&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.578&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.951&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;46&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13.419&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.017&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.95&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.059&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.793&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we see that the &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1) = .58\)&lt;/span&gt;, which is slightly lower compared to the results reported in Hedge et al. (&lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1) = .6\)&lt;/span&gt;). Remember, we did not use the same pre-processing steps as Hedge et al. (i.e. we used all trials to compute RTs, rather than removing those beyond certain thresholds), which is why we have slightly different results. We can additionally compute a traditional Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; to assess test-retest reliability, which offers similar conclusions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Pearson&amp;#39;s correlation
stroop_unpooled %&amp;gt;%
  select(Stroop_T1, Stroop_T2) %&amp;gt;%
  {cor.test(.$Stroop_T1, .$Stroop_T2)} %&amp;gt;%
  tidy() %&amp;gt;%
  kable(digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;estimate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;statistic&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p.value&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;parameter&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.low&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;conf.high&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;method&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;alternative&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.503&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.908&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.253&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.691&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Pearson’s product-moment correlation&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;two.sided&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;In fact, Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r = .5\)&lt;/span&gt;—even lower than the &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt; estimate! This happens because the model we are using for the &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt; estimate treats the individual-level Stroop effect estimates as average units (since they are averages across all trials), which affects how variance is estimated. Still, the main conclusion holds—with a test-retest reliability estimate of only &lt;span class=&#34;math inline&#34;&gt;\(.5\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(.58\)&lt;/span&gt;, the utility of the Stroop effect for individual-differences research is limited relative to more reliable measures.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;building-a-generative-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building a Generative Model&lt;/h2&gt;
&lt;p&gt;While the above results paint a clear picture, there are a few problems with the methodology that could be improved upon to make better inference on individual-level Stroop estimates.&lt;/p&gt;
&lt;p&gt;First, when we average across the RT data before including the averages in a model (i.e. to compute the &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt;), we are throwing away useful information by making the assumption that individual-level Stroop effects can be estimated without measurement error. We should be able to do much better by including &lt;em&gt;all trials&lt;/em&gt; (not just averages) in the statistical model that we use to estimate test-retest reliability, which will allow us to build a hierarchical model that can pool information across individuals. By building our model to respect the structure of our data (e.g., trials within timepoints within subjects), we are able to gain a better understanding of the data-generating process that gives rise to the Stroop effect.&lt;/p&gt;
&lt;p&gt;Second, the Stroop task exhibits practice effects, which could attenuate &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt; estimates. For example, if everyone becomes faster by timepoint 2, then the &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1)\)&lt;/span&gt; will go down even if their are consistent individual differences across timepoints. In fact, plotting out the Stroop effects at each timepoint reveals evidence for practice effects:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Test-retest plot
stroop_unpooled %&amp;gt;%
  ggplot(aes(x = Stroop_T1, y = Stroop_T2)) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = &amp;quot;black&amp;quot;, size = 1) +
  geom_point(color = I(&amp;quot;#b5000c&amp;quot;)) +
  theme_minimal(base_size = 20) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-9-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the black dotted line indicates the identity line, which is where all the red points (unpooled individual-level Stroop estimates) would fall if all subjects had the same Stroop effect estimates at each timepoint. Overall, we see that Stroop effects at time 2 are generally lower than at time 1, which is indicative of some form of practice effect.&lt;/p&gt;
&lt;div id=&#34;the-hierarchical-bayesian-approach&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Hierarchical Bayesian Approach&lt;/h3&gt;
&lt;p&gt;Hierarchical Bayesian Analysis (HBA) offers a statistical framework that allows us to develop a model that better captures the data-generating distribution of the group-level Stroop effect. Specifically, HBA simultaneously: (1) uses information from each individual subject to inform a group-level estimate, and (2) uses group-level information to inform all individual-level estimates. Also known as partial pooling, the structure imposed by a hierarchical model therefore allows us to more precisely estimate individual-level effects compared to methods that do not use pooling, which can lead to better estimates of data-generating parameters (e.g., &lt;a href=&#34;https://ccs-lab.github.io/papers/ahn2011jnpe/&#34;&gt;Ahn et al., 2011&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Before walking through the equations, let’s first determine how we would like to structure our hierarchical model in a way that captures our assumptions. When most psychologists think of hierarchical modeling, they think of estimating “random slopes” or “random intercepts”, or some combination of the two. This style of thinking can sometimes obscure our actual intentions. Instead, we will think of hierarchical models as a way to encode our assumptions regarding different behavioral effects in the Stroop task. Specifically, we want a model that does the following:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Captures uncertainty in individual-level Stroop estimates from within-subject variability,&lt;/li&gt;
&lt;li&gt;Estimates the difference in RT between incongruent and congruent trials,&lt;/li&gt;
&lt;li&gt;Estimates the correlation between the difference score from (2) between timepoint 1 and 2 across subjects (i.e. &lt;em&gt;test-retest reliability&lt;/em&gt;), and&lt;/li&gt;
&lt;li&gt;Does 1-3 within a single hierarchical model&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is worth emphasizing that there are multiple ways we could construct a model to encode these assumptions (see Rouder &amp;amp; Haaf’s word mentioned previously), and the model we will develop is only one example. Now, here is the model we will use:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\text{RT}_{i,t} &amp;amp; \sim \text{Normal} (\mu_{i,t}, \sigma_{\text{condition}}) \\
\mu_{i,t} &amp;amp; = (\beta_{\text{congruent}_{1,i}} + \beta_{\Delta_{1,i}} \cdot I_{\text{condition}}) \cdot I_\text{time} + \\
      &amp;amp; ~~~~~ (\beta_{\text{congruent}_{2,i}} + \beta_{\Delta_{2,i}}  \cdot I_{\text{condition}}) \cdot (1-I_\text{time}) \\
\begin{bmatrix} \beta_{\Delta_{1,i}} \\ \beta_{\Delta_{2,i}} \end{bmatrix} &amp;amp; \sim \text{MVNormal} \bigg (\begin{bmatrix} \mu_{\beta_{\Delta_{1}}} \\ \mu_{\beta_{\Delta_{2}}} \end{bmatrix}, \mathbf{S}  \bigg ) \\
\mathbf S     &amp;amp; = \begin{pmatrix} \sigma_{\beta_{\Delta_{1}}} &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\beta_{\Delta_{2}}} \end{pmatrix} \mathbf R \begin{pmatrix} \sigma_{\beta_{\Delta_{1}}} &amp;amp; 0 \\ 0 &amp;amp; \sigma_{\beta_{\Delta_{2}}} \end{pmatrix} \\
\beta_{\text{congruent}_{1,i}} &amp;amp; \sim \text{Normal} (\mu_{\beta_{\text{congruent}_1}}, \sigma_{\beta_{\text{congruent}_1}}) \\
\beta_{\text{congruent}_{2,i}} &amp;amp; \sim \text{Normal} (\mu_{\beta_{\text{congruent}_2}}, \sigma_{\beta_{\text{congruent}_2}}) \\
\mu_{\beta_{\text{congruent}}}    &amp;amp; \sim \text{Normal} (0, 1) \\
\mu_{\beta_{\Delta}}            &amp;amp; \sim \text{Normal} (0, 1) \\
\sigma_{\beta_{\text{congruent}}} &amp;amp; \sim \text{HalfCauchy} (0, 1) \\
\sigma_{\beta_{\Delta}}         &amp;amp; \sim \text{HalfCauchy} (0, 1) \\
\sigma_{\text{condition}}           &amp;amp; \sim \text{HalfCauchy} (0, 1) \\
\mathbf R                           &amp;amp; \sim \text{LKJcorr} (1)
\end{align*}\tag{2}
\]&lt;/span&gt;
Obviously, there is a lot going on here—yet, at its core, the model is not much different from a traditional linear regression model. Above, &lt;span class=&#34;math inline&#34;&gt;\(\text{RT}_{i,t}\)&lt;/span&gt; indicates the response time for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. We assume that each &lt;span class=&#34;math inline&#34;&gt;\(\text{RT}_{i,t}\)&lt;/span&gt; is distributed normally, with a mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{i,t}\)&lt;/span&gt; and standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\text{condition}}\)&lt;/span&gt;. Note that a different standard deviation is assumed for the incongruent versus congruent conditions (indicated by &lt;span class=&#34;math inline&#34;&gt;\(\text{condition}\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\mu_{i,t}\)&lt;/span&gt; term is determined as a linear combination of some &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights that we are estimating across conditions and timepoints. Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{congruent}_{1, i}}\)&lt;/span&gt; can be thought of as an “intercept”, which captures the average &lt;span class=&#34;math inline&#34;&gt;\(\text{RT}\)&lt;/span&gt; at timepoint 1 for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; (i.e. the first time they took the Stroop task). Then, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\Delta_{1, i}}\)&lt;/span&gt; is a “slope” (or different score) that is added to the intercept term when the condition identifier (&lt;span class=&#34;math inline&#34;&gt;\(I_\text{condition}\)&lt;/span&gt;) returns 1. Note that &lt;span class=&#34;math inline&#34;&gt;\(I_\text{condition}\)&lt;/span&gt; returns 1 only for the incongruent trials, and returns 0 otherwise. In this way, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\Delta_{1, i}}\)&lt;/span&gt; &lt;em&gt;is our estimate for the Stroop effect&lt;/em&gt; for subject &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; at the first timepoint. Importantly, the entire term representing the first timpeoint is then multiplied by &lt;span class=&#34;math inline&#34;&gt;\(I_\text{time}\)&lt;/span&gt;, which indicates whether the current trial is from the first (1) or second (0) timepoint. Then, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{congruent}_{2, i}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\Delta_{2, i}}\)&lt;/span&gt; are interpreted in the same way, except they represent corresponding estimates for the second timepoint.&lt;/p&gt;
&lt;p&gt;To estimate test-retest reliability, we then make the assumption that our individual-level estimates for &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\Delta_{1, i}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\Delta_{2, i}}\)&lt;/span&gt; (i.e. the Stroop effect estimates for each subject at timepoints 1 and 2) are drawn from a multivariate normal distribution, which allows us to estimate the Pearson’s correlation between Stroop effect timepoints across subjects &lt;strong&gt;in the same model that we use to estimate individual-level Stroop effects&lt;/strong&gt;. The multivariate normal distribution assumes that the individual-level effects are drawn from group-level Stroop effects for timepoint 1 (&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\beta_{\Delta_{1}}}\)&lt;/span&gt;) and timepoint 2 (&lt;span class=&#34;math inline&#34;&gt;\(\mu_{\beta_{\Delta_{2}}}\)&lt;/span&gt;), and with covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{S}\)&lt;/span&gt;. Covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{S}\)&lt;/span&gt; is itself constructed with the estimated SDs of each of the Stroop effects and correlation matrix &lt;span class=&#34;math inline&#34;&gt;\(\bf{R}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\bf{R}\)&lt;/span&gt; is where we get our test-retest reliability estimate.&lt;/p&gt;
&lt;p&gt;Then, we assume that the individual-level &lt;span class=&#34;math inline&#34;&gt;\(\beta_{\text{congruent}}\)&lt;/span&gt; weights for each timepoint are drawn from separate group-level normal distributions, with means indicated by &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\beta_{\text{congruent}_1}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{\beta_{\text{congruent}_2}}\)&lt;/span&gt; and SDs indicated by &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta_{\text{congruent}_1}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta_{\text{congruent}_2}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Lastly, we have the prior distributions on the group-level parameters. These priors in particular are not really very informative, given that the RTs are in seconds (see the descriptives above). The one prior worth mentioning is the the &lt;span class=&#34;math inline&#34;&gt;\(\text{LKJcorr}\)&lt;/span&gt; distribution, which specifies a prior on a correlation matrix. Since we specified 1 as the shape parameter and we are only estimating the correlation between two values, this particular prior is uniform from -1 to 1, which assumes that all possible correlations for test-retest reliability of the Stroop effect are equally likely. We could certainly do better, but this will be fine for a first pass at the data. For a more detailed description of this particular distribution and some informative visualizations, check out &lt;a href=&#34;https://www.psychstatistics.com/2014/12/27/d-lkj-priors/&#34;&gt;this great blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that we have walked through the model, here is the &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; code that we will use to fit it:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N;      // # of subjects
    int N_cond; // # of conditions
    int N_time; // # of timepoints
    int T_max;  // max # of trials across subjects
    real RT[N, N_cond, N_time, T_max]; // Reaction times for each subject, condition, timepoint, and trial
}
parameters {
  // Correlation matrix for Stroop effect test-retest reliability
  corr_matrix[N_time] R;
  
  // SDs for group-level parameters  
  vector&amp;lt;lower=0&amp;gt;[N_time] sigma_con;
  vector&amp;lt;lower=0&amp;gt;[N_time] sigma_delta;
  
  // SDs for normal model on RTs
  vector&amp;lt;lower=0&amp;gt;[N_cond] sigma_RT;  
  
  // Means for group-level parameters
  vector[N_time] mu_beta_con;  
  vector[N_time] mu_beta_delta;
  
  // Individual-level parameters
  vector[N_time] beta_con[N];
  vector[N_time] beta_delta[N];
}
transformed parameters {
  // Construct covariance matrix from SDs and correlation matrix
  cov_matrix[N_time] S; 
  S = quad_form_diag(R, sigma_delta);
}
model {
  // Priors on group-level SDs and correlation matrix
  R ~ lkj_corr(1); 
  sigma_delta ~ normal(0, 1);
  sigma_con   ~ normal(0, 1);
  sigma_RT    ~ normal(0, 1); 
  
  // Priors on group-level means
  mu_beta_con   ~ normal(0,1);
  mu_beta_delta ~ normal(0,1);
  
  // Priors on individual parameters
  beta_con[1] ~ normal(mu_beta_con[1], sigma_con[1]);
  beta_con[2] ~ normal(mu_beta_con[2], sigma_con[2]);
  beta_delta  ~ multi_normal(mu_beta_delta, S);
    
  // For each subject
  for (i in 1:N) {
    // Congruent at time 1
    RT[i,1,1,:] ~ normal(beta_con[i,1], sigma_RT[1]);
    // Incongruent at time 1
    RT[i,2,1,:] ~ normal(beta_con[i,1] + beta_delta[i,1], sigma_RT[2]);
    // Congruent at time 2
    RT[i,1,2,:] ~ normal(beta_con[i,2], sigma_RT[1]);
    // Incongruent at time 2
    RT[i,2,2,:] ~ normal(beta_con[i,2] + beta_delta[i,2], sigma_RT[2]);
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The comments describe much of what is going on in the code, and I wrote it to match equation 2 as best as possible. The only major change worth noting is that as opposed to using identifiers as in equation 2 (e.g., &lt;span class=&#34;math inline&#34;&gt;\(I_\text{condition}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(I_\text{time}\)&lt;/span&gt;), the Stan model has the different conditions and timepoints stored in different dimensions of a single array, which is more efficient. The underlying model is equivalent to equaiton 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fitting-and-refining-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fitting and Refining Our Model&lt;/h3&gt;
&lt;p&gt;Now, we can prepare the data and fit the Stan model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of subjects
n_subj &amp;lt;- length(unique(long_stroop$subj_num))
# Number of conditions 
n_cond &amp;lt;- 2
# Number of timepoints
n_time &amp;lt;- 2
# All subjects have 240 trials within each condition within timepoints
T_max &amp;lt;- 240

# Create RT data array for stan; dims = (subject, condition, time, trial)
RT &amp;lt;- array(NA, dim = c(n_subj, n_cond, n_time, T_max))
for (i in 1:n_subj) {
  # RTs for congruent condition at time 1
  RT[i, 1, 1,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==0 &amp;amp; time==1])
  # RTs for incongruent condition at time 1
  RT[i, 2, 1,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==2 &amp;amp; time==1])
  # RTs for congruent condition at time 2
  RT[i, 1, 2,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==0 &amp;amp; time==2])
  # RTs for incongruent condition at time 2
  RT[i, 2, 2,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==2 &amp;amp; time==2])
}

# Stan-ready data list
stan_dat &amp;lt;- list(N      = n_subj,
                 N_cond = n_cond,
                 N_time = n_time,
                 T_max  = T_max,
                 RT     = RT)

# Fit the hierarchical model
fit_m1 &amp;lt;- sampling(stroop_m1,
                   data    = stan_dat,
                   iter    = 2000, 
                   warmup  = 500,
                   chains  = 3,
                   cores   = 3, 
                   seed    = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 451 divergent transitions after warmup. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 1 chains where the estimated Bayesian Fraction of Missing Information was low. See
## http://mc-stan.org/misc/warnings.html#bfmi-low&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Examine the pairs() plot to diagnose sampling problems&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: The largest R-hat is NA, indicating chains have not mixed.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#r-hat&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Great! The model runs fine, although we get warning messages about divergent transitions. This happens in Stan because the sampler used (i.e. the No-U-Turn Hamiltonian Monte Carlo sampler) returns &lt;em&gt;divergences&lt;/em&gt; when the sampler explores difficult parts of the parameter space. Such a large number of divergences is problematic, suggesting that we may not have explored important parts of the parameter space. This often happens when the means and SDs of hierarchical parameters are correlated in strange ways, which we can confirm by checking the bivariate distributions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Pairs plot of the group-level congruent cond. means and SDs
pairs(fit_m1, pars = c(&amp;quot;mu_beta_con&amp;quot;, &amp;quot;sigma_con&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-12-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The red dots above indicate the divergent transitions. It is clear that the divergences are occuring most frequently in parts of the parameter space that are funnel-shaped. Now known as &lt;a href=&#34;https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html&#34;&gt;Neal’s funnel&lt;/a&gt;, we can take care of this problem by re-parameterizing the model such that the hierarchical means and SDs are less dependent on each other (i.e. a non-centered parameterization). Additionally, we will re-parameterize the correlation matrix using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Cholesky_decomposition&#34;&gt;Cholesky decomposition&lt;/a&gt;, which will also help with estimation a bit. I will not go into more detail about why Cholesky decomposition is useful, and instead refer interested readers to the wiki page referred to above.&lt;/p&gt;
&lt;p&gt;The updated Stan code below uses a &lt;a href=&#34;https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html&#34;&gt;non-centered parameterization&lt;/a&gt; to estimate all hierarchical parameters, which is mathematically identical to the model above but allows for better parameter estimation due to changes in the shape of the parameter space that Stan’s NUTS-HMC sampler needs to explore.&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data {
    int N;      // # of subjects
    int N_cond; // # of conditions
    int N_time; // # of timepoints
    int T_max;  // max # of trials across subjects
    real RT[N, N_cond, N_time, T_max]; // Reaction times for each subject, condition, timepoint, and trial
}
parameters {
  // Group-level correlation matrix (cholesky factor for faster computation)
  cholesky_factor_corr[2] R_cholesky; 
  
  // Group-level parameter SDs
  vector&amp;lt;lower=0&amp;gt;[2] sigma_con;
  vector&amp;lt;lower=0&amp;gt;[2] sigma_delta; 
  
  // Group-level SDs for normal model
  vector&amp;lt;lower=0&amp;gt;[2] sigma_RT;
  
  // Group-level parameter means
  vector[2] mu_beta_con;        
  vector[2] mu_beta_delta;      
  
  // Individual-level parameters (before being transformed)
    matrix[N,2] beta_con_pr;  
    matrix[2,N] beta_delta_pr; // order flipped here for operation below
}
transformed parameters {
  // Individual-level parameter off-sets (for non-centered parameterization)
  matrix[2,N] beta_delta_tilde;
  
  // Individual-level parameters 
  matrix[N,2] beta_con;
  matrix[N,2] beta_delta;
  
  // Construct inidividual offsets (for non-centered parameterization)
  beta_delta_tilde = diag_pre_multiply(sigma_delta, R_cholesky) * beta_delta_pr; 
  
  // Compute individual-level parameters from non-centered parameterization
  for (i in 1:N) {
    // Congruent at time 1
    beta_con[i,1] = mu_beta_con[1] + sigma_con[1] * beta_con_pr[i,1];
    // Congruent at time 2
    beta_con[i,2] = mu_beta_con[2] + sigma_con[2] * beta_con_pr[i,2];
    // Stroop effect at time 1
    beta_delta[i,1] = mu_beta_delta[1] + beta_delta_tilde[1, i];
    // Stroop effect at time 2
    beta_delta[i,2] = mu_beta_delta[2] + beta_delta_tilde[2, i];
  }
}
model {
  // Prior on cholesky factor of correlation matrix
  R_cholesky ~ lkj_corr_cholesky(1); 
  
  // Priors on group-level SDs
  sigma_delta ~ cauchy(0, 1);
  sigma_con   ~ cauchy(0, 1);
  sigma_RT    ~ cauchy(0, 1); 
  
  // Priors on individual-level parameters
  to_vector(beta_delta_pr) ~ normal(0, 1); 
  to_vector(beta_con_pr)   ~ normal(0, 1);
    
  // For each subject
  for (i in 1:N) {
    // Congruent at time 1
    RT[i,1,1,:] ~ normal(beta_con[i,1], sigma_RT[1]);
    // Incongruent at time 1
    RT[i,2,1,:] ~ normal(beta_con[i,1] + beta_delta[i,1], sigma_RT[2]);
    // Congruent at time 2
    RT[i,1,2,:] ~ normal(beta_con[i,2], sigma_RT[1]);
    // Incongruent at time 2
    RT[i,2,2,:] ~ normal(beta_con[i,2] + beta_delta[i,2], sigma_RT[2]);
  }
}
generated quantities { 
  corr_matrix[2] R;
    // Reconstruct correlation matrix from cholesky factor
  R = R_cholesky * R_cholesky&amp;#39;;
} &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, let’s fit the non-centered model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit the non-centered hierarchical model
fit_m2 &amp;lt;- sampling(stroop_m2, 
                   data    = stan_dat,
                   iter    = 2000, 
                   warmup  = 500,
                   chains  = 3,
                   cores   = 3, 
                   seed    = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And just like that, no divergent transitions! Also, we get the added bonus of slightly faster computation. We can again look at the pairs plot to see what happened to Neal’s funnel:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Pairs plot of the group-level congruent cond. means and SDs
pairs(fit_m2, pars = c(&amp;quot;mu_beta_con&amp;quot;, &amp;quot;sigma_con&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-15-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Above, you can see that the funnel has disappeared completely. Instead, we have well-behaved, more-or-less elliptical bivariate distributions (which MCMC samplers love).&lt;/p&gt;
&lt;p&gt;We also need to check convergence more generally, using both visual and quantitative diagnostics. First, we can graph the traceplots, which should look like “furry caterpillars”:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Check all group-level parameters (and test-retest correlation estimate)
traceplot(fit_m2, pars = c(&amp;quot;mu_beta_con&amp;quot;, &amp;quot;mu_beta_delta&amp;quot;, &amp;quot;sigma_con&amp;quot;, &amp;quot;sigma_delta&amp;quot;, &amp;quot;sigma_RT&amp;quot;, &amp;quot;R[1,2]&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-16-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;These look good. We would usually also check individual-level parameters, but for the sake of brevity we can look at some quantitative diagnostics. In particular, &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; (a.k.a. the Gelman-Rubin statistic) is a measure of within- relative to between-chain variance, which should be close to 1 for all parameters if chains mix well. We can easily plot a distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; for all parameters in the model as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# rstan&amp;#39;s default plotting method
stan_rhat(fit_m2, bins = 30)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 3 rows containing non-finite values (stat_bin).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-17-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, all the &lt;span class=&#34;math inline&#34;&gt;\(\hat{R}\)&lt;/span&gt; statistics are very close to 1. In combination with the traceplots above and lack of divergences, we can be pretty sure that the three chains we used have converged.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-inference-with-our-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Making Inference with Our Model&lt;/h3&gt;
&lt;p&gt;Now, the next step is to extract the parameter estimates and check the estimated correlation (i.e. test-retest estimate) across the individual-level Stroop effect parameters at each timepoint! First, we can plot the posterior distribution of the test-retest correlation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract parameters from model
pars &amp;lt;- extract(fit_m2)

# Plot density of test-retest correlation estimate
qplot(pars$R[,1,2], geom = &amp;quot;density&amp;quot;, fill = I(&amp;quot;#b5000c&amp;quot;)) +
  ggtitle(paste0(&amp;quot;Posterior Mode = &amp;quot;, round(estimate_mode(pars$R[,1,2]), 2))) +
  xlab(&amp;quot;Test-Retest Correlation&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-18-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The results are quite stunning—the mass of the posterior is up against 1, with a mode of &lt;span class=&#34;math inline&#34;&gt;\(0.96\)&lt;/span&gt;. Such a high test-retest reliability estimate is clearly at odds with the findings of Hedge et al., who reported an estimate of &lt;span class=&#34;math inline&#34;&gt;\(ICC(2,1) = .6\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;As discussed before, it is the pooling of information both across individual subjects and across timepoints within subjects that gives us these results. Now that our model is fit, we can readily visualize this pooling. The plot below shows the unpooled estimates from equation 1 (shown in our first figure at the start of the post), and the pooled estimates from our non-centered hierarchical model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extracting posterior modes of individual-level Stroop effect estimates
stroop_pooled &amp;lt;- apply(pars$beta_delta, c(2,3), mean) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  rename(Stroop_T1 = V1,
         Stroop_T2 = V2) %&amp;gt;%
  mutate(subj_num = row_number(),
         pooled = &amp;quot;Yes&amp;quot;)
# Pooled correlation
pooled_cor &amp;lt;- with(stroop_pooled, round(cor(Stroop_T1[pooled==&amp;quot;Yes&amp;quot;], Stroop_T2[pooled==&amp;quot;Yes&amp;quot;]), 2))

# My favorite visualization of all time
bind_rows(stroop_unpooled, stroop_pooled) %&amp;gt;%
  mutate(subj_num = as.factor(subj_num)) %&amp;gt;%
  ggplot(aes(x = Stroop_T1, y = Stroop_T2)) +
  ggtitle(paste0(&amp;quot;Posterior Means r = &amp;quot;, pooled_cor)) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = &amp;quot;black&amp;quot;, size = 1) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=1/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=2/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=3/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=4/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=5/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=6/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=7/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=8/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=9/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=.99, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  geom_line(aes(group = subj_num), size = 1/4) +
  geom_point(aes(group = subj_num, color = pooled)) +
  scale_color_manual(&amp;quot;Pooled?&amp;quot;,
                     values = c(&amp;quot;#990000&amp;quot;, &amp;quot;#fee8c8&amp;quot;)) +
  theme_minimal(base_size = 20) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-19-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here, we can see that the pooled estimates (individual-level posterior means pictured here) are actually highly consistent across timepoints within individuals. In fact, the correlation between the posterior means of the individual-level Stroop effects at time 1 versus time 2 is &lt;span class=&#34;math inline&#34;&gt;\(r = 0.98\)&lt;/span&gt;. We cannot really get much better than that! Of course, there is more uncertainty in the correlation matrix we estimated (pictured above), but this plot shows just how powerful the pooling from a well-constructed hierarchical model can be. Moreover, the hierarchical pooling removes the need to get rid of outliers at the individual-subject level—as you can see, data-points that fall far from the group average (which usually show large variance) shrink more strongly toward the group-level average.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;on-averaging-before-modeling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;On Averaging Before Modeling&lt;/h2&gt;
&lt;p&gt;If you are not yet convinced that the traditional practice of averaging across trials before modeling a correlation is sub-optimal, we can actually view such practices as a special case within the context of the generative model that we developed above (equation 2). Specifically, equation 1 assumes that individual-level Stroop effects can be estimated with no measurement error—but how much does this actually affect our inference? We will explore this question below before summarizing our findings and discussing implications for future research on behavioral tasks.&lt;/p&gt;
&lt;p&gt;Let’s begin with a thought experiment. If we were to encode the assumption of 0 measurement error into the generative model that we developed, how would we do so? At first, we may think to change the prior distributions, the likelihood of the model, or some other aspect of the model that can encode certainty/uncertainty. However, an idea that leads to better intuition of the implications is that we could:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Take our dataset containing all trials from all subjects in each condition/timepoint,&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Append our dataset to itself an infinite number of times&lt;/em&gt;&lt;/strong&gt; (yes, literally copy pasting your dataset repeatedly to create infinite data for each subject), and&lt;/li&gt;
&lt;li&gt;Fit the data using the generative model from equation 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If we follow these three steps, the test-retest correlation we estimate in equation 2 will converge to the correlation we get by using the traditional analysis described by equation 1 and surrounding text. This follows because as we continue to artificially replicate our dataset ad infinitum, the estimates for each individual-level estimate will approach the sample mean, and the uncertainty estimates will reduce to single points—this is identical to what we are doing in equation 1. Therefore, &lt;strong&gt;the traditional practice of averaging before modeling is as functionally problematic as artificially replicating our datasets an infinite number of times before fitting our model&lt;/strong&gt;. In fact, we can demonstrate this by observing what happens as we artificially replicate our dataset an increasing number of times. Of course, we cannot do this anywhere near an infinite number of times, but we can at least get a sense of the problem using this method. The R code below does exactly this, varying the number of artificially replicated datasets &lt;span class=&#34;math inline&#34;&gt;\(\in \{1, 2, 4, 8, 16, 32\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of artificially replicated datasets
reps &amp;lt;- c(1, 2, 4, 8, 16, 32)

# Looping through each replication and saving model fits
results &amp;lt;- foreach(r=reps) %do% {
  # Same as above
  n_subj &amp;lt;- length(unique(long_stroop$subj_num))
  n_cond &amp;lt;- 2
  n_time &amp;lt;- 2
  # If we multiply T_max by the reps variable, the number of trials alloted in 
  # the `RT` array for each condition/timepoint will increase accordingly
  T_max &amp;lt;- 240 * r
  
  # Create RT data array for stan; dims = (subject, condition, time, trial)
  RT &amp;lt;- array(NA, dim = c(n_subj, n_cond, n_time, T_max))
  for (i in 1:n_subj) {
    # Because we created an array that is larger than the vector of data that
    # we assigned to it, R will (by default) replicate the observations we are
    # assigning to each condition/timepoint to fill the corresponding RT array
    RT[i, 1, 1,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==0 &amp;amp; time==1])
    RT[i, 2, 1,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==2 &amp;amp; time==1])
    RT[i, 1, 2,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==0 &amp;amp; time==2])
    RT[i, 2, 2,] = with(long_stroop, RT[subj_num==i &amp;amp; Condition==2 &amp;amp; time==2])
  }
  
  # Stan-ready data list
  rep_dat &amp;lt;- list(N      = n_subj,
                  N_cond = n_cond,
                  N_time = n_time,
                  T_max  = T_max,
                  RT     = RT)

  # Fit the model with artificially replicated data
  tmp_fit &amp;lt;- sampling(stroop_m2,
                      data    = rep_dat,
                      iter    = 2000, 
                      warmup  = 500,
                      chains  = 3,
                      cores   = 3, 
                      seed    = 2)
  
  # Save model fit in list (`results`) 
  tmp_fit
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess

## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess

## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 4 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
## http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Examine the pairs() plot to diagnose sampling problems&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After fitting everything, we can visualize how the estimated test-retest correlation changes with respect the the number of times we artificially replicated our data (remember that the correlation should converge to the unpooled sample correlation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot posterior distribution of test-retest correlation across replications
foreach(i=seq_along(results), .combine = &amp;quot;rbind&amp;quot;) %do% {
    data.frame(R = rstan::extract(results[[i]])$R[,1,2]) %&amp;gt;%
        mutate(Replication = as.factor(reps[i]))
} %&amp;gt;%
    ggplot(aes(x = Replication, y = R)) +
    geom_violin(fill = I(&amp;quot;#b5000c&amp;quot;)) +
    stat_summary(aes(x = Replication, y = R), fun.y = mean, geom = &amp;quot;point&amp;quot;, size = 2) +
    geom_hline(yintercept = .5, linetype = 2, color = I(&amp;quot;black&amp;quot;)) +
    theme_minimal(base_size = 20) +
    theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `fun.y` is deprecated. Use `fun` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-21-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As expected, as we artificially replicated our dataset more and more, the model-estimated test-retest correlation (posterior means indicated by black points) converges toward the unpooled sample Pearson’s correlation (indicated by the dashed black line). Next, let’s take a look at this effect at the individual-level by plotting out the changes in posterior means with respect to the number of artificially replicated datasets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plot convergence of estimated individual-level posterior means with sample means (Sample Means = equation 1/unpooled estimates)
foreach(i=seq_along(results), .combine = &amp;quot;rbind&amp;quot;) %do% {
  apply(rstan::extract(results[[i]])$beta_delta, c(2,3), mean) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  rename(Stroop_T1 = V1,
         Stroop_T2 = V2) %&amp;gt;%
  mutate(subj_num = row_number(),
         Replication = as.character(reps[i]))
} %&amp;gt;%
  bind_rows(stroop_unpooled) %&amp;gt;%
  mutate(subj_num = as.factor(subj_num),
         Replication = factor(Replication, 
                              levels = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;16&amp;quot;, &amp;quot;32&amp;quot;, &amp;quot;Sample Mean&amp;quot;),
                              labels = c(&amp;quot;1&amp;quot;, &amp;quot;2&amp;quot;, &amp;quot;4&amp;quot;, &amp;quot;8&amp;quot;, &amp;quot;16&amp;quot;, &amp;quot;32&amp;quot;, &amp;quot;Sample Mean&amp;quot;))) %&amp;gt;%
  ggplot(aes(x = Stroop_T1, y = Stroop_T2)) +
  geom_abline(intercept = 0, slope = 1, linetype = 2, color = &amp;quot;black&amp;quot;, size = 1) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=1/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=2/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=3/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=4/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=5/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=6/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=7/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=8/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=9/10, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  stat_ellipse(geom=&amp;quot;polygon&amp;quot;, type=&amp;quot;norm&amp;quot;, level=.99, size=0, alpha=1/10, fill=&amp;quot;gray&amp;quot;) +
  geom_line(aes(group = subj_num), size = 1/4) +
  geom_point(aes(group = subj_num, color = Replication)) +
  scale_color_brewer(&amp;quot;Number of\nReplications&amp;quot;,
                     type = &amp;quot;seq&amp;quot;,
                     palette = &amp;quot;Reds&amp;quot;) +
  theme_minimal(base_size = 20) +
  theme(panel.grid = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories_files/figure-html/unnamed-chunk-22-1.svg&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to the group-level test-retest correlation estimate, we see that the individual posterior means converge to the unpooled sample means as we increase the number of times we artificially replicate our data before fitting the model.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;In this post, we showed that hierarchical models can: (1) pool information across individual subjects to more precisely estimate individual-level behavioral effects, and subsequently (2) increase test-retest reliability esimtates to the extent that we can infer strong, reliable individual differences between subjects in behavioral tasks. Therefore, our findings show that despite the conclusions drawn by Hedge et al., robust effects can in fact be reliable, with the caveat that we need to use more computationally expensive models that are more difficult to work with. Our findings are consistent with &lt;a href=&#34;https://link.springer.com/article/10.3758/s13423-018-1558-y&#34;&gt;Rouder &amp;amp; Haaf (2019)&lt;/a&gt;, who similarly showed that hierarchical models (even using a different functional form) are necessary to properly account for the high measurement error of behavioral effects when the goal is to make inference on individual-differences. I would highly recommend taking a look at Rouder &amp;amp; Haaf’s work if you are interested in reading more about the benefits of hierarchical modeling in the context of behavioral tasks.&lt;/p&gt;
&lt;p&gt;More broadly, our results show the benefits of thinking about the data-generating process when analyzing behavioral data. For example, traditional analyses (e.g., equation 1) fail to consider variability within subjects, which is detrimental when we aim to make inference at the individual-subject level. Specifically, by failing to consider within-subject variability, we implicitly assume that behavioral summary statistics are infinitely precise—when made explicit, it is apparent just how inconsistent such assumptions are with our existing knowledge regarding behavioral data.&lt;/p&gt;
&lt;p&gt;Equally important to the statistical issues that our results reveal are the ethical concerns brought forth by our last analysis. Specifically, we showed that averaging across trials before analyzing behavioral summary statistics is equivalent to artificially replicating each subject’s data an infinite number of times before fitting an inferential statistical model. Put in these terms, we can see just how problematic measurement error is when not properly accounted for. For example, if a researcher were caught artificially replicating their data before analyzing it, they would immediately be ridiculed—and likely formally investigated—for committing research misconduct and/or fraud. However, as it currently stands, researchers regularly commit to the practice of averaging before modeling, and we would never think to suggest that such behavior is outright fraud. This raises the question—should we?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Thanks for reading! I learned quite a lot writing this post, and I hope you gained some insight reading it&lt;/em&gt; :D&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression</title>
      <link>http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/plotly-binding/plotly.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/typedarray/typedarray.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://haines-lab.com/rmarkdown-libs/crosstalk/css/crosstalk.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/crosstalk/js/crosstalk.min.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://haines-lab.com/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/plotly-main/plotly-latest.min.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In this post, we will explore frequentist and Bayesian analogues of &lt;em&gt;regularized/penalized linear regression&lt;/em&gt; models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y = \beta_{0}+X\beta + \epsilon\tag{1}\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is the error, which is normally distributed as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\epsilon \sim  \mathcal{N}(0, \sigma)\tag{2}\]&lt;/span&gt;
Unlike these traditional linear regression models, regularized linear regression models produce &lt;strong&gt;biased estimates&lt;/strong&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights. Specifically, both frequentist and Bayesian regularized linear regression models pool information across &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights, resulting in regression toward a common mean. When the common mean is centered at 0, this pooling of information produces more conservative estimates for each &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight (they are biased toward 0). In contrast, traditional linear regression models assume that &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights share no group-level information (i.e. they are independent), which leads to so-called &lt;strong&gt;unbiased estimates&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So then, why are these models—which produce biased estimates—becoming increasingly popular throughout the social and behavioral sciences?&lt;/strong&gt;&lt;/p&gt;
&lt;div id=&#34;learning-objectives&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Learning Objectives&lt;/h2&gt;
&lt;p&gt;The current post seeks to show that we actually want biased estimates in many contexts. In doing so, we will also explore associations between frequentist and Bayesian regularization. Therefore, the &lt;strong&gt;learning objectives&lt;/strong&gt; of the current post are to develop an understanding of:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;why so-called &lt;strong&gt;biased&lt;/strong&gt; estimates are actually good for science,&lt;/li&gt;
&lt;li&gt;the difference between traditional and regularized linear regression models, and&lt;/li&gt;
&lt;li&gt;the correspondence between frequentist regularization and hierarchical Bayesian regression&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;p&gt;For this post, we will use a college-admissions dataset that is freely available from &lt;a href=&#34;https://www.kaggle.com/mohansacharya/graduate-admissions&#34;&gt;Kaggle&lt;/a&gt;. Make sure to download the .csv file named &lt;code&gt;Admission_Predict_Ver1.1.csv&lt;/code&gt;. This dataset contains 500 observations (i.e. rows) and 9 total variables (i.e. columns). 1 of these columns is a subject ID, which we will note use for modeling. Taken directly from the link:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;“The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : 1. GRE Scores ( out of 340 ) 2. TOEFL Scores ( out of 120 ) 3. University Rating ( out of 5 ) 4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) 5. Undergraduate GPA ( out of 10 ) 6. Research Experience ( either 0 or 1 ) 7. Chance of Admit ( ranging from 0 to 1 )”&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to predict the likelihood of being admitted to graduate school (&lt;em&gt;Chance of Admit&lt;/em&gt;), given the other variables, which we will now refer to as &lt;strong&gt;predictors&lt;/strong&gt;.&lt;/p&gt;
&lt;div id=&#34;getting-started&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting Started&lt;/h2&gt;
&lt;p&gt;First off, let’s load the libraries that we will use:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For traditional LASSO/Ridge regression
library(glmnet)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;glmnet&amp;#39; was built under R version 4.1.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For Bayesian modeling
library(rstan)
# For data wrangling/plotting
library(dplyr)
library(tidyr)
library(foreach)
library(ggplot2)
library(bayesplot) # Visualizing posteriors 
library(akima)     # for 3D plotting&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;akima&amp;#39; was built under R version 4.1.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plotly)    # for 3D plotting&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;plotly&amp;#39; was built under R version 4.1.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we need to read in the data. Assuming that you have already downloaded the data from the link above, we can read it into R as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# I was lazy and just left this in the downloads folder...
grad_dat &amp;lt;- read.csv(&amp;quot;~/Downloads/Admission_Predict_Ver1.1.csv&amp;quot;)

# View first few observations 
tbl_df(grad_dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## Please use `tibble::as_tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 500 × 9
##    Serial.No. GRE.Score TOEFL.Score University.Rating   SOP   LOR  CGPA Research
##         &amp;lt;int&amp;gt;     &amp;lt;int&amp;gt;       &amp;lt;int&amp;gt;             &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;int&amp;gt;
##  1          1       337         118                 4   4.5   4.5  9.65        1
##  2          2       324         107                 4   4     4.5  8.87        1
##  3          3       316         104                 3   3     3.5  8           1
##  4          4       322         110                 3   3.5   2.5  8.67        1
##  5          5       314         103                 2   2     3    8.21        0
##  6          6       330         115                 5   4.5   3    9.34        1
##  7          7       321         109                 3   3     4    8.2         1
##  8          8       308         101                 2   3     4    7.9         0
##  9          9       302         102                 1   2     1.5  8           0
## 10         10       323         108                 3   3.5   3    8.6         0
## # … with 490 more rows, and 1 more variable: Chance.of.Admit &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-training-and-test-sets&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating Training and Test Sets&lt;/h2&gt;
&lt;p&gt;Before really looking at the data, we need to separate out the training and testing portions. The original competition version of data uploaded to Kaggle only included the first 400 observations, and competitors had to make predictions on the remaining 100 observations before the actual outcomes (i.e. likelihood of getting into graduate school) were released.&lt;/p&gt;
&lt;p&gt;To show off the benefits of regularized over traditional methods, we will only train our models on &lt;strong&gt;the first 20 observations, and make predictions on the remaining 480&lt;/strong&gt;. In these low data settings—which are common to many areas of social and behavioral science (e.g., psychology, neuroscience, human ecology, etc.)—regularized regression models show clear advantages over traditional regression models. In fact, I will go as far as claiming that &lt;strong&gt;regularized models should be the default choice in most areas of science&lt;/strong&gt;, and the following examples should explain why.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Training data (used to fit model)
train_dat &amp;lt;- grad_dat[1:20,] # Only first 20 for training

# Testing data (used to test model generalizability)
test_dat &amp;lt;- grad_dat[21:500,] # Testing on the rest&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the data read in and separated, it would be useful to visualize our training data to get a sense of what we are working with. A correlation matrix should do a good job here:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Plotting correlation matrix
cor(train_dat) %&amp;gt;%
  as.data.frame() %&amp;gt;%
  mutate(Var1 = factor(row.names(.), levels=row.names(.))) %&amp;gt;% # For nice order
  gather(Var2, Correlation, 1:9) %&amp;gt;%
  ggplot(aes(reorder(Var2, Correlation), # Reorder to visualize
             reorder(Var1, -Correlation), fill = Correlation)) +
  geom_tile() +
  scale_fill_continuous(type = &amp;quot;viridis&amp;quot;) +
  xlab(&amp;quot;Variable&amp;quot;) +
  ylab(&amp;quot;Variable&amp;quot;) +
  theme_minimal(base_size = 15) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig0-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;This correlation matrix shows us that &lt;code&gt;Serial.No&lt;/code&gt; is not really correlated with any of the other 8 variables. This makes sense, given that &lt;code&gt;Serial.No&lt;/code&gt; is the subject ID (we will not use that in our models). Otherwise, it appears that there is a moderate amount of collinearity amoung the predictor and outcome variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;traditional-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Traditional Regression&lt;/h2&gt;
&lt;p&gt;Traditional linear regression models seek to mimimize the squared error between predicted and actual observations. Formally, we can represent this with a &lt;em&gt;loss function&lt;/em&gt; of the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\underset{\boldsymbol{\beta}}{argmin}\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2\tag{3}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(y_{i}\)&lt;/span&gt; is the outcome variable (probability of acceptance in our example) for observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is the total number of observations (20 students in our training data example), &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is the number of predictors (7 in our example), &lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt; is the intercept of the model, and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1,2,...,j}\)&lt;/span&gt; are the weights (i.e. slopes) for each of the &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; predictor variables. Under the assumption of normality, both ordinary least squares and maximum likelihood estimation of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights in equation 3 will offer the same results—from here on, we will refer to these methods of minimizing equation 3 as &lt;em&gt;traditional linear regression&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Below, we will fit a traditional linear regression model on the training set (which contains 20 observations total), and we will see how well it predicts the graduate school acceptance probability (&lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(Acceptance)\)&lt;/span&gt;) for each of the remaining 480 observations. We begin by scaling all variables—that is, we mean-center and divide each column by its own standard deviation (SD). Then, we apply the same standardization to the test data, followed by fitting the traditional model and making test-set predictions.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scale training data (and get rid of ID)
train_scale &amp;lt;- scale(train_dat[,2:9])

# Find means and SDs of training data variables
means &amp;lt;- attributes(train_scale)$`scaled:center`
SDs &amp;lt;- attributes(train_scale)$`scaled:scale`

# Scale test data using training data summary stats (no cheating!)
test_scale &amp;lt;- scale(test_dat[,-1], center = means, scale = SDs)

# Fit linear regression
fit_lr &amp;lt;- lm(Chance.of.Admit ~ ., data = as.data.frame(train_scale))

# Generate test-set predictions with linear regression
y_pred_lr &amp;lt;- predict(fit_lr, newdata = as.data.frame(test_scale[,-8]))

# Plot cor(predicted, actual)
qplot(x = y_pred_lr, y = test_scale[,8],
      main = paste0(&amp;quot;Traditional Linear Regression\n&amp;quot;, 
                    &amp;quot;r = &amp;quot;, round(cor(test_scale[,8], y_pred_lr), 2))) +
  xlab(&amp;quot;Model Predicted Pr(Acceptance)&amp;quot;) +
  ylab(&amp;quot;Actual Pr(Acceptance)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Wow, not bad at all! Even with only 20 observations, the correlation between predicted and actual probability of acceptance for the remaining 480 observations is $r = $ &lt;span class=&#34;math inline&#34;&gt;\(0.76\)&lt;/span&gt;. This suggests that probability of acceptance is rather well described as a simple linear combination of the predictors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;regularized-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regularized Regression&lt;/h2&gt;
&lt;p&gt;As described above, regularized linear regression models aim to estimate more conservative values for the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights in a model, and this is true for both frequentist and Bayesian versions of regularization. While there are many methods that can be used to regularize your estimation procedure, we will focus specifically on two popular forms—namely, ridge and LASSO regression. We start below by describing each regression generally, and then proceed to implement both the frequentist and Bayesian versions.&lt;/p&gt;
&lt;div id=&#34;ridge-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Ridge Regression&lt;/h3&gt;
&lt;p&gt;The extention from traditional to ridge regression is actually very straightforward! Specifically, we modify the loss function (equation 3) to include a &lt;strong&gt;penalty term&lt;/strong&gt; for model complexity, where model complexity is operationalized as the sum of squared &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\underbrace{\underset{\boldsymbol{\beta}}{argmin}\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2}_{\text{Traditional Loss Function}} + \underbrace{\lambda\sum_{j=1}^{p}\beta_{j}^2}_{\text{Ridge Penalty}}\tag{4}\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\lambda~(0&amp;lt;\lambda&amp;lt;\infty)\)&lt;/span&gt; is a penalty parameter, which controls how much regularization we would like in the model. To gain an intuition for the behavior of this model, think of what happens at the very extremes of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;. For example, when &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt;, what happens to equation 4? Well, the model simply reduces to equation 3, and we are back to traditional regression! What about as &lt;span class=&#34;math inline&#34;&gt;\(\lambda \rightarrow \infty\)&lt;/span&gt;? In that case, any non-zero values for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; will lead to an infinitely large penalty—then, the only solution to equation 4 is for all &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights to be equal to 0, indicating no learning from the data at all.&lt;/p&gt;
&lt;p&gt;Therefore, we can think of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; as a parameter that &lt;strong&gt;controls how much we learn from the data&lt;/strong&gt;, with smaller and larger values leading to more and less learning, respectively. Note that parameters that control how much we learn from data are typically called &lt;strong&gt;hyper-parameters&lt;/strong&gt;. Framed in these terms, we can view traditional regression methods as those that maximally learn from the data, and regularized regression models as those that restrict learning from the data. It is in this way that traditional regression produces &lt;em&gt;unbiased&lt;/em&gt; estimate of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights. That is, &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights are unbiased with respect to the information available in the training data. However, when data are noisy (i.e. contain large amounts of variability), such &lt;em&gt;unbiased&lt;/em&gt; estimates will be unduly influenced by noise. Thus, although traditional regression offers &lt;em&gt;unbiased&lt;/em&gt; estimates, it is also succeptable to estimating large magnitude &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights based purely on noise within the data. Ridge regression minimizes the potential learning from noise by penalizing the model for the squared sum of all the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights. The squaring of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights encodes our knowledge that large-magnitude &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights are much less likely than small-magnitude &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights (given that all our variables are on the same scale after standardization). Practically, this means that if a traditional regression would give us a very large magnitude &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight, when data are highly variable, ridge regression will &lt;em&gt;bias such large &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; estimates toward 0&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;We can actually visualize the effects of the ridge penalty on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights by plotting out the ridge penalty from equation 4. Specifically, the below plot shows the resulting penalty (where 0 is no penalty and increasingly negative numbers are stronger penalties) for the 2-dimensional case, where we only have 2 predictors. Importantly, the plot also includes the penalty functions for varying settings of &lt;span class=&#34;math inline&#34;&gt;\(\lambda \in \{0, .5, 1.5\}\)&lt;/span&gt;. Note that the flat surface is when &lt;span class=&#34;math inline&#34;&gt;\(\lambda = 0\)&lt;/span&gt;, which leads to no penalization.&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;15060540265fc&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;15060540265fc&#34;,&#34;attrs&#34;:{&#34;15060540265fc&#34;:{&#34;showscale&#34;:false,&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;z&#34;:{},&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;opacity&#34;:0.5,&#34;inherit&#34;:true},&#34;15060540265fc.1&#34;:{&#34;showscale&#34;:false,&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;z&#34;:{},&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;inherit&#34;:true},&#34;15060540265fc.2&#34;:{&#34;showscale&#34;:false,&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;z&#34;:{},&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;inherit&#34;:true}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:&#34;Beta_1&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Beta_2&#34;},&#34;zaxis&#34;:{&#34;title&#34;:&#34;Penalty&#34;}},&#34;title&#34;:&#34;Ridge Penalty Contour&#34;,&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:true},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;modeBarButtonsToAdd&#34;:[&#34;hoverclosest&#34;,&#34;hovercompare&#34;],&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;colorbar&#34;:{&#34;title&#34;:&#34;ridge_p1$z&lt;br /&gt;ridge_p2$z&lt;br /&gt;ridge_p3$z&#34;,&#34;ticklen&#34;:2},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:false,&#34;z&#34;:[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,null,null,null,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0,0,0,0]],&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;opacity&#34;:0.5,&#34;frame&#34;:null},{&#34;colorbar&#34;:{&#34;title&#34;:&#34;ridge_p1$z&lt;br /&gt;ridge_p2$z&lt;br /&gt;ridge_p3$z&#34;,&#34;ticklen&#34;:2},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:false,&#34;z&#34;:[[9,8.55038461538462,8.12442307692308,7.72211538461538,7.34346153846154,6.98846153846154,6.65711538461538,6.34942307692308,6.06538461538462,5.805,5.56826923076923,5.35519230769231,5.16576923076923,5,4.85807692307692,4.73980769230769,4.64519230769231,4.57423076923077,4.52692307692308,4.50326923076923,4.50326923076923,4.52692307692308,4.57423076923077,4.64519230769231,4.73980769230769,4.85807692307692,5,5.16576923076923,5.35519230769231,5.56826923076923,5.805,6.06538461538462,6.34942307692308,6.65711538461539,6.98846153846154,7.34346153846154,7.72211538461538,8.12442307692308,8.55038461538462,9],[8.55038461538462,8.10076923076923,7.67480769230769,7.2725,6.89384615384615,6.53884615384615,6.2075,5.89980769230769,5.61576923076923,5.35538461538462,5.11865384615385,4.90557692307692,4.71615384615385,4.55038461538462,4.40846153846154,4.29019230769231,4.19557692307692,4.12461538461539,4.07730769230769,4.05365384615385,4.05365384615385,4.07730769230769,4.12461538461539,4.19557692307692,4.29019230769231,4.40846153846154,4.55038461538462,4.71615384615385,4.90557692307692,5.11865384615385,5.35538461538462,5.61576923076923,5.89980769230769,6.2075,6.53884615384615,6.89384615384616,7.2725,7.67480769230769,8.10076923076923,8.55038461538462],[8.12442307692308,7.67480769230769,7.24884615384615,6.84653846153846,6.46788461538462,6.11288461538462,5.78153846153846,5.47384615384615,5.18980769230769,4.92942307692308,4.69269230769231,4.47961538461538,4.29019230769231,4.12442307692308,3.9825,3.86423076923077,3.76961538461539,3.69865384615385,3.65134615384615,3.62769230769231,3.62769230769231,3.65134615384615,3.69865384615385,3.76961538461539,3.86423076923077,3.9825,4.12442307692308,4.29019230769231,4.47961538461539,4.69269230769231,4.92942307692308,5.18980769230769,5.47384615384616,5.78153846153846,6.11288461538462,6.46788461538462,6.84653846153846,7.24884615384615,7.67480769230769,8.12442307692308],[7.72211538461538,7.2725,6.84653846153846,6.44423076923077,6.06557692307692,5.71057692307692,5.37923076923077,5.07153846153846,4.7875,4.52711538461539,4.29038461538462,4.07730769230769,3.88788461538461,3.72211538461538,3.58019230769231,3.46192307692308,3.36730769230769,3.29634615384615,3.24903846153846,3.22538461538462,3.22538461538462,3.24903846153846,3.29634615384615,3.36730769230769,3.46192307692308,3.58019230769231,3.72211538461538,3.88788461538462,4.07730769230769,4.29038461538462,4.52711538461539,4.7875,5.07153846153846,5.37923076923077,5.71057692307692,6.06557692307692,6.44423076923077,6.84653846153846,7.2725,7.72211538461538],[7.34346153846154,6.89384615384615,6.46788461538462,6.06557692307692,5.68692307692308,5.33192307692308,5.00057692307692,4.69288461538462,4.40884615384615,4.14846153846154,3.91173076923077,3.69865384615385,3.50923076923077,3.34346153846154,3.20153846153846,3.08326923076923,2.98865384615385,2.91769230769231,2.87038461538462,2.84673076923077,2.84673076923077,2.87038461538462,2.91769230769231,2.98865384615385,3.08326923076923,3.20153846153846,3.34346153846154,3.50923076923077,3.69865384615385,3.91173076923077,4.14846153846154,4.40884615384615,4.69288461538462,5.00057692307692,5.33192307692308,5.68692307692308,6.06557692307692,6.46788461538462,6.89384615384615,7.34346153846154],[6.98846153846154,6.53884615384615,6.11288461538462,5.71057692307692,5.33192307692308,4.97692307692308,4.64557692307692,4.33788461538462,4.05384615384615,3.79346153846154,3.55673076923077,3.34365384615385,3.15423076923077,2.98846153846154,2.84653846153846,2.72826923076923,2.63365384615385,2.56269230769231,2.51538461538462,2.49173076923077,2.49173076923077,2.51538461538462,2.56269230769231,2.63365384615385,2.72826923076923,2.84653846153846,2.98846153846154,3.15423076923077,3.34365384615385,3.55673076923077,3.79346153846154,4.05384615384615,4.33788461538462,4.64557692307693,4.97692307692308,5.33192307692308,5.71057692307692,6.11288461538462,6.53884615384615,6.98846153846154],[6.65711538461538,6.2075,5.78153846153846,5.37923076923077,5.00057692307692,4.64557692307692,4.31423076923077,4.00653846153846,3.7225,3.46211538461538,3.22538461538461,3.01230769230769,2.82288461538461,2.65711538461538,2.51519230769231,2.39692307692308,2.30230769230769,2.23134615384615,2.18403846153846,2.16038461538461,2.16038461538461,2.18403846153846,2.23134615384615,2.30230769230769,2.39692307692308,2.51519230769231,2.65711538461538,2.82288461538462,3.01230769230769,3.22538461538461,3.46211538461538,3.7225,4.00653846153846,4.31423076923077,4.64557692307692,5.00057692307692,5.37923076923077,5.78153846153846,6.2075,6.65711538461538],[6.34942307692308,5.89980769230769,5.47384615384615,5.07153846153846,4.69288461538462,4.33788461538462,4.00653846153846,3.69884615384615,3.41480769230769,3.15442307692308,2.91769230769231,2.70461538461538,2.51519230769231,2.34942307692308,2.2075,2.08923076923077,1.99461538461538,1.92365384615385,1.87634615384615,1.85269230769231,1.85269230769231,1.87634615384615,1.92365384615385,1.99461538461538,2.08923076923077,2.2075,2.34942307692308,2.51519230769231,2.70461538461539,2.91769230769231,3.15442307692308,3.41480769230769,3.69884615384615,4.00653846153846,4.33788461538462,4.69288461538462,5.07153846153846,5.47384615384615,5.89980769230769,6.34942307692308],[6.06538461538462,5.61576923076923,5.18980769230769,4.7875,4.40884615384615,4.05384615384615,3.7225,3.41480769230769,3.13076923076923,2.87038461538462,2.63365384615385,2.42057692307692,2.23115384615385,2.06538461538462,1.92346153846154,1.80519230769231,1.71057692307692,1.63961538461538,1.59230769230769,1.56865384615385,1.56865384615385,1.59230769230769,1.63961538461538,1.71057692307692,1.80519230769231,1.92346153846154,2.06538461538462,2.23115384615385,2.42057692307692,2.63365384615385,2.87038461538462,3.13076923076923,3.41480769230769,3.7225,4.05384615384615,4.40884615384615,4.7875,5.18980769230769,5.61576923076923,6.06538461538462],[5.805,5.35538461538462,4.92942307692308,4.52711538461539,4.14846153846154,3.79346153846154,3.46211538461538,3.15442307692308,2.87038461538462,2.61,2.37326923076923,2.16019230769231,1.97076923076923,1.805,1.66307692307692,1.54480769230769,1.45019230769231,1.37923076923077,1.33192307692308,1.30826923076923,1.30826923076923,1.33192307692308,1.37923076923077,1.45019230769231,1.54480769230769,1.66307692307692,1.805,1.97076923076923,2.16019230769231,2.37326923076923,2.61,2.87038461538462,3.15442307692308,3.46211538461539,3.79346153846154,4.14846153846154,4.52711538461538,4.92942307692308,5.35538461538462,5.805],[5.56826923076923,5.11865384615385,4.69269230769231,4.29038461538462,3.91173076923077,3.55673076923077,3.22538461538461,2.91769230769231,2.63365384615385,2.37326923076923,2.13653846153846,1.92346153846154,1.73403846153846,1.56826923076923,1.42634615384615,1.30807692307692,1.21346153846154,1.1425,1.09519230769231,1.07153846153846,1.07153846153846,1.09519230769231,1.1425,1.21346153846154,1.30807692307692,1.42634615384615,1.56826923076923,1.73403846153846,1.92346153846154,2.13653846153846,2.37326923076923,2.63365384615385,2.91769230769231,3.22538461538462,3.55673076923077,3.91173076923077,4.29038461538462,4.69269230769231,5.11865384615385,5.56826923076923],[5.35519230769231,4.90557692307692,4.47961538461539,4.07730769230769,3.69865384615385,3.34365384615385,3.01230769230769,2.70461538461538,2.42057692307692,2.16019230769231,1.92346153846154,1.71038461538461,1.52096153846154,1.35519230769231,1.21326923076923,1.095,1.00038461538461,0.929423076923077,0.882115384615384,0.858461538461538,0.858461538461538,0.882115384615384,0.929423076923077,1.00038461538462,1.095,1.21326923076923,1.35519230769231,1.52096153846154,1.71038461538462,1.92346153846154,2.16019230769231,2.42057692307692,2.70461538461538,3.01230769230769,3.34365384615385,3.69865384615385,4.07730769230769,4.47961538461538,4.90557692307692,5.35519230769231],[5.16576923076923,4.71615384615385,4.29019230769231,3.88788461538461,3.50923076923077,3.15423076923077,2.82288461538461,2.51519230769231,2.23115384615385,1.97076923076923,1.73403846153846,1.52096153846154,1.33153846153846,1.16576923076923,1.02384615384615,0.905576923076923,0.810961538461538,0.74,0.692692307692308,0.669038461538461,0.669038461538461,0.692692307692307,0.74,0.810961538461539,0.905576923076923,1.02384615384615,1.16576923076923,1.33153846153846,1.52096153846154,1.73403846153846,1.97076923076923,2.23115384615385,2.51519230769231,2.82288461538462,3.15423076923077,3.50923076923077,3.88788461538461,4.29019230769231,4.71615384615385,5.16576923076923],[5,4.55038461538462,4.12442307692308,3.72211538461538,3.34346153846154,2.98846153846154,2.65711538461538,2.34942307692308,2.06538461538462,1.805,1.56826923076923,1.35519230769231,1.16576923076923,1,0.858076923076923,0.739807692307692,0.645192307692308,0.574230769230769,0.526923076923077,0.503269230769231,0.503269230769231,0.526923076923077,0.574230769230769,0.645192307692308,0.739807692307692,0.858076923076923,1,1.16576923076923,1.35519230769231,1.56826923076923,1.805,2.06538461538462,2.34942307692308,2.65711538461539,2.98846153846154,3.34346153846154,3.72211538461538,4.12442307692308,4.55038461538462,5],[4.85807692307692,4.40846153846154,3.9825,3.58019230769231,3.20153846153846,2.84653846153846,2.51519230769231,2.2075,1.92346153846154,1.66307692307692,1.42634615384615,1.21326923076923,1.02384615384615,0.858076923076923,0.716153846153846,0.597884615384615,0.50326923076923,0.432307692307692,0.385,0.361346153846154,0.361346153846154,0.385,0.432307692307692,0.503269230769231,0.597884615384615,0.716153846153846,0.858076923076923,1.02384615384615,1.21326923076923,1.42634615384615,1.66307692307692,1.92346153846154,2.2075,2.51519230769231,2.84653846153846,3.20153846153846,3.58019230769231,3.9825,4.40846153846154,4.85807692307692],[4.73980769230769,4.29019230769231,3.86423076923077,3.46192307692308,3.08326923076923,2.72826923076923,2.39692307692308,2.08923076923077,1.80519230769231,1.54480769230769,1.30807692307692,1.095,0.905576923076923,0.739807692307692,0.597884615384615,0.479615384615384,0.385,0.314038461538461,0.266730769230769,0.243076923076923,0.243076923076923,0.266730769230769,0.314038461538461,0.385,null,0.597884615384615,0.739807692307692,0.905576923076923,1.095,1.30807692307692,1.54480769230769,1.80519230769231,2.08923076923077,2.39692307692308,2.72826923076923,3.08326923076923,3.46192307692308,null,4.29019230769231,4.73980769230769],[4.64519230769231,4.19557692307692,3.76961538461539,3.36730769230769,2.98865384615385,2.63365384615385,2.30230769230769,1.99461538461538,1.71057692307692,1.45019230769231,1.21346153846154,1.00038461538461,0.810961538461538,0.645192307692308,0.50326923076923,0.385,0.290384615384615,0.219423076923077,0.172115384615385,0.148461538461538,0.148461538461538,0.172115384615385,0.219423076923077,0.290384615384615,0.385,0.503269230769231,0.645192307692308,0.810961538461539,1.00038461538462,1.21346153846154,1.45019230769231,1.71057692307692,1.99461538461539,2.30230769230769,2.63365384615385,2.98865384615385,3.36730769230769,3.76961538461538,4.19557692307693,4.64519230769231],[4.57423076923077,4.12461538461539,3.69865384615385,3.29634615384615,2.91769230769231,2.56269230769231,2.23134615384615,1.92365384615385,1.63961538461538,1.37923076923077,1.1425,0.929423076923077,0.74,0.574230769230769,0.432307692307692,0.314038461538461,0.219423076923077,0.148461538461538,0.101153846153846,0.0775,0.0775,0.101153846153846,0.148461538461539,0.219423076923077,0.314038461538462,0.432307692307692,0.574230769230769,0.74,0.929423076923078,1.1425,1.37923076923077,1.63961538461538,1.92365384615385,2.23134615384616,2.56269230769231,2.91769230769231,3.29634615384615,3.69865384615385,4.12461538461539,4.57423076923077],[4.52692307692308,4.07730769230769,3.65134615384615,3.24903846153846,2.87038461538462,2.51538461538462,2.18403846153846,1.87634615384615,1.59230769230769,1.33192307692308,1.09519230769231,0.882115384615384,0.692692307692308,0.526923076923077,0.385,0.266730769230769,0.172115384615385,0.101153846153846,0.0538461538461539,0.0301923076923077,0.0301923076923077,0.0538461538461539,0.101153846153846,0.172115384615385,0.266730769230769,0.385,0.526923076923077,0.692692307692308,0.882115384615386,1.09519230769231,1.33192307692308,1.59230769230769,1.87634615384615,2.18403846153846,2.51538461538462,2.87038461538462,3.24903846153846,3.65134615384615,4.07730769230769,4.52692307692308],[4.50326923076923,4.05365384615385,3.62769230769231,3.22538461538462,2.84673076923077,2.49173076923077,2.16038461538461,1.85269230769231,1.56865384615385,1.30826923076923,1.07153846153846,0.858461538461538,0.669038461538461,0.503269230769231,0.361346153846154,0.243076923076923,0.148461538461538,0.0775,0.0301923076923077,0.00653846153846149,0.00653846153846153,0.0301923076923077,0.0775000000000001,0.148461538461539,0.243076923076923,0.361346153846154,0.503269230769231,0.669038461538462,0.858461538461539,1.07153846153846,1.30826923076923,1.56865384615385,1.85269230769231,2.16038461538462,2.49173076923077,2.84673076923077,3.22538461538461,3.62769230769231,4.05365384615385,4.50326923076923],[4.50326923076923,4.05365384615385,3.62769230769231,3.22538461538462,2.84673076923077,2.49173076923077,2.16038461538461,1.85269230769231,1.56865384615385,1.30826923076923,1.07153846153846,0.858461538461538,0.669038461538461,0.503269230769231,0.361346153846154,0.243076923076923,0.148461538461538,0.0775,0.0301923076923077,0.00653846153846153,0.00653846153846156,0.0301923076923077,0.0775000000000002,0.148461538461539,0.243076923076923,0.361346153846154,0.503269230769231,0.669038461538462,0.858461538461539,1.07153846153846,1.30826923076923,1.56865384615385,1.85269230769231,2.16038461538462,2.49173076923077,2.84673076923077,3.22538461538462,3.62769230769231,4.05365384615385,4.50326923076923],[4.52692307692308,4.07730769230769,3.65134615384615,3.24903846153846,2.87038461538462,2.51538461538462,2.18403846153846,1.87634615384615,1.59230769230769,1.33192307692308,1.09519230769231,0.882115384615384,0.692692307692308,0.526923076923077,0.385,0.266730769230769,0.172115384615385,0.101153846153846,0.0538461538461539,0.0301923076923077,0.0301923076923077,0.0538461538461539,0.101153846153846,0.172115384615385,0.266730769230769,0.385,0.526923076923077,0.692692307692308,0.882115384615386,1.09519230769231,1.33192307692308,1.59230769230769,1.87634615384615,2.18403846153846,2.51538461538462,2.87038461538462,3.24903846153846,3.65134615384615,4.07730769230769,4.52692307692308],[4.57423076923077,4.12461538461539,3.69865384615385,3.29634615384615,2.91769230769231,2.56269230769231,2.23134615384615,1.92365384615385,1.63961538461538,1.37923076923077,1.1425,0.929423076923077,0.74,0.574230769230769,0.432307692307692,0.314038461538462,0.219423076923077,0.148461538461539,0.101153846153846,0.0775000000000001,0.0775000000000002,0.101153846153846,0.148461538461539,0.219423076923077,0.314038461538462,0.432307692307693,0.574230769230769,0.740000000000001,0.929423076923078,1.1425,1.37923076923077,1.63961538461538,1.92365384615385,2.23134615384616,2.56269230769231,2.91769230769231,3.29634615384615,3.69865384615385,4.12461538461539,4.57423076923077],[4.64519230769231,4.19557692307692,3.76961538461539,3.36730769230769,2.98865384615385,2.63365384615385,2.30230769230769,1.99461538461538,1.71057692307692,1.45019230769231,1.21346153846154,1.00038461538462,0.810961538461539,0.645192307692308,0.503269230769231,0.385,0.290384615384615,0.219423076923077,0.172115384615385,0.148461538461539,0.148461538461539,0.172115384615385,0.219423076923077,0.290384615384616,0.385,0.503269230769231,0.645192307692308,0.810961538461539,1.00038461538462,1.21346153846154,1.45019230769231,1.71057692307692,1.99461538461539,2.30230769230769,2.63365384615385,2.98865384615385,3.36730769230769,3.76961538461539,4.19557692307693,4.64519230769231],[4.73980769230769,4.29019230769231,3.86423076923077,3.46192307692308,3.08326923076923,2.72826923076923,2.39692307692308,2.08923076923077,1.80519230769231,1.54480769230769,1.30807692307692,1.095,0.905576923076923,0.739807692307692,0.597884615384615,null,0.385,0.314038461538462,0.266730769230769,0.243076923076923,0.243076923076923,0.266730769230769,0.314038461538462,0.385,0.479615384615385,0.597884615384616,0.739807692307692,0.905576923076924,1.095,1.30807692307692,1.54480769230769,1.80519230769231,2.08923076923077,2.39692307692308,2.72826923076923,3.08326923076923,3.46192307692308,3.86423076923077,4.29019230769231,4.73980769230769],[4.85807692307692,4.40846153846154,3.9825,3.58019230769231,3.20153846153846,2.84653846153846,2.51519230769231,2.2075,1.92346153846154,1.66307692307692,1.42634615384615,1.21326923076923,1.02384615384615,0.858076923076923,0.716153846153846,0.597884615384615,0.503269230769231,0.432307692307692,0.385,0.361346153846154,0.361346153846154,0.385,0.432307692307693,0.503269230769231,0.597884615384616,0.716153846153846,0.858076923076923,null,1.21326923076923,1.42634615384615,1.66307692307692,1.92346153846154,2.2075,2.51519230769231,2.84653846153846,3.20153846153846,3.58019230769231,3.9825,4.40846153846154,4.85807692307692],[5,4.55038461538462,4.12442307692308,3.72211538461538,3.34346153846154,2.98846153846154,2.65711538461538,2.34942307692308,2.06538461538462,1.805,1.56826923076923,1.35519230769231,1.16576923076923,1,0.858076923076923,0.739807692307692,0.645192307692308,0.574230769230769,0.526923076923077,0.503269230769231,0.503269230769231,0.526923076923077,0.574230769230769,0.645192307692308,0.739807692307692,0.858076923076923,1,1.16576923076923,1.35519230769231,1.56826923076923,1.805,2.06538461538462,2.34942307692308,2.65711538461539,2.98846153846154,3.34346153846154,3.72211538461538,4.12442307692308,4.55038461538462,5],[5.16576923076923,4.71615384615385,4.29019230769231,3.88788461538462,3.50923076923077,3.15423076923077,2.82288461538461,2.51519230769231,2.23115384615385,1.97076923076923,1.73403846153846,1.52096153846154,1.33153846153846,1.16576923076923,1.02384615384615,0.905576923076923,0.810961538461539,0.74,0.692692307692308,0.669038461538462,0.669038461538462,0.692692307692308,0.740000000000001,0.810961538461539,0.905576923076924,null,1.16576923076923,1.33153846153846,1.52096153846154,1.73403846153846,1.97076923076923,2.23115384615385,2.51519230769231,2.82288461538462,3.15423076923077,3.50923076923077,3.88788461538462,4.29019230769231,null,5.16576923076923],[5.35519230769231,4.90557692307692,4.47961538461539,4.07730769230769,3.69865384615385,3.34365384615385,3.01230769230769,2.70461538461539,2.42057692307692,2.16019230769231,1.92346153846154,1.71038461538462,1.52096153846154,1.35519230769231,1.21326923076923,1.095,1.00038461538462,0.929423076923078,0.882115384615386,0.858461538461539,0.858461538461539,0.882115384615386,0.929423076923078,1.00038461538462,1.095,1.21326923076923,1.35519230769231,1.52096153846154,1.71038461538462,1.92346153846154,2.16019230769231,2.42057692307692,2.70461538461539,3.01230769230769,3.34365384615385,3.69865384615385,4.07730769230769,4.47961538461539,4.90557692307693,5.35519230769231],[5.56826923076923,5.11865384615385,4.69269230769231,4.29038461538462,3.91173076923077,3.55673076923077,3.22538461538461,2.91769230769231,2.63365384615385,2.37326923076923,2.13653846153846,1.92346153846154,1.73403846153846,1.56826923076923,1.42634615384615,1.30807692307692,1.21346153846154,1.1425,1.09519230769231,1.07153846153846,1.07153846153846,1.09519230769231,1.1425,1.21346153846154,1.30807692307692,1.42634615384615,1.56826923076923,1.73403846153846,1.92346153846154,2.13653846153846,2.37326923076923,2.63365384615385,2.91769230769231,3.22538461538462,3.55673076923077,3.91173076923077,4.29038461538461,4.69269230769231,5.11865384615385,5.56826923076923],[5.805,5.35538461538462,4.92942307692308,4.52711538461539,4.14846153846154,3.79346153846154,3.46211538461538,3.15442307692308,2.87038461538462,2.61,2.37326923076923,2.16019230769231,1.97076923076923,1.805,1.66307692307692,1.54480769230769,1.45019230769231,1.37923076923077,1.33192307692308,1.30826923076923,1.30826923076923,1.33192307692308,1.37923076923077,1.45019230769231,1.54480769230769,1.66307692307692,1.805,1.97076923076923,2.16019230769231,2.37326923076923,2.61,2.87038461538462,3.15442307692308,3.46211538461539,3.79346153846154,4.14846153846154,4.52711538461539,4.92942307692308,5.35538461538462,5.805],[6.06538461538462,5.61576923076923,5.18980769230769,4.7875,4.40884615384615,4.05384615384615,3.7225,3.41480769230769,3.13076923076923,2.87038461538462,2.63365384615385,2.42057692307692,2.23115384615385,2.06538461538462,1.92346153846154,1.80519230769231,1.71057692307692,1.63961538461538,1.59230769230769,1.56865384615385,1.56865384615385,1.59230769230769,1.63961538461538,1.71057692307692,1.80519230769231,1.92346153846154,2.06538461538462,2.23115384615385,2.42057692307692,2.63365384615385,2.87038461538462,3.13076923076923,3.41480769230769,3.7225,4.05384615384615,4.40884615384615,4.7875,5.18980769230769,5.61576923076923,6.06538461538462],[6.34942307692308,5.89980769230769,5.47384615384616,5.07153846153846,4.69288461538462,4.33788461538462,4.00653846153846,3.69884615384615,3.41480769230769,3.15442307692308,2.91769230769231,2.70461538461538,2.51519230769231,2.34942307692308,2.2075,2.08923076923077,1.99461538461539,1.92365384615385,1.87634615384615,1.85269230769231,1.85269230769231,1.87634615384615,1.92365384615385,1.99461538461539,2.08923076923077,2.2075,2.34942307692308,2.51519230769231,2.70461538461539,2.91769230769231,3.15442307692308,3.41480769230769,3.69884615384615,4.00653846153846,4.33788461538462,4.69288461538462,5.07153846153846,5.47384615384616,5.89980769230769,6.34942307692308],[6.65711538461539,6.2075,5.78153846153846,5.37923076923077,5.00057692307692,4.64557692307692,4.31423076923077,4.00653846153846,3.7225,3.46211538461539,3.22538461538462,3.01230769230769,2.82288461538462,2.65711538461539,2.51519230769231,2.39692307692308,2.30230769230769,2.23134615384616,2.18403846153846,2.16038461538462,2.16038461538462,2.18403846153846,2.23134615384616,2.30230769230769,2.39692307692308,2.51519230769231,2.65711538461539,2.82288461538462,3.01230769230769,3.22538461538462,3.46211538461539,3.7225,4.00653846153846,4.31423076923077,4.64557692307692,5.00057692307693,5.37923076923077,5.78153846153846,6.2075,null],[6.98846153846154,6.53884615384615,6.11288461538462,5.71057692307692,5.33192307692308,4.97692307692308,4.64557692307692,4.33788461538462,4.05384615384615,3.79346153846154,3.55673076923077,3.34365384615385,3.15423076923077,2.98846153846154,2.84653846153846,2.72826923076923,2.63365384615385,2.56269230769231,2.51538461538462,2.49173076923077,2.49173076923077,2.51538461538462,2.56269230769231,2.63365384615385,2.72826923076923,2.84653846153846,2.98846153846154,3.15423076923077,3.34365384615385,3.55673076923077,3.79346153846154,4.05384615384615,4.33788461538462,4.64557692307692,4.97692307692308,5.33192307692308,5.71057692307692,6.11288461538462,null,null],[7.34346153846154,6.89384615384616,6.46788461538462,6.06557692307692,5.68692307692308,5.33192307692308,5.00057692307692,4.69288461538462,4.40884615384615,4.14846153846154,3.91173076923077,3.69865384615385,3.50923076923077,3.34346153846154,3.20153846153846,3.08326923076923,2.98865384615385,2.91769230769231,2.87038461538462,2.84673076923077,2.84673076923077,2.87038461538462,2.91769230769231,2.98865384615385,3.08326923076923,3.20153846153846,3.34346153846154,3.50923076923077,3.69865384615385,3.91173076923077,4.14846153846154,4.40884615384615,4.69288461538462,5.00057692307693,5.33192307692308,5.68692307692308,6.06557692307692,null,null,7.34346153846154],[7.72211538461538,7.2725,6.84653846153846,6.44423076923077,6.06557692307692,5.71057692307692,5.37923076923077,5.07153846153846,4.7875,4.52711538461538,4.29038461538461,4.07730769230769,3.88788461538461,3.72211538461538,3.58019230769231,3.46192307692308,3.36730769230769,3.29634615384615,3.24903846153846,3.22538461538462,3.22538461538462,3.24903846153846,3.29634615384615,3.36730769230769,3.46192307692308,3.58019230769231,3.72211538461538,3.88788461538462,4.07730769230769,4.29038461538462,4.52711538461539,4.7875,5.07153846153846,5.37923076923077,5.71057692307692,6.06557692307692,null,null,null,7.72211538461538],[8.12442307692308,7.67480769230769,7.24884615384616,6.84653846153846,6.46788461538462,6.11288461538462,5.78153846153846,5.47384615384615,5.18980769230769,4.92942307692308,4.69269230769231,4.47961538461539,4.29019230769231,4.12442307692308,3.9825,null,3.76961538461539,3.69865384615385,3.65134615384615,3.62769230769231,3.62769230769231,3.65134615384615,3.69865384615385,3.76961538461539,3.86423076923077,3.9825,4.12442307692308,4.29019230769231,4.47961538461539,4.69269230769231,4.92942307692308,5.18980769230769,5.47384615384616,5.78153846153846,6.11288461538462,null,null,null,7.67480769230769,8.12442307692308],[8.55038461538462,8.10076923076923,7.67480769230769,7.2725,6.89384615384616,6.53884615384615,6.2075,5.89980769230769,5.61576923076923,5.35538461538462,5.11865384615385,4.90557692307692,4.71615384615385,4.55038461538462,4.40846153846154,4.29019230769231,4.19557692307692,4.12461538461539,4.07730769230769,4.05365384615385,4.05365384615385,4.07730769230769,4.12461538461539,4.19557692307692,4.29019230769231,4.40846153846154,4.55038461538462,null,4.90557692307693,5.11865384615385,5.35538461538462,5.61576923076923,5.89980769230769,null,null,null,7.2725,7.67480769230769,8.10076923076923,8.55038461538462],[9,8.55038461538462,8.12442307692308,7.72211538461538,7.34346153846154,6.98846153846154,6.65711538461538,6.34942307692308,6.06538461538462,5.805,5.56826923076923,5.35519230769231,5.16576923076923,5,4.85807692307692,4.73980769230769,4.64519230769231,4.57423076923077,4.52692307692308,4.50326923076923,4.50326923076923,4.52692307692308,4.57423076923077,4.64519230769231,4.73980769230769,4.85807692307692,5,5.16576923076923,5.35519230769231,5.56826923076923,5.805,6.06538461538462,6.34942307692308,6.65711538461539,null,null,7.72211538461538,8.12442307692308,8.55038461538462,9]],&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;frame&#34;:null},{&#34;colorbar&#34;:{&#34;title&#34;:&#34;ridge_p1$z&lt;br /&gt;ridge_p2$z&lt;br /&gt;ridge_p3$z&#34;,&#34;ticklen&#34;:2},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:false,&#34;z&#34;:[[27,25.6511538461538,24.3732692307692,23.1663461538462,22.0303846153846,20.9653846153846,19.9713461538462,19.0482692307692,18.1961538461538,17.415,16.7048076923077,16.0655769230769,15.4973076923077,15,14.5742307692308,14.2194230769231,13.9355769230769,13.7226923076923,13.5807692307692,13.5098076923077,13.5098076923077,13.5807692307692,13.7226923076923,13.9355769230769,14.2194230769231,14.5742307692308,15,15.4973076923077,16.0655769230769,16.7048076923077,17.415,18.1961538461538,19.0482692307692,19.9713461538462,20.9653846153846,22.0303846153846,23.1663461538462,24.3732692307692,25.6511538461538,27],[25.6511538461538,24.3023076923077,23.0244230769231,21.8175,20.6815384615385,19.6165384615385,18.6225,17.6994230769231,16.8473076923077,16.0661538461538,15.3559615384615,14.7167307692308,14.1484615384615,13.6511538461538,13.2253846153846,12.8705769230769,12.5867307692308,12.3738461538462,12.2319230769231,12.1609615384615,12.1609615384615,12.2319230769231,12.3738461538462,12.5867307692308,12.8705769230769,13.2253846153846,13.6511538461538,14.1484615384615,14.7167307692308,15.3559615384615,16.0661538461538,16.8473076923077,17.6994230769231,18.6225,19.6165384615385,20.6815384615385,21.8175,23.0244230769231,24.3023076923077,25.6511538461538],[24.3732692307692,23.0244230769231,21.7465384615385,20.5396153846154,19.4036538461538,18.3386538461538,17.3446153846154,16.4215384615385,15.5694230769231,14.7882692307692,14.0780769230769,13.4388461538462,12.8705769230769,12.3732692307692,11.9475,11.5926923076923,11.3088461538462,11.0959615384615,10.9540384615385,10.8830769230769,10.8830769230769,10.9540384615385,11.0959615384615,11.3088461538462,11.5926923076923,11.9475,12.3732692307692,12.8705769230769,13.4388461538462,14.0780769230769,14.7882692307692,15.5694230769231,16.4215384615385,17.3446153846154,18.3386538461538,19.4036538461539,20.5396153846154,21.7465384615385,23.0244230769231,24.3732692307692],[23.1663461538462,21.8175,20.5396153846154,19.3326923076923,18.1967307692308,17.1317307692308,16.1376923076923,15.2146153846154,14.3625,13.5813461538462,12.8711538461538,12.2319230769231,11.6636538461538,11.1663461538462,10.7405769230769,10.3857692307692,10.1019230769231,9.88903846153846,9.74711538461539,9.67615384615385,9.67615384615384,9.74711538461538,9.88903846153846,10.1019230769231,10.3857692307692,10.7405769230769,11.1663461538462,11.6636538461538,12.2319230769231,12.8711538461538,13.5813461538462,14.3625,15.2146153846154,16.1376923076923,17.1317307692308,18.1967307692308,19.3326923076923,20.5396153846154,21.8175,23.1663461538462],[22.0303846153846,20.6815384615385,19.4036538461539,18.1967307692308,17.0607692307692,15.9957692307692,15.0017307692308,14.0786538461538,13.2265384615385,12.4453846153846,11.7351923076923,11.0959615384615,10.5276923076923,10.0303846153846,9.60461538461538,9.24980769230769,8.96596153846154,8.75307692307692,8.61115384615385,8.54019230769231,8.54019230769231,8.61115384615385,8.75307692307692,8.96596153846154,9.24980769230769,9.60461538461538,10.0303846153846,10.5276923076923,11.0959615384615,11.7351923076923,12.4453846153846,13.2265384615385,14.0786538461538,15.0017307692308,15.9957692307692,17.0607692307692,18.1967307692308,19.4036538461539,20.6815384615385,22.0303846153846],[20.9653846153846,19.6165384615385,18.3386538461538,17.1317307692308,15.9957692307692,14.9307692307692,13.9367307692308,13.0136538461538,12.1615384615385,11.3803846153846,10.6701923076923,10.0309615384615,9.46269230769231,8.96538461538462,8.53961538461538,8.18480769230769,7.90096153846154,7.68807692307692,7.54615384615385,7.47519230769231,7.47519230769231,7.54615384615385,7.68807692307692,7.90096153846154,8.18480769230769,8.53961538461539,8.96538461538461,9.46269230769231,10.0309615384615,10.6701923076923,11.3803846153846,12.1615384615385,13.0136538461538,13.9367307692308,14.9307692307692,15.9957692307692,17.1317307692308,18.3386538461538,19.6165384615385,20.9653846153846],[19.9713461538462,18.6225,17.3446153846154,16.1376923076923,15.0017307692308,13.9367307692308,12.9426923076923,12.0196153846154,11.1675,10.3863461538462,9.67615384615384,9.03692307692308,8.46865384615385,7.97134615384615,7.54557692307692,7.19076923076923,6.90692307692308,6.69403846153846,6.55211538461538,6.48115384615384,6.48115384615384,6.55211538461538,6.69403846153846,6.90692307692308,7.19076923076923,7.54557692307692,7.97134615384615,8.46865384615385,9.03692307692308,9.67615384615384,10.3863461538462,11.1675,12.0196153846154,12.9426923076923,13.9367307692308,15.0017307692308,16.1376923076923,17.3446153846154,18.6225,19.9713461538462],[19.0482692307692,17.6994230769231,16.4215384615385,15.2146153846154,14.0786538461538,13.0136538461538,12.0196153846154,11.0965384615385,10.2444230769231,9.46326923076923,8.75307692307692,8.11384615384615,7.54557692307692,7.04826923076923,6.6225,6.26769230769231,5.98384615384615,5.77096153846154,5.62903846153846,5.55807692307692,5.55807692307692,5.62903846153846,5.77096153846154,5.98384615384615,6.26769230769231,6.6225,7.04826923076923,7.54557692307692,8.11384615384616,8.75307692307692,9.46326923076923,10.2444230769231,11.0965384615385,12.0196153846154,13.0136538461538,14.0786538461539,15.2146153846154,16.4215384615385,17.6994230769231,19.0482692307692],[18.1961538461538,16.8473076923077,15.5694230769231,14.3625,13.2265384615385,12.1615384615385,11.1675,10.2444230769231,9.39230769230769,8.61115384615385,7.90096153846154,7.26173076923077,6.69346153846154,6.19615384615385,5.77038461538461,5.41557692307692,5.13173076923077,4.91884615384615,4.77692307692308,4.70596153846154,4.70596153846154,4.77692307692308,4.91884615384615,5.13173076923077,5.41557692307692,5.77038461538462,6.19615384615385,6.69346153846154,7.26173076923077,7.90096153846154,8.61115384615385,9.39230769230769,10.2444230769231,11.1675,12.1615384615385,13.2265384615385,14.3625,15.5694230769231,16.8473076923077,18.1961538461538],[17.415,16.0661538461538,14.7882692307692,13.5813461538462,12.4453846153846,11.3803846153846,10.3863461538462,9.46326923076923,8.61115384615385,7.83,7.11980769230769,6.48057692307692,5.91230769230769,5.415,4.98923076923077,4.63442307692308,4.35057692307692,4.13769230769231,3.99576923076923,3.92480769230769,3.92480769230769,3.99576923076923,4.13769230769231,4.35057692307692,4.63442307692308,4.98923076923077,5.415,5.91230769230769,6.48057692307693,7.11980769230769,7.83,8.61115384615385,9.46326923076923,10.3863461538462,11.3803846153846,12.4453846153846,13.5813461538462,14.7882692307692,16.0661538461538,17.415],[16.7048076923077,15.3559615384615,14.0780769230769,12.8711538461538,11.7351923076923,10.6701923076923,9.67615384615384,8.75307692307692,7.90096153846154,7.11980769230769,6.40961538461538,5.77038461538461,5.20211538461538,4.70480769230769,4.27903846153846,3.92423076923077,3.64038461538462,3.4275,3.28557692307692,3.21461538461538,3.21461538461538,3.28557692307692,3.4275,3.64038461538462,3.92423076923077,4.27903846153846,4.70480769230769,5.20211538461539,5.77038461538462,6.40961538461538,7.11980769230769,7.90096153846154,8.75307692307692,9.67615384615385,10.6701923076923,11.7351923076923,12.8711538461538,14.0780769230769,15.3559615384615,16.7048076923077],[16.0655769230769,14.7167307692308,13.4388461538462,12.2319230769231,11.0959615384615,10.0309615384615,9.03692307692307,8.11384615384615,7.26173076923077,6.48057692307692,5.77038461538461,5.13115384615384,4.56288461538461,4.06557692307692,3.63980769230769,3.285,3.00115384615385,2.78826923076923,2.64634615384615,2.57538461538461,2.57538461538461,2.64634615384615,2.78826923076923,3.00115384615385,3.285,3.63980769230769,4.06557692307692,4.56288461538462,5.13115384615385,5.77038461538462,6.48057692307693,7.26173076923077,8.11384615384615,9.03692307692308,10.0309615384615,11.0959615384615,12.2319230769231,13.4388461538462,14.7167307692308,16.0655769230769],[15.4973076923077,14.1484615384615,12.8705769230769,11.6636538461538,10.5276923076923,9.46269230769231,8.46865384615384,7.54557692307692,6.69346153846154,5.91230769230769,5.20211538461538,4.56288461538461,3.99461538461538,3.49730769230769,3.07153846153846,2.71673076923077,2.43288461538461,2.22,2.07807692307692,2.00711538461538,2.00711538461538,2.07807692307692,2.22,2.43288461538462,2.71673076923077,3.07153846153846,3.49730769230769,3.99461538461539,4.56288461538462,5.20211538461538,5.91230769230769,6.69346153846154,7.54557692307692,8.46865384615385,9.46269230769231,10.5276923076923,11.6636538461538,12.8705769230769,14.1484615384615,15.4973076923077],[15,13.6511538461538,12.3732692307692,11.1663461538462,10.0303846153846,8.96538461538462,7.97134615384615,7.04826923076923,6.19615384615385,5.415,4.70480769230769,4.06557692307692,3.49730769230769,3,2.57423076923077,2.21942307692308,1.93557692307692,1.72269230769231,1.58076923076923,1.50980769230769,1.50980769230769,1.58076923076923,1.72269230769231,1.93557692307692,2.21942307692308,2.57423076923077,3,3.49730769230769,4.06557692307693,4.70480769230769,5.415,6.19615384615385,7.04826923076923,7.97134615384616,8.96538461538462,10.0303846153846,11.1663461538462,12.3732692307692,13.6511538461538,15],[14.5742307692308,13.2253846153846,11.9475,10.7405769230769,9.60461538461538,8.53961538461538,7.54557692307692,6.6225,5.77038461538461,4.98923076923077,4.27903846153846,3.63980769230769,3.07153846153846,2.57423076923077,2.14846153846154,1.79365384615384,1.50980769230769,1.29692307692308,1.155,1.08403846153846,1.08403846153846,1.155,1.29692307692308,1.50980769230769,1.79365384615385,2.14846153846154,2.57423076923077,3.07153846153846,3.63980769230769,4.27903846153846,4.98923076923077,5.77038461538461,6.6225,7.54557692307693,8.53961538461538,9.60461538461539,10.7405769230769,11.9475,13.2253846153846,14.5742307692308],[14.2194230769231,12.8705769230769,11.5926923076923,10.3857692307692,9.24980769230769,8.18480769230769,7.19076923076923,6.26769230769231,5.41557692307692,4.63442307692308,3.92423076923077,3.285,2.71673076923077,2.21942307692308,1.79365384615384,1.43884615384615,1.155,0.942115384615384,0.800192307692307,0.729230769230769,0.729230769230769,0.800192307692307,0.942115384615385,1.155,null,1.79365384615385,2.21942307692308,2.71673076923077,3.285,3.92423076923077,4.63442307692308,5.41557692307692,6.26769230769231,7.19076923076923,8.18480769230769,9.24980769230769,10.3857692307692,null,12.8705769230769,14.2194230769231],[13.9355769230769,12.5867307692308,11.3088461538462,10.1019230769231,8.96596153846154,7.90096153846154,6.90692307692308,5.98384615384615,5.13173076923077,4.35057692307692,3.64038461538462,3.00115384615384,2.43288461538461,1.93557692307692,1.50980769230769,1.155,0.871153846153846,0.65826923076923,0.516346153846154,0.445384615384615,0.445384615384615,0.516346153846154,0.658269230769231,0.871153846153846,1.155,1.50980769230769,1.93557692307692,2.43288461538462,3.00115384615385,3.64038461538462,4.35057692307693,5.13173076923077,5.98384615384616,6.90692307692308,7.90096153846154,8.96596153846154,10.1019230769231,11.3088461538462,12.5867307692308,13.9355769230769],[13.7226923076923,12.3738461538462,11.0959615384615,9.88903846153846,8.75307692307692,7.68807692307692,6.69403846153846,5.77096153846154,4.91884615384615,4.13769230769231,3.4275,2.78826923076923,2.22,1.72269230769231,1.29692307692308,0.942115384615384,0.65826923076923,0.445384615384615,0.303461538461539,0.2325,0.2325,0.303461538461538,0.445384615384616,0.658269230769231,0.942115384615385,1.29692307692308,1.72269230769231,2.22,2.78826923076923,3.4275,4.13769230769231,4.91884615384615,5.77096153846154,6.69403846153847,7.68807692307692,8.75307692307693,9.88903846153846,11.0959615384615,12.3738461538462,13.7226923076923],[13.5807692307692,12.2319230769231,10.9540384615385,9.74711538461538,8.61115384615385,7.54615384615385,6.55211538461538,5.62903846153846,4.77692307692308,3.99576923076923,3.28557692307692,2.64634615384615,2.07807692307692,1.58076923076923,1.155,0.800192307692307,0.516346153846154,0.303461538461538,0.161538461538462,0.0905769230769231,0.0905769230769232,0.161538461538462,0.303461538461539,0.516346153846154,0.800192307692308,1.155,1.58076923076923,2.07807692307692,2.64634615384616,3.28557692307692,3.99576923076923,4.77692307692308,5.62903846153846,6.55211538461539,7.54615384615385,8.61115384615385,9.74711538461538,10.9540384615385,12.2319230769231,13.5807692307692],[13.5098076923077,12.1609615384615,10.8830769230769,9.67615384615385,8.54019230769231,7.47519230769231,6.48115384615384,5.55807692307692,4.70596153846154,3.92480769230769,3.21461538461538,2.57538461538461,2.00711538461538,1.50980769230769,1.08403846153846,0.729230769230769,0.445384615384615,0.2325,0.0905769230769231,0.0196153846153845,0.0196153846153846,0.0905769230769231,0.2325,0.445384615384616,0.72923076923077,1.08403846153846,1.50980769230769,2.00711538461539,2.57538461538462,3.21461538461539,3.92480769230769,4.70596153846154,5.55807692307692,6.48115384615385,7.47519230769231,8.54019230769231,9.67615384615384,10.8830769230769,12.1609615384615,13.5098076923077],[13.5098076923077,12.1609615384615,10.8830769230769,9.67615384615385,8.54019230769231,7.47519230769231,6.48115384615384,5.55807692307692,4.70596153846154,3.92480769230769,3.21461538461538,2.57538461538461,2.00711538461538,1.50980769230769,1.08403846153846,0.729230769230769,0.445384615384615,0.2325,0.0905769230769232,0.0196153846153846,0.0196153846153847,0.0905769230769232,0.232500000000001,0.445384615384616,0.72923076923077,1.08403846153846,1.50980769230769,2.00711538461539,2.57538461538462,3.21461538461539,3.9248076923077,4.70596153846154,5.55807692307692,6.48115384615385,7.47519230769231,8.54019230769231,9.67615384615384,10.8830769230769,12.1609615384615,13.5098076923077],[13.5807692307692,12.2319230769231,10.9540384615385,9.74711538461538,8.61115384615385,7.54615384615385,6.55211538461538,5.62903846153846,4.77692307692308,3.99576923076923,3.28557692307692,2.64634615384615,2.07807692307692,1.58076923076923,1.155,0.800192307692307,0.516346153846154,0.303461538461538,0.161538461538462,0.0905769230769231,0.0905769230769232,0.161538461538462,0.303461538461539,0.516346153846154,0.800192307692308,1.155,1.58076923076923,2.07807692307692,2.64634615384616,3.28557692307692,3.99576923076923,4.77692307692308,5.62903846153846,6.55211538461539,7.54615384615385,8.61115384615385,9.74711538461538,10.9540384615385,12.2319230769231,13.5807692307692],[13.7226923076923,12.3738461538462,11.0959615384615,9.88903846153846,8.75307692307692,7.68807692307692,6.69403846153846,5.77096153846154,4.91884615384615,4.13769230769231,3.4275,2.78826923076923,2.22,1.72269230769231,1.29692307692308,0.942115384615385,0.658269230769231,0.445384615384616,0.303461538461539,0.2325,0.232500000000001,0.303461538461539,0.445384615384616,0.658269230769232,0.942115384615386,1.29692307692308,1.72269230769231,2.22,2.78826923076923,3.4275,4.13769230769231,4.91884615384615,5.77096153846154,6.69403846153847,7.68807692307692,8.75307692307693,9.88903846153846,11.0959615384615,12.3738461538462,13.7226923076923],[13.9355769230769,12.5867307692308,11.3088461538462,10.1019230769231,8.96596153846154,7.90096153846154,6.90692307692308,5.98384615384615,5.13173076923077,4.35057692307692,3.64038461538462,3.00115384615385,2.43288461538462,1.93557692307692,1.50980769230769,1.155,0.871153846153847,0.658269230769231,0.516346153846154,0.445384615384616,0.445384615384616,0.516346153846154,0.658269230769232,0.871153846153847,1.155,1.50980769230769,1.93557692307692,2.43288461538462,3.00115384615385,3.64038461538462,4.35057692307693,5.13173076923077,5.98384615384616,6.90692307692308,7.90096153846154,8.96596153846154,10.1019230769231,11.3088461538462,12.5867307692308,13.9355769230769],[14.2194230769231,12.8705769230769,11.5926923076923,10.3857692307692,9.24980769230769,8.18480769230769,7.19076923076923,6.26769230769231,5.41557692307692,4.63442307692308,3.92423076923077,3.285,2.71673076923077,2.21942307692308,1.79365384615385,null,1.155,0.942115384615385,0.800192307692308,0.72923076923077,0.72923076923077,0.800192307692308,0.942115384615386,1.155,1.43884615384615,1.79365384615385,2.21942307692308,2.71673076923077,3.285,3.92423076923077,4.63442307692308,5.41557692307692,6.26769230769231,7.19076923076923,8.18480769230769,9.2498076923077,10.3857692307692,11.5926923076923,12.8705769230769,14.2194230769231],[14.5742307692308,13.2253846153846,11.9475,10.7405769230769,9.60461538461538,8.53961538461539,7.54557692307692,6.6225,5.77038461538462,4.98923076923077,4.27903846153846,3.63980769230769,3.07153846153846,2.57423076923077,2.14846153846154,1.79365384615385,1.50980769230769,1.29692307692308,1.155,1.08403846153846,1.08403846153846,1.155,1.29692307692308,1.50980769230769,1.79365384615385,2.14846153846154,2.57423076923077,null,3.6398076923077,4.27903846153846,4.98923076923077,5.77038461538462,6.6225,7.54557692307693,8.53961538461539,9.60461538461539,10.7405769230769,11.9475,13.2253846153846,14.5742307692308],[15,13.6511538461538,12.3732692307692,11.1663461538462,10.0303846153846,8.96538461538462,7.97134615384615,7.04826923076923,6.19615384615385,5.415,4.70480769230769,4.06557692307692,3.49730769230769,3,2.57423076923077,2.21942307692308,1.93557692307692,1.72269230769231,1.58076923076923,1.50980769230769,1.50980769230769,1.58076923076923,1.72269230769231,1.93557692307692,2.21942307692308,2.57423076923077,3,3.49730769230769,4.06557692307693,4.70480769230769,5.415,6.19615384615385,7.04826923076923,7.97134615384616,8.96538461538462,10.0303846153846,11.1663461538462,12.3732692307692,13.6511538461538,15],[15.4973076923077,14.1484615384615,12.8705769230769,11.6636538461538,10.5276923076923,9.46269230769231,8.46865384615385,7.54557692307692,6.69346153846154,5.91230769230769,5.20211538461539,4.56288461538462,3.99461538461538,3.49730769230769,3.07153846153846,2.71673076923077,2.43288461538462,2.22,2.07807692307692,2.00711538461539,2.00711538461539,2.07807692307692,2.22,2.43288461538462,2.71673076923077,null,3.49730769230769,3.99461538461539,4.56288461538462,5.20211538461539,5.9123076923077,6.69346153846154,7.54557692307693,8.46865384615385,9.46269230769231,10.5276923076923,11.6636538461538,12.8705769230769,null,15.4973076923077],[16.0655769230769,14.7167307692308,13.4388461538462,12.2319230769231,11.0959615384615,10.0309615384615,9.03692307692308,8.11384615384616,7.26173076923077,6.48057692307693,5.77038461538462,5.13115384615385,4.56288461538462,4.06557692307693,3.63980769230769,3.285,3.00115384615385,2.78826923076923,2.64634615384616,2.57538461538462,2.57538461538462,2.64634615384616,2.78826923076923,3.00115384615385,3.285,3.63980769230769,4.06557692307693,4.56288461538462,5.13115384615385,5.77038461538462,6.48057692307693,7.26173076923077,8.11384615384616,9.03692307692308,10.0309615384615,11.0959615384615,12.2319230769231,13.4388461538462,14.7167307692308,16.0655769230769],[16.7048076923077,15.3559615384615,14.0780769230769,12.8711538461538,11.7351923076923,10.6701923076923,9.67615384615384,8.75307692307692,7.90096153846154,7.11980769230769,6.40961538461538,5.77038461538462,5.20211538461538,4.70480769230769,4.27903846153846,3.92423076923077,3.64038461538462,3.4275,3.28557692307692,3.21461538461539,3.21461538461539,3.28557692307692,3.4275,3.64038461538462,3.92423076923077,4.27903846153846,4.70480769230769,5.20211538461539,5.77038461538462,6.40961538461539,7.1198076923077,7.90096153846154,8.75307692307693,9.67615384615385,10.6701923076923,11.7351923076923,12.8711538461538,14.0780769230769,15.3559615384615,16.7048076923077],[17.415,16.0661538461538,14.7882692307692,13.5813461538462,12.4453846153846,11.3803846153846,10.3863461538462,9.46326923076923,8.61115384615385,7.83,7.11980769230769,6.48057692307693,5.91230769230769,5.415,4.98923076923077,4.63442307692308,4.35057692307693,4.13769230769231,3.99576923076923,3.92480769230769,3.9248076923077,3.99576923076923,4.13769230769231,4.35057692307693,4.63442307692308,4.98923076923077,5.415,5.9123076923077,6.48057692307693,7.1198076923077,7.83,8.61115384615385,9.46326923076923,10.3863461538462,11.3803846153846,12.4453846153846,13.5813461538462,14.7882692307692,16.0661538461539,17.415],[18.1961538461538,16.8473076923077,15.5694230769231,14.3625,13.2265384615385,12.1615384615385,11.1675,10.2444230769231,9.39230769230769,8.61115384615385,7.90096153846154,7.26173076923077,6.69346153846154,6.19615384615385,5.77038461538461,5.41557692307692,5.13173076923077,4.91884615384615,4.77692307692308,4.70596153846154,4.70596153846154,4.77692307692308,4.91884615384615,5.13173076923077,5.41557692307692,5.77038461538462,6.19615384615385,6.69346153846154,7.26173076923077,7.90096153846154,8.61115384615385,9.39230769230769,10.2444230769231,11.1675,12.1615384615385,13.2265384615385,14.3625,15.5694230769231,16.8473076923077,18.1961538461538],[19.0482692307692,17.6994230769231,16.4215384615385,15.2146153846154,14.0786538461538,13.0136538461538,12.0196153846154,11.0965384615385,10.2444230769231,9.46326923076923,8.75307692307693,8.11384615384615,7.54557692307692,7.04826923076923,6.6225,6.26769230769231,5.98384615384616,5.77096153846154,5.62903846153846,5.55807692307692,5.55807692307692,5.62903846153846,5.77096153846154,5.98384615384616,6.26769230769231,6.6225,7.04826923076923,7.54557692307693,8.11384615384616,8.75307692307693,9.46326923076924,10.2444230769231,11.0965384615385,12.0196153846154,13.0136538461538,14.0786538461539,15.2146153846154,16.4215384615385,17.6994230769231,19.0482692307692],[19.9713461538462,18.6225,17.3446153846154,16.1376923076923,15.0017307692308,13.9367307692308,12.9426923076923,12.0196153846154,11.1675,10.3863461538462,9.67615384615385,9.03692307692308,8.46865384615385,7.97134615384616,7.54557692307693,7.19076923076923,6.90692307692308,6.69403846153847,6.55211538461539,6.48115384615385,6.48115384615385,6.55211538461539,6.69403846153847,6.90692307692308,7.19076923076924,7.54557692307693,7.97134615384616,8.46865384615385,9.03692307692308,9.67615384615385,10.3863461538462,11.1675,12.0196153846154,12.9426923076923,13.9367307692308,15.0017307692308,16.1376923076923,17.3446153846154,18.6225,null],[20.9653846153846,19.6165384615385,18.3386538461538,17.1317307692308,15.9957692307692,14.9307692307692,13.9367307692308,13.0136538461538,12.1615384615385,11.3803846153846,10.6701923076923,10.0309615384615,9.46269230769231,8.96538461538462,8.53961538461538,8.18480769230769,7.90096153846154,7.68807692307692,7.54615384615385,7.47519230769231,7.47519230769231,7.54615384615385,7.68807692307692,7.90096153846154,8.18480769230769,8.53961538461539,8.96538461538462,9.46269230769231,10.0309615384615,10.6701923076923,11.3803846153846,12.1615384615385,13.0136538461538,13.9367307692308,14.9307692307692,15.9957692307692,17.1317307692308,18.3386538461538,null,null],[22.0303846153846,20.6815384615385,19.4036538461538,18.1967307692308,17.0607692307692,15.9957692307692,15.0017307692308,14.0786538461538,13.2265384615385,12.4453846153846,11.7351923076923,11.0959615384615,10.5276923076923,10.0303846153846,9.60461538461539,9.24980769230769,8.96596153846154,8.75307692307693,8.61115384615385,8.54019230769231,8.54019230769231,8.61115384615385,8.75307692307693,8.96596153846154,9.2498076923077,9.60461538461539,10.0303846153846,10.5276923076923,11.0959615384615,11.7351923076923,12.4453846153846,13.2265384615385,14.0786538461539,15.0017307692308,15.9957692307692,17.0607692307692,18.1967307692308,null,null,22.0303846153846],[23.1663461538462,21.8175,20.5396153846154,19.3326923076923,18.1967307692308,17.1317307692308,16.1376923076923,15.2146153846154,14.3625,13.5813461538462,12.8711538461538,12.2319230769231,11.6636538461538,11.1663461538462,10.7405769230769,10.3857692307692,10.1019230769231,9.88903846153846,9.74711538461538,9.67615384615385,9.67615384615385,9.74711538461538,9.88903846153846,10.1019230769231,10.3857692307692,10.7405769230769,11.1663461538462,11.6636538461538,12.2319230769231,12.8711538461538,13.5813461538462,14.3625,15.2146153846154,16.1376923076923,17.1317307692308,18.1967307692308,null,null,null,23.1663461538462],[24.3732692307692,23.0244230769231,21.7465384615385,20.5396153846154,19.4036538461538,18.3386538461538,17.3446153846154,16.4215384615385,15.5694230769231,14.7882692307692,14.0780769230769,13.4388461538462,12.8705769230769,12.3732692307692,11.9475,null,11.3088461538462,11.0959615384615,10.9540384615385,10.8830769230769,10.8830769230769,10.9540384615385,11.0959615384615,11.3088461538462,11.5926923076923,11.9475,12.3732692307692,12.8705769230769,13.4388461538462,14.0780769230769,14.7882692307692,15.5694230769231,16.4215384615385,17.3446153846154,18.3386538461538,null,null,null,23.0244230769231,24.3732692307692],[25.6511538461538,24.3023076923077,23.0244230769231,21.8175,20.6815384615385,19.6165384615385,18.6225,17.6994230769231,16.8473076923077,16.0661538461538,15.3559615384615,14.7167307692308,14.1484615384615,13.6511538461538,13.2253846153846,12.8705769230769,12.5867307692308,12.3738461538462,12.2319230769231,12.1609615384615,12.1609615384615,12.2319230769231,12.3738461538462,12.5867307692308,12.8705769230769,13.2253846153846,13.6511538461538,null,14.7167307692308,15.3559615384615,16.0661538461539,16.8473076923077,17.6994230769231,null,null,null,21.8175,23.0244230769231,24.3023076923077,25.6511538461538],[27,25.6511538461538,24.3732692307692,23.1663461538462,22.0303846153846,20.9653846153846,19.9713461538462,19.0482692307692,18.1961538461538,17.415,16.7048076923077,16.0655769230769,15.4973076923077,15,14.5742307692308,14.2194230769231,13.9355769230769,13.7226923076923,13.5807692307692,13.5098076923077,13.5098076923077,13.5807692307692,13.7226923076923,13.9355769230769,14.2194230769231,14.5742307692308,15,15.4973076923077,16.0655769230769,16.7048076923077,17.415,18.1961538461538,19.0482692307692,19.9713461538462,null,null,23.1663461538462,24.3732692307692,25.6511538461538,27]],&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;The goal of ridge regression is to find the optimal combination of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights that minimizes the traditional loss function, while also accounting for the added “loss” incurred by the penalty term. Looking at the plot above, you can see that as the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights both move away from 0, the ridge penalty function shows a slow increase near 0, which becomes progressively steeper as we move away from the origin. Further, as &lt;span class=&#34;math inline&#34;&gt;\(\lambda \rightarrow \infty\)&lt;/span&gt;, the penalty function becomes steeper and steeper. Altogether, ridge regression does not prefer particular values for &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights—it only prefers for all &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights to be closer to 0.&lt;/p&gt;
&lt;div id=&#34;frequentist-ridge-regression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Frequentist Ridge Regression&lt;/h4&gt;
&lt;p&gt;One of the problems that arises from equation 4 is that we need to select a value for &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, the parameter that controls how much we learn from our training data. But how should we select an optimal &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;? Of course, we could just pick a value based on our intuition, but that would likely lead to non-optimal solutions in many cases. Therefore, &lt;strong&gt;frequentist ridge regression typically relies on cross-validation (CV) to select&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; (see &lt;a href=&#34;https://en.wikipedia.org/wiki/Hyperparameter_optimization&#34;&gt;here&lt;/a&gt; for more details on hyper-parameter tuning/optimization). CV proceeds by:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;generating a grid of values for our hyper-parameter (&lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;splitting the training data into k equivalently sized training sets (&lt;em&gt;k folds&lt;/em&gt;),&lt;/li&gt;
&lt;li&gt;fitting a model to k-1 folds and testing prediction accuracy on the left-out fold,&lt;/li&gt;
&lt;li&gt;repeat step 3 until each fold has been left out exactly once, and&lt;/li&gt;
&lt;li&gt;iterating steps 2-4 for each hyper-parameter in the grid defined by step 1&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When we are finished, we select the value of &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; in the grid that minimizes the CV error (step 3). The R code below proceeds through the above steps, using the R &lt;code&gt;glmnet&lt;/code&gt; package to fit the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Penalty term values to test
lambdas &amp;lt;- 10^seq(3, -2, by = -.1)

# Fit and cross-validate
fit_ridge &amp;lt;- glmnet(x = train_scale[,-8], y = train_scale[,8], 
                    alpha = 0, lambda = lambdas, intercept = F)
cv_ridge &amp;lt;- cv.glmnet(x = train_scale[,-8], y = train_scale[,8], 
                      alpha = 0, lambda = lambdas, intercept = F, 
                      grouped = FALSE)

# Find optimal penalty term (lambda)
opt_lambda &amp;lt;- cv_ridge$lambda.min

# Generate predictions with opitmal lambda model from cross-validation
y_pred &amp;lt;- predict(fit_ridge, s = opt_lambda, newx = test_scale[,-8])

# Plot cor(predicted, actual)
qplot(x = y_pred[,1], y = test_scale[,8],
      main = paste0(&amp;quot;Frequentist Ridge Regression:\nEstimating &amp;quot;, expression(lambda), 
                    &amp;quot; with CV\nr = &amp;quot;, round(cor(test_scale[,8], y_pred[,1]), 2))) +
  #geom_point(aes(x = , y = train_scale[,8]))
  xlab(&amp;quot;Model Predicted Pr(Acceptance)&amp;quot;) +
  ylab(&amp;quot;Actual Pr(Acceptance)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig3-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Woah, much better than traditional linear regression! Before diving into these results, however, let’s first explore Bayesian ridge regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-ridge-regression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bayesian Ridge Regression&lt;/h4&gt;
&lt;p&gt;Bayesian Ridge regression differs from the frequentist variant in only one way, and it is with how we think of the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; penalty term. In the frequentist perspective, we showed that &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; effectively tells our model how much it is allowed to learn from the data. &lt;em&gt;In the Bayesian world, we can capture such an effect in the form of a prior distribution over our&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &lt;em&gt;weights&lt;/em&gt;. To reveal the extraordinary power hiding behind this simple idea, let’s first discuss Bayesian linear regression.&lt;/p&gt;
&lt;p&gt;Bayesian models view estimation as a problem of integrating prior information with information gained from data, which we formalize using probability distributions. This differs from the frequntist view, which treats regression as an opimization problem that results in a point estimate (e.g., minimizing squared error). A Bayesian regression model takes the form of:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{i} | \beta_{0},\boldsymbol{x_{i}},\boldsymbol{\beta},\sigma \sim \mathcal{N}(\beta_{0} + \sum_{j=1}^{p}x_{ij}\beta_{j}, \sigma) \tag{5}\]&lt;/span&gt;
Importantly, &lt;em&gt;Bayesian models require us to specify a &lt;strong&gt;prior distribution&lt;/strong&gt; for each parameter we seek to estimate&lt;/em&gt;. Therefore, we need to specify a prior on the intercept (&lt;span class=&#34;math inline&#34;&gt;\(\beta_{0}\)&lt;/span&gt;), slopes (&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt;), and error variance (&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;) in equation 5. Since we are standardizing all of our predictors and outcome variable(s), we will ignore the intercept term. Then, we are left with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. &lt;strong&gt;Crucially, our choice of prior distribution on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; is what determines how much information we learn from the data, analagous to the penalty term &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; used for frequentist regularization.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Like before, you can gain an intuition for this behavior by imagnining extreme cases. For example, suppose that we assumed that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta} \sim \mathcal{U}(-\infty,+\infty)\)&lt;/span&gt;. That is, we assume that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; can take on any real-valued number, and every value is equally likely. In this case, the mode of the posterior distribution on each &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight will be equivalent to the maximum likelihood estimate of the respective &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight, and by extention the OLS estimate under the normality assumption (see our prior &lt;a href=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/&#34;&gt;post on parameter estimation&lt;/a&gt; for more details). If an unbounded uniform distribution on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; produces the same behavior as traditional linear regression, it follows from our discussions above that it also allows us to maximally learn from the data.&lt;/p&gt;
&lt;p&gt;Now, what can we do to restrict learning in a Bayesian model? Well, we can use a prior distribution that pulls the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights toward 0 (unlike the unbounded uniform distribution), similar to the &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; penalty parameter in frequentist regularization! Specifically, specifying the following prior distribution on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; is mathematically equivalent in expectation (see &lt;a href=&#34;https://stats.stackexchange.com/questions/163388/l2-regularization-is-equivalent-to-gaussian-prior&#34;&gt;here&lt;/a&gt;) to using the ridge penalty in the frequentist model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\boldsymbol{\beta} \sim \mathcal{N}(0, \sigma_{\beta})\tag{6}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are two important points that the prior distribution in equation 6 conveys:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The normal distribution places very little prior probability on large-magnitude &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights (i.e. far from 0), while placing high prior probability on small-magnitude weights (i.e. near 0), and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta}\)&lt;/span&gt; controls how wide the normal distribution is, thus controlling the specific amount of prior probability placed on small- to large-magnitude &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Bayesian version differs most in that we jointly estimate the “penalization term” &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; in a single model, as opposed to using CV to select an optimal &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; for frequentist regression. Because we are jointly estimating &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta}\)&lt;/span&gt; along with individual-level &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights, we can actually view Bayesian ridge regression as a simple hierarchical Bayesian model, where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta}\)&lt;/span&gt; is interpreted as a group-level scaling parameter that is estimated from pooled information across individual &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights.&lt;/p&gt;
&lt;p&gt;Below is the Stan code that specifies the Bayesian variant of ridge regression. Note that this code is based on &lt;a href=&#34;https://osf.io/cg8fq/&#34;&gt;Erp, Oberski, &amp;amp; Mulder (2019)&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data{
    int N_train;             // &amp;quot;# training observations&amp;quot;
    int N_test;              // &amp;quot;# test observations&amp;quot;
    int N_pred;              // &amp;quot;# predictor variables&amp;quot;
    vector[N_train] y_train; // &amp;quot;training outcomes&amp;quot;
    matrix[N_train, N_pred] X_train; // &amp;quot;training data&amp;quot;
    matrix[N_test, N_pred] X_test;   // &amp;quot;testing data&amp;quot;
}
parameters{
    real&amp;lt;lower=0&amp;gt; sigma;   // &amp;quot;error SD&amp;quot;
    real&amp;lt;lower=0&amp;gt; sigma_B; // &amp;quot;hierarchical SD across betas&amp;quot;
    vector[N_pred] beta;   // &amp;quot;regression beta weights&amp;quot;
}
model{
  // &amp;quot;group-level (hierarchical) SD across betas&amp;quot;
  sigma_B ~ cauchy(0, 1);
  
  // &amp;quot;model error SD&amp;quot;
  sigma ~ normal(0, 1);
  
  // &amp;quot;beta prior (provides &amp;#39;ridge&amp;#39; regularization)&amp;quot;
  beta ~ normal(0, sigma_B);
    
  // &amp;quot;model likelihood&amp;quot;
    y_train ~ normal(X_train*beta, sigma);
}
generated quantities{ 
    real y_test[N_test]; // &amp;quot;test data predictions&amp;quot;
    for(i in 1:N_test){
        y_test[i] = normal_rng(X_test[i,] * beta, sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The important parts of the above code are in the &lt;code&gt;model{...}&lt;/code&gt; section. While the parameterization looks different from the frequentist model (equation 4), the behavior of the model is equivalent. In particular, we have a &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta}\)&lt;/span&gt; “penalty” term, can shrink the variation between &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights toward 0. Intuitively, as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta} \rightarrow 0\)&lt;/span&gt;, all (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;) weights are shrunk toward 0 (no learning from data). Conversely, as &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta} \rightarrow \infty\)&lt;/span&gt;, prior on &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{\beta}\)&lt;/span&gt; becomes uniform. A uniform prior denotes no penalty at all, and we are left with traditional, non-regularized regression (albeit we get a joint posterior distribution as opposed to point estimates). It is worth noting that the mode of the resulting posterior distribution over &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; will be equivalent to the maximum likelihood estimate when the prior on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\beta}\)&lt;/span&gt; is uniform (for details, see MLE/MAP estimation described in a &lt;a href=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/&#34;&gt;previous post&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Now, let’s try fitting the model!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First, prepare data for Stan
stan_dat &amp;lt;- list(N_train = nrow(train_scale),
                 N_test  = nrow(test_scale),
                 N_pred  = ncol(train_scale)-1,
                 y_train = train_scale[,8],
                 X_train = train_scale[,-8],
                 X_test  = test_scale[,-8])

# Fit the model using Stan&amp;#39;s NUTS HMC sampler
fit_bayes_ridge &amp;lt;- sampling(bayes_ridge, stan_dat, iter = 2000, 
                            warmup = 500, chains = 3, cores = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: There were 1 divergent transitions after warmup. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Examine the pairs() plot to diagnose sampling problems&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract posterior distribution (parameters and predictions)
post_ridge &amp;lt;- rstan::extract(fit_bayes_ridge)

# Compute mean of the posterior predictive distribution over test set predictors,
# which integrates out uncertainty in parameter estimates
y_pred_bayes &amp;lt;- apply(post_ridge$y_test, 2, mean)

# Plot correlation between posterior predicted mean and actual Pr(Acceptance)
qplot(x = y_pred_bayes, y = test_scale[,8],
      main = paste0(&amp;quot;Bayesian Ridge Regression:\nEstimating &amp;quot;, expression(lambda), 
                    &amp;quot; Hierarchically\nr = &amp;quot;, round(cor(test_scale[,8], y_pred_bayes), 2))) +
    xlab(&amp;quot;Model Predicted Pr(Acceptance)&amp;quot;) +
    ylab(&amp;quot;Actual Pr(Acceptance)&amp;quot;) +
    theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig4-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;It is clear that the Bayesian ridge regression is giving results very similar (almost identical) to frequentist ridge regression as outlined above. Importantly, the Bayesian method also offers straightforward measures of uncertainty for both parameter estimates and predictions on the test data. The plot above integrates over the prediction uncertainty (i.e. integrating over the posterior predictive distribution). To really appreciate the prediction uncertainty, we compute the correlation between predicted and actual Pr(Acceptance) for each posterior sample, and then visualize the resulting distribution of correlations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Iterate through each posterior prediction and compute cor(predicted, actual),
# which preserves uncertainty in parameter estimates
y_pred_bayes2 &amp;lt;- apply(post_ridge$y_test, 1, function(x) cor(x, test_scale[,8]))

# Plot posterior predictive distribution of cor(predicted, actual)
qplot(x = y_pred_bayes2, geom = &amp;quot;histogram&amp;quot;, bins = 50,
      xlab = &amp;quot;Correlation between\nPredicted and Actual Pr(Acceptance)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig5-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, there is good amount of uncertainty contained within the posterior predictive distribution, and the above plot shows it well. Importantly, this uncertainty reduce in proportion to how much data (i.e. observations) we have. Given that we only used 20 observations, these results are actually pretty good!&lt;/p&gt;
&lt;p&gt;Another method is to visualize the scatterplot like above, but to include prediction intervals around the predicted mean estimates for each observation in the test set:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# `bayesplot` has many convenience functions for working with posteriors
color_scheme_set(scheme = &amp;quot;darkgray&amp;quot;)
ppc_intervals(x = colMeans(post_ridge$y_test), y = test_scale[,8],
              yrep = post_ridge$y_test, prob = 0.95) +
  ggtitle(&amp;quot;95% Posterior Prediction Intervals&amp;quot;) +
  xlab(&amp;quot;Model Predicted Pr(Acceptance)&amp;quot;) +
  ylab(&amp;quot;Actual Pr(Acceptance)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig6-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Looks great! We see that the 95% posterior prediction intervals contain the actual values in virtually all cases. Importantly, this visualization also makes it clear just how much uncertainty there is for each individual observation.&lt;/p&gt;
&lt;p&gt;Next up, we will explore Traditional and Bayesian LASSO regressions, followed by a discussion of the benefits of Bayesian over frequentist regularization more broadly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;lasso-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;LASSO Regression&lt;/h3&gt;
&lt;p&gt;Now that we have covered ridge regression, LASSO regression only involves a minor revision to the loss function. Specifically, as opposed to penalizing the model based on the sum of squared &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights, we will penalize the model by the sum of the absolute value of &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\underbrace{\underset{\boldsymbol{\beta}}{argmin}\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2}_{\text{Traditional Loss Function}} + \underbrace{\lambda\sum_{j=1}^{p}|\beta_{j}|}_{\text{LASSO Penalty}}\tag{7}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That’s it! But what difference does this make? At this point, it is useful to visualize the effect of the LASSO penalty on &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight estimates, as we did above for the ridge penalty. As before, we are looking at penalty functions for varying settings of &lt;span class=&#34;math inline&#34;&gt;\(\lambda \in \{0, .5, 1.5\}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:672px;height:480px;&#34; class=&#34;plotly html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;visdat&#34;:{&#34;1506065ae472e&#34;:[&#34;function () &#34;,&#34;plotlyVisDat&#34;]},&#34;cur_data&#34;:&#34;1506065ae472e&#34;,&#34;attrs&#34;:{&#34;1506065ae472e&#34;:{&#34;showscale&#34;:false,&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;z&#34;:{},&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;opacity&#34;:0.5,&#34;inherit&#34;:true},&#34;1506065ae472e.1&#34;:{&#34;showscale&#34;:false,&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;z&#34;:{},&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;inherit&#34;:true},&#34;1506065ae472e.2&#34;:{&#34;showscale&#34;:false,&#34;alpha_stroke&#34;:1,&#34;sizes&#34;:[10,100],&#34;spans&#34;:[1,20],&#34;z&#34;:{},&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;inherit&#34;:true}},&#34;layout&#34;:{&#34;margin&#34;:{&#34;b&#34;:40,&#34;l&#34;:60,&#34;t&#34;:25,&#34;r&#34;:10},&#34;scene&#34;:{&#34;xaxis&#34;:{&#34;title&#34;:&#34;Beta_1&#34;},&#34;yaxis&#34;:{&#34;title&#34;:&#34;Beta_2&#34;},&#34;zaxis&#34;:{&#34;title&#34;:&#34;Penalty&#34;}},&#34;title&#34;:&#34;LASSO Penalty Contour&#34;,&#34;hovermode&#34;:&#34;closest&#34;,&#34;showlegend&#34;:true},&#34;source&#34;:&#34;A&#34;,&#34;config&#34;:{&#34;modeBarButtonsToAdd&#34;:[&#34;hoverclosest&#34;,&#34;hovercompare&#34;],&#34;showSendToCloud&#34;:false},&#34;data&#34;:[{&#34;colorbar&#34;:{&#34;title&#34;:&#34;lasso_p1$z&lt;br /&gt;lasso_p2$z&lt;br /&gt;lasso_p3$z&#34;,&#34;ticklen&#34;:2},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:false,&#34;z&#34;:[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,null,null,null,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0,0,0,0]],&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;opacity&#34;:0.5,&#34;frame&#34;:null},{&#34;colorbar&#34;:{&#34;title&#34;:&#34;lasso_p1$z&lt;br /&gt;lasso_p2$z&lt;br /&gt;lasso_p3$z&#34;,&#34;ticklen&#34;:2},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:false,&#34;z&#34;:[[3,2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077,2.84615384615385,2.92307692307692,3],[2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077,2.84615384615385,2.92307692307692],[2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077,2.84615384615385],[2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077],[2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769],[2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,2.61538461538462],[2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154],[2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846],[2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538],[2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231],[2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923],[2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615],[2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308],[2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.538461538461539,0.615384615384615,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2],[1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.461538461538461,0.538461538461538,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692],[1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615384,0.384615384615385,0.461538461538461,0.538461538461539,0.615384615384615,null,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,null,1.76923076923077,1.84615384615385],[1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692307,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077],[1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769],[1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.153846153846154,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307693,0.76923076923077,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462],[1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615384,0.307692307692307,0.230769230769231,0.153846153846154,0.0769230769230766,0.0769230769230769,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538461,0.538461538461538,0.615384615384615,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154],[1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461539,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.153846153846154,0.0769230769230769,0.0769230769230771,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154],[1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.153846153846154,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307693,0.76923076923077,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462],[1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307693,0.615384615384615,0.538461538461539,0.461538461538462,0.384615384615385,0.307692307692308,0.230769230769231,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.76923076923077,0.846153846153847,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769],[1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461539,0.461538461538462,0.384615384615385,0.307692307692308,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076924,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077],[1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,null,0.615384615384615,0.538461538461539,0.461538461538462,0.384615384615385,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385],[1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461539,0.461538461538461,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,null,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692],[2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.538461538461539,0.615384615384615,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2],[2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307693,0.615384615384615,0.615384615384616,0.692307692307693,0.76923076923077,0.846153846153846,0.923076923076923,null,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,null,2.07692307692308],[2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461539,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.76923076923077,0.692307692307693,0.692307692307693,0.76923076923077,0.846153846153847,0.923076923076924,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615],[2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923],[2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231],[2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538],[2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846],[2.53846153846154,2.46153846153846,2.38461538461539,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,null],[2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,null,null],[2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461539,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,null,null,2.69230769230769],[2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,null,null,null,2.76923076923077],[2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,null,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,null,null,null,2.76923076923077,2.84615384615385],[2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,null,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,null,null,null,2.69230769230769,2.76923076923077,2.84615384615385,2.92307692307692],[3,2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,null,null,2.76923076923077,2.84615384615385,2.92307692307692,3]],&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;frame&#34;:null},{&#34;colorbar&#34;:{&#34;title&#34;:&#34;lasso_p1$z&lt;br /&gt;lasso_p2$z&lt;br /&gt;lasso_p3$z&#34;,&#34;ticklen&#34;:2},&#34;colorscale&#34;:[[&#34;0&#34;,&#34;rgba(68,1,84,1)&#34;],[&#34;0.0416666666666667&#34;,&#34;rgba(70,19,97,1)&#34;],[&#34;0.0833333333333333&#34;,&#34;rgba(72,32,111,1)&#34;],[&#34;0.125&#34;,&#34;rgba(71,45,122,1)&#34;],[&#34;0.166666666666667&#34;,&#34;rgba(68,58,128,1)&#34;],[&#34;0.208333333333333&#34;,&#34;rgba(64,70,135,1)&#34;],[&#34;0.25&#34;,&#34;rgba(60,82,138,1)&#34;],[&#34;0.291666666666667&#34;,&#34;rgba(56,93,140,1)&#34;],[&#34;0.333333333333333&#34;,&#34;rgba(49,104,142,1)&#34;],[&#34;0.375&#34;,&#34;rgba(46,114,142,1)&#34;],[&#34;0.416666666666667&#34;,&#34;rgba(42,123,142,1)&#34;],[&#34;0.458333333333333&#34;,&#34;rgba(38,133,141,1)&#34;],[&#34;0.5&#34;,&#34;rgba(37,144,140,1)&#34;],[&#34;0.541666666666667&#34;,&#34;rgba(33,154,138,1)&#34;],[&#34;0.583333333333333&#34;,&#34;rgba(39,164,133,1)&#34;],[&#34;0.625&#34;,&#34;rgba(47,174,127,1)&#34;],[&#34;0.666666666666667&#34;,&#34;rgba(53,183,121,1)&#34;],[&#34;0.708333333333333&#34;,&#34;rgba(79,191,110,1)&#34;],[&#34;0.75&#34;,&#34;rgba(98,199,98,1)&#34;],[&#34;0.791666666666667&#34;,&#34;rgba(119,207,85,1)&#34;],[&#34;0.833333333333333&#34;,&#34;rgba(147,214,70,1)&#34;],[&#34;0.875&#34;,&#34;rgba(172,220,52,1)&#34;],[&#34;0.916666666666667&#34;,&#34;rgba(199,225,42,1)&#34;],[&#34;0.958333333333333&#34;,&#34;rgba(226,228,40,1)&#34;],[&#34;1&#34;,&#34;rgba(253,231,37,1)&#34;]],&#34;showscale&#34;:false,&#34;z&#34;:[[9,8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538462,7.84615384615385,8.07692307692308,8.30769230769231,8.53846153846154,8.76923076923077,9],[8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461538,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538461,7.84615384615385,8.07692307692308,8.30769230769231,8.53846153846154,8.76923076923077],[8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,7.61538461538462,7.84615384615385,8.07692307692308,8.30769230769231,8.53846153846154],[8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461538,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538461,7.84615384615385,8.07692307692308,8.30769230769231],[8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461538,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461538,7.61538461538461,7.84615384615385,8.07692307692308],[7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538462,7.84615384615385],[7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538461],[7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539],[7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615],[6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692],[6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769],[6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846],[6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923],[6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6],[5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.38461538461538,1.61538461538461,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077],[5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,1.15384615384615,1.38461538461538,1.61538461538462,1.84615384615385,null,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538462,4.84615384615385,null,5.30769230769231,5.53846153846154],[5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076922,0.923076923076923,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231],[5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307692,0.692307692307693,0.923076923076923,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538462,4.84615384615385,5.07692307692308],[4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307693,0.461538461538461,0.461538461538462,0.692307692307693,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385],[4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076922,0.692307692307692,0.461538461538461,0.23076923076923,0.230769230769231,0.461538461538461,0.692307692307693,0.923076923076923,1.15384615384615,1.38461538461538,1.61538461538461,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461],[4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307693,0.461538461538462,0.230769230769231,0.230769230769231,0.461538461538462,0.692307692307693,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462],[4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307693,0.461538461538461,0.461538461538462,0.692307692307693,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385],[5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461539,1.15384615384615,0.923076923076924,0.692307692307693,0.692307692307693,0.923076923076924,1.15384615384616,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308],[5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384616,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461539,1.15384615384615,0.923076923076923,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231],[5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,null,1.84615384615385,1.61538461538462,1.38461538461539,1.15384615384615,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154],[5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461538,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,null,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077],[6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6],[6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,null,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,null,6.23076923076923],[6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307693,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846],[6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769],[6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384616,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692],[7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615],[7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539],[7.61538461538462,7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,null],[7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,null,null],[8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,null,null,8.07692307692308],[8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,null,null,null,8.30769230769231],[8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,null,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,null,null,null,8.30769230769231,8.53846153846154],[8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,null,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,null,null,null,8.07692307692308,8.30769230769231,8.53846153846154,8.76923076923077],[9,8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538462,null,null,8.30769230769231,8.53846153846154,8.76923076923077,9]],&#34;type&#34;:&#34;surface&#34;,&#34;x&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;y&#34;:[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],&#34;frame&#34;:null}],&#34;highlight&#34;:{&#34;on&#34;:&#34;plotly_click&#34;,&#34;persistent&#34;:false,&#34;dynamic&#34;:false,&#34;selectize&#34;:false,&#34;opacityDim&#34;:0.2,&#34;selected&#34;:{&#34;opacity&#34;:1},&#34;debounce&#34;:0},&#34;shinyEvents&#34;:[&#34;plotly_hover&#34;,&#34;plotly_click&#34;,&#34;plotly_selected&#34;,&#34;plotly_relayout&#34;,&#34;plotly_brushed&#34;,&#34;plotly_brushing&#34;,&#34;plotly_clickannotation&#34;,&#34;plotly_doubleclick&#34;,&#34;plotly_deselect&#34;,&#34;plotly_afterplot&#34;,&#34;plotly_sunburstclick&#34;],&#34;base_url&#34;:&#34;https://plot.ly&#34;},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Interesting! We can see here that unlike for the ridge penalty, there are sharp corners in the geometry, which correspond to when either &lt;span class=&#34;math inline&#34;&gt;\(\beta_{1}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\beta_{2}\)&lt;/span&gt; equal 0. Further, the LASSO penalty shows a linear increase as we move away from the origin. Together, the sharp corners on 0-valued &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights and the linear increase in penalty make the LASSO penalty favor solutions that set one (or more) &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights to exactly 0.&lt;/p&gt;
&lt;div id=&#34;frequentist-lasso-regression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Frequentist LASSO Regression&lt;/h4&gt;
&lt;p&gt;The code below is very similar to what we had for traditional ridge regression, except that we will set &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 1\)&lt;/span&gt; as opposed to &lt;span class=&#34;math inline&#34;&gt;\(\alpha = 0\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Scale training data (get rid of ID)
train_scale &amp;lt;- scale(train_dat[,2:9])

# Penalty term values to test
lambdas &amp;lt;- 10^seq(3, -2, by = -.1)

# Fit and cross-validate
fit_lasso &amp;lt;- glmnet(x = train_scale[,-8], y = train_scale[,8], 
                    alpha = 1, lambda = lambdas, intercept = F)
cv_lasso &amp;lt;- cv.glmnet(x = train_scale[,-8], y = train_scale[,8], 
                      alpha = 1, lambda = lambdas, intercept = F, 
                      grouped = FALSE)

# Find optimal penalty term (lambda)
opt_lambda_lasso &amp;lt;- cv_lasso$lambda.min

# Generate predictions on the test set
y_pred_lasso &amp;lt;- predict(fit_lasso, s = opt_lambda_lasso, newx = test_scale[,-8])

# Plot cor(predicted, actual)
qplot(x = y_pred_lasso[,1], y = test_scale[,8],
      main = paste0(&amp;quot;r = &amp;quot;, round(cor(test_scale[,8], y_pred_lasso[,1]), 2))) +
  xlab(&amp;quot;Model Predicted Pr(Acceptance)&amp;quot;) +
  ylab(&amp;quot;Actual Pr(Acceptance)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig8-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, there is not much of a performance difference between frequentist ridge and LASSO regression in this dataset. However, it is worth noting that the LASSO is often more useful when we expect the solution to be sparse—that is, we expect that many of the predictors in our model are mostly noise, and are goal is to identify more robust effects. In such a case, the LASSO does a good job of setting &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights on noisy predictors to exactly 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bayesian-lasso-regression&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Bayesian LASSO Regression&lt;/h4&gt;
&lt;p&gt;Recall that for Bayesian ridge regression, we only needed to specifiy a normal prior distribution to the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights that we were aiming to regularize. For Bayesian LASSO regression, the only difference is in the form of the prior distribution. Specifically, &lt;strong&gt;setting a Laplace (i.e. double-exponential) prior on the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights is mathematically equivalent in expectation to the frequentist LASSO penalty&lt;/strong&gt; (see e.g., &lt;a href=&#34;https://stats.stackexchange.com/questions/182098/why-is-lasso-penalty-equivalent-to-the-double-exponential-laplace-prior&#34;&gt;here&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\boldsymbol{\beta} \sim \text{double-exponential}(0, \tau_{\beta})\tag{8}\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(\tau_{\beta}~(0 &amp;lt; \tau_{\beta} &amp;lt; \infty)\)&lt;/span&gt; is a scale parameter that controls how peaked the prior distribution is around the center (0 in this case). As &lt;span class=&#34;math inline&#34;&gt;\(\tau_{\beta} \rightarrow 0\)&lt;/span&gt;, then the model assigns infinite weight on 0-valued &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights (i.e. no learning from data). Conversely, as &lt;span class=&#34;math inline&#34;&gt;\(\tau_{\beta} \rightarrow \infty\)&lt;/span&gt;, the prior reduces to a uniform prior, thus leading to no regularization at all (i.e traditional regression, albeit with posterior distributions rather than point estimates). It can be useful to visualize what this prior distribution looks like over a single &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight. For example, centered at 0 with &lt;span class=&#34;math inline&#34;&gt;\(\tau_{\beta} = 0.5\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(x = seq(-3, 3, .1), y = rmutil::dlaplace(seq(-3, 3, .1), 0, .5), geom = &amp;quot;line&amp;quot;) +
  ggtitle(&amp;quot;Laplace Prior with tau = 0.5&amp;quot;) +
  xlab(expression(beta[1])) +
  ylab(&amp;quot;Density&amp;quot;) +
  theme_minimal(base_size = 20) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;rmutil&amp;#39;:
##   method         from
##   print.response httr&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig9-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Compared to the ridge prior, which is a normal distribution, it is clear that the Laplace distribiution places much more probability mass directly on 0, which produces the variable selection effect specific to LASSO regression. Note also that such peakedness explains why there are sharp corners in the frequentist penalty function (see the LASSO contour plot above).&lt;/p&gt;
&lt;p&gt;Below is the Stan code that specifies this Bayesian variant of LASSO regression. Like for the Bayesian ridge regression above, this code is loosely based on &lt;a href=&#34;https://osf.io/cg8fq/&#34;&gt;Erp, Oberski, &amp;amp; Mulder (2019)&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;stan&#34;&gt;&lt;code&gt;data{
    int N_train;             // &amp;quot;# training observations&amp;quot;
    int N_test;              // &amp;quot;# test observations&amp;quot;
    int N_pred;              // &amp;quot;# predictor variables&amp;quot;
    vector[N_train] y_train; // &amp;quot;training outcomes&amp;quot;
    matrix[N_train, N_pred] X_train; // &amp;quot;training data&amp;quot;
    matrix[N_test, N_pred] X_test;   // &amp;quot;testing data&amp;quot;
}
parameters{
    real&amp;lt;lower=0&amp;gt; sigma;   // &amp;quot;error SD&amp;quot;
    real&amp;lt;lower=0&amp;gt; sigma_B; // &amp;quot;(hierarchical) SD across betas&amp;quot;
    vector[N_pred] beta;   // &amp;quot;regression beta weights&amp;quot;
}
model{
  // &amp;quot;group-level (hierarchical) SD across betas&amp;quot;
  sigma_B ~ cauchy(0, 1);
  
  // &amp;quot;Prior on SD&amp;quot;
  sigma ~ normal(0, 1);
  
  // &amp;quot;beta prior (Note this is the only change!)&amp;quot;
  beta ~ double_exponential(0, sigma_B); 
    
  // &amp;quot;model likelihood&amp;quot;
    y_train ~ normal(X_train*beta, sigma);
}
generated quantities{ 
    real y_test[N_test]; // &amp;quot;test data predictions&amp;quot;
    for(i in 1:N_test){
        y_test[i] = normal_rng(X_test[i,] * beta, sigma);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Time to fit the model and visualize results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit the model using Stan&amp;#39;s NUTS HMC sampler
fit_bayes_lasso &amp;lt;- sampling(bayes_lasso, stan_dat, iter = 2000, 
                            warmup = 500, chains = 3, cores = 3)

# Extract posterior distribution (parameters and predictions)
post_lasso &amp;lt;- rstan::extract(fit_bayes_lasso)

# Compute mean of the posterior predictive distribution over test set predictors,
# which integrates out uncertainty in parameter estimates
y_pred_bayes_lasso &amp;lt;- apply(post_lasso$y_test, 2, mean)

# Plot correlation between posterior predicted mean and actual Pr(Acceptance)
qplot(x = y_pred_bayes_lasso, y = test_scale[,8],
      main = paste0(&amp;quot;r = &amp;quot;, round(cor(test_scale[,8], y_pred_bayes_lasso), 2))) +
    xlab(&amp;quot;Model Predicted Pr(Acceptance)&amp;quot;) +
    ylab(&amp;quot;Actual Pr(Acceptance)&amp;quot;) +
    theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig10-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Like before, the Bayesian LASSO is producing very similar results compared to the frequentist version.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-the-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing the Models&lt;/h2&gt;
&lt;p&gt;So far, we have described and fit both the frequentist and Bayesian versions of ridge and LASSO regression to our training data, and we have shown that we can make pretty outstanding predictions on our held-out test set! However, we have not explored the parameters that each model has estimated. Here, we will begin to probe our models.&lt;/p&gt;
&lt;p&gt;First off, let’s look at the ridge regression model parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract beta weights from traditional regression model
betas_trad &amp;lt;- fit_lr$coefficients[-1]

# Extract optimal ridge beta weights from CV
betas_ridge &amp;lt;- fit_ridge$beta[,which(cv_ridge$lambda==cv_ridge$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p1 &amp;lt;- mcmc_areas(as.array(fit_bayes_ridge), pars = paste0(&amp;quot;beta[&amp;quot;, 1:7, &amp;quot;]&amp;quot;), 
           prob = 0.8, prob_outer = 0.99, point_est = &amp;quot;mean&amp;quot;) +
  geom_point(x = rep(betas_ridge, 1024), y = rep(7:1, 1024), color = I(&amp;quot;#c61d29&amp;quot;), size = 2) + # rep() here is an annoying work-around to this code breaking in an update...
  geom_point(x = rep(betas_trad, 1024), y = rep(7:1, 1024), color = I(&amp;quot;black&amp;quot;), size = 2) +
  ggtitle(&amp;quot;Ridge Penalty&amp;quot;) +
  theme_minimal(base_size = 20)

# Extract optimal LASSO beta weights from CV
betas_lasso &amp;lt;- fit_lasso$beta[,which(cv_lasso$lambda==cv_lasso$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p2 &amp;lt;- mcmc_areas(as.array(fit_bayes_lasso), pars = paste0(&amp;quot;beta[&amp;quot;, 1:7, &amp;quot;]&amp;quot;), 
           prob = 0.8, prob_outer = 0.99) +
  geom_point(x = rep(betas_lasso, 1024), y = rep(7:1, 1024), color = I(&amp;quot;#c61d29&amp;quot;), size = 2) +
  geom_point(x = rep(betas_trad, 1024), y = rep(7:1, 1024), color = I(&amp;quot;black&amp;quot;), size = 2) +
  ggtitle(&amp;quot;LASSO Penalty&amp;quot;) +
  theme_minimal(base_size = 20)

# Plot in grid
cowplot::plot_grid(p1, p2, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig11-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Above, the disributions reflect the Bayesian posteriors for each &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weight (shading represents the 80% central interval, based on quantiles; bar representing the posterior mean), the red points indicate the regularized frequentist point estimates, and black points are estimates from the trditional, non-regularized frequentist regression model (shown on each plot for comparison purposes). Importantly, the traditional regression estimates are far from 0 in many cases, whereas both frequentist and Bayesian regularization estimates are much closer to 0. &lt;strong&gt;It is this conservatism that allows &lt;em&gt;biased&lt;/em&gt; estimates (from regularization/penalization) to outperform traditional models that offer &lt;em&gt;unbiased estimates&lt;/em&gt; when tested on external data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Thus, the benefit of regularization is that we get better parameter estimates (translating to better model performance) in low and/or noisy data settings. Crucially, as we collect more and more data, the effects of regularization become less apparent. We can demonstrate this by fitting all the models above to 400—as opposed to only 20—observations and compare the coefficients to the above plot.&lt;/p&gt;
&lt;p&gt;To get started, we first go through our pre-processing steps once again:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Training data
train_dat2 &amp;lt;- grad_dat[1:400,] # Use 400 observations now

# Testing data
test_dat2 &amp;lt;- grad_dat[401:500,] # Only using last 100 now

# Scale training data (and get rid of ID)
train_scale2 &amp;lt;- scale(train_dat2[,2:9])

# Find means and SDs of training data variables
means2 &amp;lt;- attributes(train_scale2)$`scaled:center`
SDs2 &amp;lt;- attributes(train_scale2)$`scaled:scale`

# Scale test data using training data summary stats (no cheating!)
test_scale2 &amp;lt;- scale(test_dat2[,-1], center = means2, scale = SDs2)

# Prepare data for Stan
stan_dat2 &amp;lt;- list(N_train = nrow(train_scale2),
                  N_test  = nrow(test_scale2),
                  N_pred  = ncol(train_scale2)-1,
                  y_train = train_scale2[,8],
                  X_train = train_scale2[,-8],
                  X_test  = test_scale2[,-8])&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we re-fit all the models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit linear regression
fit_lr2 &amp;lt;- lm(Chance.of.Admit ~ ., data = as.data.frame(train_scale2))

# Fit and cross-validate ridge
fit_ridge2 &amp;lt;- glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                     alpha = 0, lambda = lambdas, intercept = F)
cv_ridge2 &amp;lt;- cv.glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                       alpha = 0, lambda = lambdas, intercept = F)

# Fit the model using Stan&amp;#39;s NUTS HMC sampler
fit_bayes_ridge2 &amp;lt;- sampling(bayes_ridge, stan_dat2, iter = 2000, 
                             warmup = 500, chains = 3, cores = 3)
# Fit and cross-validate LASSO
fit_lasso2 &amp;lt;- glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                     alpha = 1, lambda = lambdas, intercept = F)
cv_lasso2 &amp;lt;- cv.glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                       alpha = 1, lambda = lambdas, intercept = F)
# Fit the model using Stan&amp;#39;s NUTS HMC sampler
fit_bayes_lasso2 &amp;lt;- sampling(bayes_lasso, stan_dat2, iter = 2000, 
                             warmup = 500, chains = 3, cores = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And time to plot!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Extract beta weights from traditional regression model
betas_trad2 &amp;lt;- fit_lr2$coefficients[-1]

# Extract optimal ridge beta weights from CV
betas_ridge2 &amp;lt;- fit_ridge2$beta[,which(cv_ridge2$lambda==cv_ridge2$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p3 &amp;lt;- mcmc_areas(as.array(fit_bayes_ridge2), pars = paste0(&amp;quot;beta[&amp;quot;, 1:7, &amp;quot;]&amp;quot;), 
           prob = 0.8, prob_outer = 0.99, point_est = &amp;quot;mean&amp;quot;) +
  geom_point(x = rep(betas_ridge2, 1024), y = rep(7:1, 1024), color = I(&amp;quot;#c61d29&amp;quot;), size = 2) +
  geom_point(x = rep(betas_trad2, 1024), y = rep(7:1, 1024), color = I(&amp;quot;black&amp;quot;), size = 2, alpha = .6) +
  ggtitle(&amp;quot;Ridge Penalty&amp;quot;) +
  theme_minimal(base_size = 20)

# Extract optimal LASSO beta weights from CV
betas_lasso2 &amp;lt;- fit_lasso2$beta[,which(cv_lasso2$lambda==cv_lasso2$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p4 &amp;lt;- mcmc_areas(as.array(fit_bayes_lasso2), pars = paste0(&amp;quot;beta[&amp;quot;, 1:7, &amp;quot;]&amp;quot;), 
           prob = 0.8, prob_outer = 0.99) +
  geom_point(x = rep(betas_lasso2, 1024), y = rep(7:1, 1024), color = I(&amp;quot;#c61d29&amp;quot;), size = 2) +
  geom_point(x = rep(betas_trad2, 1024), y = rep(7:1, 1024), color = I(&amp;quot;black&amp;quot;), size = 2, alpha = .6) +
  ggtitle(&amp;quot;LASSO Penalty&amp;quot;) +
  theme_minimal(base_size = 20)

cowplot::plot_grid(p3, p4, ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig12-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Woah! Unlike before, all of our estimates pretty much fall right on top of each other. Because this dataset has both: (1) many observations, and (2) a relatively high signal-to-noise ratio, both traditional and regularized regression methods offer the same conclusion when we make use the full training dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Discussion&lt;/h1&gt;
&lt;p&gt;In this post, we learned about the benefits of using regularized/penalized regression models over traditional regression. We determined that in low and/or noisy data settings, the so-called &lt;em&gt;unbiased&lt;/em&gt; estimates given by traditional regression modeling actually lead to worse-off model performance. Importantly, we learned that this occurs because being ubiased allows a model to learn a lot from the data, including learning patterns of noise. Then, we learned that &lt;em&gt;biased&lt;/em&gt; methods such as ridge and LASSO regression restrict the amount of learning that we get from data, which leads to better estimates in low and/or noisy data settings.&lt;/p&gt;
&lt;p&gt;Finally, we saw that hierarchical Bayesian models actually contain frequentist ridge and LASSO regression as a special case—namely, we can choose a prior distribution across the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; weights that gives us a solution that is equivalent to that of the frequentist ridge or LASSO methods! Not only that, but Bayesian regression gives us a full posterior distribution for each parameter, thus circumventing problems with frequentist regularization that require the use of bootstrapping to estimate confidence intervals.&lt;/p&gt;
&lt;div id=&#34;so-what&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;So What?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Psychology&lt;/strong&gt;—and many other areas of social and behavoiral science—&lt;strong&gt;is in the midst of a replication crisis&lt;/strong&gt;. While many factors are at play here, one problem is that the methods we use to draw inference from our data are too liberal. Taken with the high measurement error, low sample sizes, and lack of mechanistic models in our research areas, we are bound to overestimate the &lt;a href=&#34;https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/&#34;&gt;magnitude (and even miss the sign&lt;/a&gt;) of effects that we are interested in if we continue to use methods that are prone to overfitting. &lt;strong&gt;&lt;em&gt;Regularization provides an elegant solution to this problem of “learning too much” from our data&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Moving forward, I hope that you consider regularizing your models! (and if you are feeling really fancy, go Bayes! Embrace the uncertainty!)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Delay Discounting of Protected Sex: Relationship Type and Sexual Orientation Influence Sexual Risk Behavior</title>
      <link>http://haines-lab.com/publication/hahn_2019/</link>
      <pubDate>Tue, 02 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/hahn_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Functionalist and Constructionist Perspectives on Emo­ tion Dysregulation</title>
      <link>http://haines-lab.com/publication/beauchaine_haines_2019/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/beauchaine_haines_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using automated computer vision and machine learning to code facial expressions of affect and arousal: Implications for emotion dysregulation research</title>
      <link>http://haines-lab.com/publication/haines_dev_psych_2019/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/haines_dev_psych_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>http://haines-lab.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using computer-vision and machine learning to automate facial coding of positive and negative affect intensity</title>
      <link>http://haines-lab.com/publication/haines_plos_one_2019/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/haines_plos_one_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Outcome‐Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task</title>
      <link>http://haines-lab.com/publication/haines_cog_sci_2018/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/haines_cog_sci_2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (3)</title>
      <link>http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;goals-of-paramter-estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Goals of Paramter Estimation&lt;/h1&gt;
&lt;p&gt;When estimating paramters for a given model, we typically aim to make an inference on an individual’s underlying decision process. We may be inferring a variety of different factors, such as the rate at which someone updates their expectations, the way that someone subjectively values an outcome, or the amount of exploration versus exploitation that someone engages in. Once we estimate an individual’s parameters, we can compare then to other people or even other groups of people. Further, we can compare parameters within subjects after an experimental manipulation (e.g., &lt;em&gt;does drug X affect a person’s learning rate?&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Below, we will explore multiple paremter estimation methods. Specifically, we will use: (1) maximum likelihood estimation, (2) maximum a posteriori estimation, (3) and fully Bayesian estimation. First, we will simulate data from models described in the previous post on a simple 2-armed bandit task. Importantly, we will simulate data using &lt;em&gt;known parameter values&lt;/em&gt;, which we will then try to recover from the simulated data. We will refer to the known paramters as the &lt;em&gt;true parameters&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Simulation&lt;/h1&gt;
&lt;p&gt;For our simulation, we will simulate choice from a model using delta-rule learning and softmax choice. To keep things simple, the learning rate will be the only free paramter in the model. Additionally, we will simulate choices in a task where there are two choices, where choice 1 has a mean payoff of 1 and choice 2 has a mean payoff of -1. Therefore, a learning agent should be able to learn that choice 1 is optimal and make selections accordingly. However, we will add noise to each choice payoff (&lt;em&gt;sigma&lt;/em&gt; below) to make things more realistic.&lt;/p&gt;
&lt;p&gt;The following R code simulates 200 trials using the model and task described above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For pretty plots and data manipulation
library(ggplot2)
library(foreach)
library(dplyr)

# Simulation paramters
set.seed(1)        # Random seed for replication
mu    &amp;lt;- c(1, -1)  # Mean payoff for choices 1 and 2 
sigma &amp;lt;- 3         # SD of payoff distributions
n_tr  &amp;lt;- 200       # Number of trials 
beta  &amp;lt;- 0.1       # True learning rate

# Initial expected value
ev &amp;lt;- c(0, 0) 

# Softmax choice function
logsumexp &amp;lt;- function (x) {
  y &amp;lt;- max(x)
  y + log(sum(exp(x - y)))
}
softmax &amp;lt;- function (x) {
  exp(x - logsumexp(x))
}

# Simulate data
sim_dat &amp;lt;- foreach(t=1:n_tr, .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Generate choice probability with softmax
  pr &amp;lt;- softmax(ev)
  
  # Use choice probability to sample choice
  choice &amp;lt;- sample(c(1,2), size = 1, prob = pr)
  
  # Generate outcome based on choice
  outcome &amp;lt;- rnorm(1, mean = mu[choice], sd = sigma)
  
  # Delta-rule learning
  ev[choice] &amp;lt;- ev[choice] + beta * (outcome - ev[choice])
  
  # Save data
  data.frame(Trial   = rep(t, 2),
             EV      = ev,
             Pr      = pr,
             Option  = paste(1:2),
             Choice  = rep(choice, 2),
             Outcome = rep(outcome, 2))
}

# Change in expected values across tirals
ggplot(sim_dat, aes(x = Trial, y = EV, geom = &amp;quot;line&amp;quot;, color = Option)) +
  geom_line() +
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) +
  ylab(&amp;quot;Expected Value&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The above graph shows the simulated agent’s expected value (&lt;em&gt;EV&lt;/em&gt;) for options 1 and 2 across the 200 trials. We can also view the probability of selecting each option across trials:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Change in probability of selecting each option across tirals
ggplot(sim_dat, aes(x = Trial, y = Pr, geom = &amp;quot;line&amp;quot;, color = Option)) +
  geom_line() +
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) +
  ylab(&amp;quot;Pr(Choice)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig2-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the agent learns to prefer option 1 over option 2 across trials. Although we know the true learning rate (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true} = 0.1\)&lt;/span&gt;), we will explore the various parameter estimation techniques below to try and recover &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true}\)&lt;/span&gt; from the simulated data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimation-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Parameter Estimation Methods&lt;/h1&gt;
&lt;div id=&#34;maximum-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.1 Maximum Likelihood&lt;/h2&gt;
&lt;p&gt;The goal of maximum likelihood estimation (MLE) is to identify the single, most likely parameter value(s) that could have produced the observed data. For our purposes, MLE will allow us to estimate the learning rate that maximizes the probability of observing the simulated data. We refer to his estimate as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; (pronounced beta-hat).&lt;/p&gt;
&lt;p&gt;Before moving on, it is worth introducing some new notation. If we refer to the observed data as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the parameters we aim to estimate as &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can refer to the likelihood function as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(X|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our case, &lt;span class=&#34;math inline&#34;&gt;\(\theta = \hat{\beta}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the vector of simulated choices from above. So then, how do we actually compute the probability of choices &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given learning rate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;? Simple! We use the model that we simulated the data from. Specifically, we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Make a guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Look into &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and find out what choice and outcome the agent made/experienced on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Use our guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; to update the EV for the chosen option accoring to the model&lt;/li&gt;
&lt;li&gt;Enter the updated EVs into the softmax function to generate the probability of selecting each option for the next trial&lt;/li&gt;
&lt;li&gt;Store the model-estimated probability of selecting the choice that the agent actually made on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We iterate through these steps for each trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and then multiply the probabilities across all trials. In practice, we take the natural log of the probability on each trial and then sum across trials, which is equivalent to multiplying out the probabilities but is more numerically stable (computers don’t like really small numbers!). We can write out this &lt;em&gt;log-likelihood&lt;/em&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{t=1}^{T}{\text{ln } Pr(Choice_{t}|\hat{\beta})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We are not finished yet! Once we compute the above sum for a given guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;, we will run through all the steps again with a new guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;. We continue to make guesses and calculate the above sum until we find a value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; that gives us the maximum probability—this final value is the &lt;em&gt;maximum likelihood estimate (MLE)&lt;/em&gt;, written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\beta}_{MLE} = \underset{\hat{\beta}}{\text{arg max}}\sum_{t=1}^{T}{\text{ln } Pr(Choice_{t}|\hat{\beta})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to find the value for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; that maximizes the above sum. Now, we could accomplish this by sampling random learning rates between 0 and 1, computing the sum for each value, and then determining which value produces the highest log-likelihood. Alternatively, we could create a grid of values from 0 to 1 (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} \in \text{{0.01, 0.02, ..., 0.99}}\)&lt;/span&gt;) and select the MLE as the value with the highest log-likelihood. In the real world, however, we usually use some sort of optimization algorithm that makes our job much easier. Below, we will use the &lt;strong&gt;optim&lt;/strong&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define the log-likelihood function used for MLE
mle_bandit &amp;lt;- function(X, beta, outcomes)  {
  # Initialize expected value
  ev &amp;lt;- c(0, 0)
  # loop through each trial and compute log-likelihood
  ll &amp;lt;- foreach(t=seq_along(X), .combine = &amp;quot;c&amp;quot;) %do% {
    # Generate choice probability with softmax
    pr &amp;lt;- softmax(ev)
    
    # Delta-rule learning
    ev[X[t]] &amp;lt;- ev[X[t]] + beta * (outcomes[t] - ev[X[t]])
    
    # log probability of &amp;quot;true&amp;quot; simulated choice
    log(pr[X[t]])
  }
  
  # return the summed (minus) log-likelihood, because optim minimizes by default
  sum(-1*ll)
}

# Because data were simulated and output into long format, we need to
# remove every other observation so that we do not double-count
fit_dat &amp;lt;- sim_dat %&amp;gt;%
  filter(Option == 1)

# Use optim to minimize the (minus) log-likelihood function
mle_results &amp;lt;- optim(par      = 0.5,             # Initial guess for beta
                     fn       = mle_bandit,      # Function we are minimizing
                     method   = &amp;quot;L-BFGS-B&amp;quot;,      # Specific algorithm used
                     lower    = 0,               # Lower bound for beta 
                     upper    = 1,               # Upper bound for beta
                     X        = fit_dat$Choice,  # Simulated choices
                     outcomes = fit_dat$Outcome) # Simulated choice outcomes

# Print results
cat(&amp;quot;The MLE for beta is: &amp;quot; , round(mle_results$par, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The MLE for beta is:  0.09&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad! &lt;strong&gt;optim&lt;/strong&gt; returns a MLE of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(0.09\)&lt;/span&gt;. Given that &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true} = 0.1\)&lt;/span&gt; (since we determined the value for the simulations), our estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is not that far off. One potential downside of the MLE approach we used above is that we only receive a single value for the MLE, which makes it difficult to know how certain the estimate is. For example, our simulation was for 200 trials, but surely we would be more confident in the estimate if the simulation was for 1,000’s of trials? MLE alone does not offer an explicit measure of uncertainty in the parameter estimates without additional analyses (and additional assumptions), which is one reason that Bayesian methods are easier to interpret.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-a-poseriori-map-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.2 Maximum A Poseriori (MAP) Estimation&lt;/h2&gt;
&lt;p&gt;MAP estimation is a straightforward extention of MLE, which allows us to incorporate prior information about the parameter that we are trying to estimate into the estimation procedure. In our example, we may know from prior studies that learning rates for a given task typically fall in the range of 0.01-0.4. MAP estimation allows us to formalize this prior research in a very simple way! We simply parameterize a prior distribution (&lt;span class=&#34;math inline&#34;&gt;\(Pr(\beta)\)&lt;/span&gt;) that is consistent with estimates from prior studies. For example, a normal distribution with a &lt;span class=&#34;math inline&#34;&gt;\(\mu = .15\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.25\)&lt;/span&gt; captures the above range nicely but does not constrain the values too much:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0, 1, length=1000)
y &amp;lt;- dnorm(x, mean = .15, sd = .25)
qplot(x = x, y = y, geom = &amp;quot;line&amp;quot;, xlab = expression(beta[prior]), ylab = &amp;quot;Density&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig4-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;So, how do we include this prior information? Easy! When we make a guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;, we will compute the likelihood just like we did for MLE, and we will multiple this value by the “likelihood” of the prior. Intuitively, this allows us to &lt;em&gt;weight&lt;/em&gt; the likelihood of each possible value for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; by the prior for that same value of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;. This behavior results in a sort of trade off between the likelihood and prior distribution, which ends up &lt;strong&gt;regularizing&lt;/strong&gt; our MLE estimate by pulling it toward the center mass of the prior distribution. Formally, we represent this by adding the prior distribution (bolded) to the MLE function from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\beta}_{MAP} = \underset{\hat{\beta}}{\text{arg max}}\sum_{t=1}^{T}{\text{ln } Pr(Choice_{t}|\hat{\beta})\bf{Pr(\beta)}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s see what this looks like in R, and note how it affects estimation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define the log-likelihood function used for MAP
map_bandit &amp;lt;- function(X, beta, outcomes)  {
  # Initialize expected value
  ev &amp;lt;- c(0, 0)
  # loop through each trial and compute log-likelihood
  ll &amp;lt;- foreach(t=seq_along(X), .combine = &amp;quot;c&amp;quot;) %do% {
    # Generate choice probability with softmax
    pr &amp;lt;- softmax(ev)
    
    # Delta-rule learning
    ev[X[t]] &amp;lt;- ev[X[t]] + beta * (outcomes[t] - ev[X[t]])
    
    # Probability/likelihood of &amp;quot;true&amp;quot; simulated choice
    like &amp;lt;- pr[X[t]]
    
    # Likelihood of current beta according to prior distribution
    prior &amp;lt;- dnorm(x = beta, mean = .15, sd = 0.25)
    
    # Log of like*prior
    log(like*prior)
  }
  
  # return the summed (minus) log-likelihood with prior information included
  sum(-1*ll)
}

# Use optim to minimize the (minus) log-likelihood function
map_results &amp;lt;- optim(par      = 0.5,             # Initial guess for beta
                     fn       = map_bandit,      # Function we are minimizing
                     method   = &amp;quot;L-BFGS-B&amp;quot;,      # Specific algorithm used
                     lower    = 0,               # Lower bound for beta 
                     upper    = 1,               # Upper bound for beta
                     X        = fit_dat$Choice,  # Simulated choices
                     outcomes = fit_dat$Outcome) # Simulated choice outcomes

# Print results
cat(&amp;quot;The MAP for beta is: &amp;quot; , round(map_results$par, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The MAP for beta is:  0.136&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Woah! The simple addition of prior information pushed our estimate of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(0.09\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(0.136\)&lt;/span&gt;. Notice that the MAP estimator was pulled toward the mean of the prior distribution. However, is this a good thing? When is this behavior beneficial? After all, our MAP estimate is actually further than our MLE estimate from &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To demonstrate the benefit of prior information, let’s take the simulated data from our learner (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.1\)&lt;/span&gt;), but only fit the model using the first 15 trials worth of data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use MLE to fit the first 15 trials
mle_results_15tr &amp;lt;- optim(par      = 0.5,             
                          fn       = mle_bandit,      
                          method   = &amp;quot;L-BFGS-B&amp;quot;,      
                          lower    = 0,               
                          upper    = 1,               
                          X        = fit_dat$Choice[1:15],  # Only using first 15 trials
                          outcomes = fit_dat$Outcome[1:15]) 

# Use MAP to fit the first 15 trials
map_results_15tr &amp;lt;- optim(par      = 0.5,             
                          fn       = map_bandit,      
                          method   = &amp;quot;L-BFGS-B&amp;quot;,      
                          lower    = 0,               
                          upper    = 1,               
                          X        = fit_dat$Choice[1:15],  # Only using first 15 trials
                          outcomes = fit_dat$Outcome[1:15]) 

cat(&amp;quot;The MLE for beta with 15 trials is: &amp;quot; , round(mle_results_15tr$par, 3), &amp;quot;\n&amp;quot;, 
    &amp;quot;The MAP for beta with 15 trials is: &amp;quot; , round(map_results_15tr$par, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The MLE for beta with 15 trials is:  0.234 
##  The MAP for beta with 15 trials is:  0.169&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at that! MLE overestimates the learning rate, and MAP gives us a much better estimate. There are two main take-aways from this toy example:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;MLE is prone to degenerate behavior in low data settings, which can push parameters to the edge of the parameter space or lead to otherwise unreliable estimates, and&lt;/li&gt;
&lt;li&gt;Introduction of our own, theory-based form of bias (i.e. regularization from the prior distribution) can help us avoid estimation problems—espectially in low data settings! (this will become clearer in future posts)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In fact, you may have realized through the above example (or math) that MLE is just a sepcial case of MAP estimation! If it is not already intuitive, think of this—what would happen to our MAP estimate if we assumed that the prior distribution was uniform (i.e. all values between 0 and 1 are equally likely for learning rate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;)? Well, we would have to multiply &lt;span class=&#34;math inline&#34;&gt;\(Pr(Choice_{t}|\hat{\beta})\)&lt;/span&gt; by 1 for each guess of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;! See this yourself by observing the likelihood of different values for &lt;code&gt;x&lt;/code&gt; drawn from a uniform distribution (code: &lt;code&gt;dunif(x = .15, min = 0, max = 1)&lt;/code&gt;). Therefore, MAP is analytically equivalent to MLE when we assume a uniform prior distibution. Of course, in many settings, we know that certain paremter values are very unlikely (e.g., a learning rate of .2 is more reasonable than of .99 in most settings). It follows that assuming a uniform distribution for the prior can be quite (mis)informative!&lt;/p&gt;
&lt;p&gt;Note that MAP, like MLE, only offers a point estimate. Again, we would ideally like a proper representation of uncertainty for our estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;markov-chain-monte-carlo-mcmc-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.3 Markov Chain Monte Carlo (MCMC) Estimation&lt;/h2&gt;
&lt;p&gt;We have finally arrived… MCMC builds on all of the above estimation methods, resulting in a powerful estimation procedure that gives as an entire distribution—rather than just a point estimate—to represent a parameter.&lt;/p&gt;
&lt;p&gt;To begin, we will first introduce Bayes’ Theorem; you have likely seen is before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(\theta | X) = \frac{Pr(X | \theta)Pr(\theta)}{Pr(X)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In English, this translates to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Posterior Distribution} = \frac{\text{Likelihood} \cdot \text{Prior Distribution}}{\text{Marginal Distribution}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may notice that the numerator (i.e. &lt;span class=&#34;math inline&#34;&gt;\(Pr(X | \theta)Pr(\theta)\)&lt;/span&gt;) looks suspiciously like the term we were trying to maximize for MAP estimation (&lt;span class=&#34;math inline&#34;&gt;\(Pr(Choice_{t}|\hat{\beta})Pr(\beta)\)&lt;/span&gt;)…which is because MAP estimation is indeed derived from Bayes’ Theorem! In fact, the MAP estimate is the &lt;em&gt;mode of the posterior distribution&lt;/em&gt;, which explains why it is called &lt;strong&gt;maximum a posteriori&lt;/strong&gt; estimation.&lt;/p&gt;
&lt;p&gt;So then, we already know what the numerator corresponds to, but what of the denominator? Referring back to our simulation, the marginal distribution &lt;span class=&#34;math inline&#34;&gt;\(Pr(X)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Pr(Choices)\)&lt;/span&gt;, which is interpreted as the probability of the observed data—what exactly does this mean? Well, it turns out that for our purposes, it is not too important! &lt;span class=&#34;math inline&#34;&gt;\(Pr(Choices)\)&lt;/span&gt; is a constant term, and it does not depend on the model we are trying to estimate parameters for. Therefore, we often write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(\theta | X) \propto Pr(X | \theta)Pr(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which translates to “&lt;em&gt;the posterior distribution is &lt;strong&gt;proportional to&lt;/strong&gt; the likelihood times the prior distribution&lt;/em&gt;”. Intuitively, this means that it is the relative differences in &lt;span class=&#34;math inline&#34;&gt;\(Pr(X | \theta)Pr(\theta)\)&lt;/span&gt; across different values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that give us information about the posterior distribution. Importantly, we already know how to work with this numerator term (from doing MAP estimation)! Therefore, fully Bayesian estimation using MCMC only requires a small extention. Specifically, instead of using optimization (i.e. R’s &lt;code&gt;optim&lt;/code&gt; function) to
find a single value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that maximizes &lt;span class=&#34;math inline&#34;&gt;\(Pr(X | \theta)Pr(\theta)\)&lt;/span&gt;, we want to use a method that tells us how likely all possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are relative to each other (i.e. a distribution!). While there are many different algorithms that can be used to accomplish this, we will start with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&#34;&gt;Metropolis&lt;/a&gt; algorithm. Referring back to our learning data, estimating &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; using the Metropolis algorithm proceeds with the steps outlined below.&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(n = 1, 2, ..., N:\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Propose a value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}&amp;#39;\)&lt;/span&gt; that is near your current guess &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Calculate the &lt;em&gt;acceptance ratio&lt;/em&gt;, defined by &lt;span class=&#34;math inline&#34;&gt;\(accept = \frac{Pr(Choices | \hat{\beta}&amp;#39;)Pr(\hat{\beta}&amp;#39;)}{Pr(Choices | \hat{\beta}_{n})Pr(\hat{\beta}_{n})}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Generate a uniform random number &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(u \le accept\)&lt;/span&gt;, set &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n+1} = \hat{\beta}&amp;#39;\)&lt;/span&gt;, otherwise set &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n+1} = \hat{\beta}_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Importantly, while iterating through all samples &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we store each value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n}\)&lt;/span&gt;. This sequence of values &lt;em&gt;is the posterior distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(Pr(\hat{\beta}|Choices)\)&lt;/span&gt;. The R code below shows the Metropolis algorithm in action, and the resulting histogram of posterior samples:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that this takes a few minutes to run because it is not optimized in the least bit!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set number of samples N for the Metropolis algorithm
samples &amp;lt;- 5000

# Set initial guess for beta
beta_n &amp;lt;- 0.5

# Take what we did above for MAP estimation and make into a function
calc_like &amp;lt;- function(beta, X, outcomes) {
  # Initialize expected value
  ev &amp;lt;- c(0, 0)
  # loop through each trial and compute log-likelihood
  ll &amp;lt;- foreach(t=seq_along(X), .combine = &amp;quot;c&amp;quot;) %do% {
    # Generate choice probability with softmax
    pr &amp;lt;- softmax(ev)
    
    # Delta-rule learning
    ev[X[t]] &amp;lt;- ev[X[t]] + beta * (outcomes[t] - ev[X[t]])
    
    # Probability/likelihood of &amp;quot;true&amp;quot; simulated choice
    like &amp;lt;- pr[X[t]]
    
    # Likelihood of current beta according to prior distribution
    prior &amp;lt;- dnorm(x = beta, mean = .15, sd = 0.25)
    
    # log of like*prior
    log(like*prior)
  }
  
  # return the summed log-likelihood with prior information included
  sum(ll)
}

# Iterate through N samples and store each result
posterior &amp;lt;- foreach(n=1:samples, .combine = &amp;quot;c&amp;quot;) %do% {
  # Step 1: Generate random proposal value with normal distribution
  beta_proposal &amp;lt;- rnorm(1, mean = beta_n, sd = .01)
  
  # If proposal is outside of parameter bounds, keep current sample, else continue
  if (0 &amp;lt; beta_proposal &amp;amp; beta_proposal &amp;lt; 1) {
    # Step 2: Calculate acceptance ratio
    like_proposal &amp;lt;- exp(calc_like(beta_proposal, fit_dat$Choice, fit_dat$Outcome))
    like_current  &amp;lt;- exp(calc_like(beta_n, fit_dat$Choice, fit_dat$Outcome))
    accept &amp;lt;- like_proposal/like_current
    
    # Step 3: Generate uniform random number on [0,1]
    u &amp;lt;- runif(1, min = 0, max = 1)
    
    # Step 4: Accept or reject proposal
    if (u &amp;lt;= accept) {
      beta_n &amp;lt;- beta_proposal
    }
  }
  
  # Retern beta_n (either updated with proposal or remains the same)
  beta_n
}

# Plot time-series of posterior samples
qplot(x = 1:samples, y = posterior, geom = &amp;quot;line&amp;quot;) + 
  geom_hline(aes(yintercept= beta, linetype = &amp;quot;True Beta&amp;quot;), color= &amp;#39;red&amp;#39;) +
  scale_linetype_manual(name = &amp;quot;&amp;quot;, values = 2) +
  xlab(&amp;quot;Posterior Sample&amp;quot;) +
  ylab(expression(hat(beta))) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig7-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;This &lt;em&gt;traceplot&lt;/em&gt; shows each accepted proposal across all 5,000 samples that we drew from the posterior distribution. As you can see, the samples are all distributed near the true value once they converge to a stable estimate. The samples at the beginning (before reaching convergence) will be discarded (termed &lt;em&gt;burn-in&lt;/em&gt; samples) for further analyses because they do not represent the posterior distribution. Unlike for MLE or MAP estimation, all the posterior samples after burn-in can be used to represent the uncertainty in the learning rate parameter! We simply plot the density of the samples (with the prior density shown in the black, dotted line for an idea of what we have learned):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(posterior[200:5000], geom = &amp;quot;density&amp;quot;, fill = I(&amp;quot;gray&amp;quot;)) +
  geom_line(aes(x = x, y = y), linetype = 2) +
  geom_vline(aes(xintercept= beta, linetype = &amp;quot;True Beta&amp;quot;), color= &amp;#39;red&amp;#39;) +
  scale_linetype_manual(name = &amp;quot;&amp;quot;, values = 2) +
  coord_cartesian(xlim = c(0, 1)) +
  xlab(expression(hat(beta))) +
  ylab(&amp;quot;Density&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig8-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the results are not perfect. Yet, our posterior distribution does contain the true value, and it is rather precise (i.e. it is narrow) relative to the prior distribution (black line) and parameter space.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Wrapping up&lt;/h1&gt;
&lt;p&gt;In this post, we covered three methods of parameter estimation including: (1) maximum likelihood estimation, (2) maximum a posteriori estimation, and (3) markov chain monte carlo estimation. In the future, we will use software packages (particularly &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;) to do MCMC for us, which will allow us to more rapidly estimate parameters compared to our simplistic MCMC implementation above. In the next post, we will use Stan to estimate the same learning rate from the model above. Soon after, we will work towards fitting hierarchical models!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Easyml: Easily Build And Evaluate Machine Learning Models</title>
      <link>http://haines-lab.com/publication/ahn_2017/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/ahn_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Indirect Effect of Emotion Regulation on Minority Stress and Problematic Substance Use in Lesbian, Gay, and Bisexual Individuals</title>
      <link>http://haines-lab.com/publication/rogers_2017/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/rogers_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (2)</title>
      <link>http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/</link>
      <pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;answer-to-post-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Answer to post 1&lt;/h1&gt;
&lt;p&gt;In the previous &lt;a href=&#34;http://haines-lab.com/2017/04/04/human-choice-and-reinforcement-learning-1/&#34;&gt;post&lt;/a&gt;, I reviewed the Rescorla-Wagner updating (Delta) rule and its contemporary instantiation. At the end, I asked the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How should you change the learning rate so that the expected win rate is always the average of all past outcomes?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will go over the answer to this question before progressing to the use of the Delta rule in modeling human choice. To begin, refer back to the Delta rule written in the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = (1 - \beta) \cdot Pr(win)_{t} + \beta \cdot \lambda_{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, we see that in the Delta rule the expected win probability for the next trial is equal to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average&#34;&gt;&lt;em&gt;exponentially weighted moving average&lt;/em&gt;&lt;/a&gt; of the past expectation and the current outcome. It is easy to show this through a visualization of the expectation over time. For example, imagine that we have a vector of outcomes &lt;span class=&#34;math inline&#34;&gt;\(\lambda = [1,0,0,1,1,1,0,1,1,1]\)&lt;/span&gt;, where 0 and 1 represent losing and winning slot machine rolls, respectively. Note that in this example, the placement of these outcomes within the vector &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; indicates their temporal order (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{2}=0\)&lt;/span&gt;, etc.). Now, if we set an arbitrary learning rate such as &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.05\)&lt;/span&gt;, what is the expected win rate after iterating through outcomes &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;? The R code below demonstrates the use of the Delta rule and an alternative exponential weighting scheme–which takes the form of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_series#Examples&#34;&gt;&lt;em&gt;power series&lt;/em&gt;&lt;/a&gt;–to determine the expectation on each trial:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For pretty plots
library(ggplot2)

# Assign lambda (lambda[1] == first trial)
lambda &amp;lt;- c(1,0,0,1,1,1,0,1,1,1)

# Set learning rate
beta &amp;lt;- 0.05

### Iterative prediction error (Delta rule) approach ###

# Function that iterates the Rescorla-Wagner rule 
  # This function is slightly modified from the last post
  # to ensure that that final expectation is stored
rw_update &amp;lt;- function(lambda, beta, init) {
  # To store expected value for each trial
  Pr_win &amp;lt;- vector(length=length(lambda)+1)
  # Set initial value
  Pr_win[1] &amp;lt;- init
  for (t in 1:(length(lambda))) {
    Pr_win[t+1] &amp;lt;- Pr_win[t] + beta * (lambda[t] - Pr_win[t])
  }
  return(Pr_win)
}

# With initial expectation = 0, iterate Delta rule
delta_results &amp;lt;- rw_update(lambda = lambda, 
                           beta   = beta, 
                           init   = 0)[-1]
                          #             ^
                          # Remove initial value (0)

### Power series approach ###

# Direct exponential weighting (saving all expectations)
power_ser &amp;lt;- NULL
for (i in 1:10) {
  power_ser[i] &amp;lt;- beta * sum((1-beta)^(0:(i-1))*lambda[i:1])
}

### Comparison of both approaches ###
all(round(delta_results, 8) == round(power_ser, 8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data.frame for ggplot
all_data &amp;lt;- stack(data.frame(delta = delta_results,
                             pow_s = power_ser))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:10, 2)
names(all_data)[2] &amp;lt;- &amp;quot;Approach&amp;quot;

# Visually
p &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Approach)) + 
  geom_line() +
  facet_grid(facets = &amp;quot;Approach ~ .&amp;quot;) + 
  ggtitle(&amp;quot;Comparison of approaches&amp;quot;) +
  xlab(&amp;quot;Trial Number&amp;quot;) +
  ylab(&amp;quot;Expected Win Probability&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/index.en_files/figure-html/2017-04-07_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the plots, both the Delta rule and the power series approach yield the same exact expectations when iterated for each trial. However, the power series form requires each past observation while the Delta rule only requires the last expectation–this feature makes the Delta rule form of the equation much more plausible as a processes that people may use to estimate the value of a choice. This is because the computational cost does not increase with the number of past observations.&lt;/p&gt;
&lt;p&gt;Through this example, those familiar with &lt;a href=&#34;http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages&#34;&gt;economics&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Signal_processing&#34;&gt;signal processing&lt;/a&gt; may find the Delta rule familiar. Essentially, we can think of the Delta rule as a smoothing function or a &lt;a href=&#34;https://en.wikipedia.org/wiki/High-pass_filter#Algorithmic_implementation&#34;&gt;high- or low-pass filter&lt;/a&gt;–albeit in the time as opposed to frequency domain–which effectively attenuates the effect of past or current outcomes, respectively. What makes this specific form interesting is again the fact that it can be iterated (i.e. it is recursive), making it a realistic approximation to the computations performed by the brain when estimating some value.&lt;/p&gt;
&lt;p&gt;With the above intuitions in mind, we will now get back to the question. How do we change the learning rate to ensure that the current expectation is always the simple average of all past outcomes? Since the above example showed that the Delta rule is really just a moving average where past outcomes are given exponentially decreasing weights, our goal is to make all outcomes equally represented. In other words, we want to weight past and current outcomes equally–this is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average&#34;&gt;&lt;em&gt;cumulative moving average&lt;/em&gt;&lt;/a&gt;. Using our slot machine example, the cumulative moving average formula (in its most common form) is written as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = \frac{\lambda_{t} + (t-1) \cdot Pr(win)_{t}}{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can re-write the above equation into one that is more familiar to us:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = (1-\frac{1}{t}) \cdot Pr(win)_{t} + \frac{1}{t} \cdot \lambda_{t} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above form, you can see that the cumulative moving average can be computed using the Delta rule by setting the learning rate to &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{t}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the trial number. Looking at the equation, you will notice that as &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; increases, the weight &lt;span class=&#34;math inline&#34;&gt;\((1-\frac{1}{t})\)&lt;/span&gt; placed on the past probability estimate &lt;span class=&#34;math inline&#34;&gt;\(Pr(win)_{t}\)&lt;/span&gt; becomes larger while the weight &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{t}\)&lt;/span&gt; on the current outcome &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{t}\)&lt;/span&gt; shrinks. This behavior ensures that past outcomes are not discounted at a higher rate than current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choice-mechanisms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Choice mechanisms&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/shoes_choice.jpeg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;With the Delta rule, we can approximate how people with a certain learning rate may update their expectations about an outcome on a trial-by-trial basis, but how does this translate to choice? In the slot machine example, we only had a single choice (i.e. pull the lever and see if you win), so this question never applied to us. &lt;strong&gt;But what if we have 2 slot machines and we want to select the one that will win most frequently?&lt;/strong&gt; In this case, we can use the Delta rule to update win probability expectations for each slot machine separately, but what do we do with these values after they are computed? Below, I will describe three different methods that can be used to translate expected values to choice.&lt;/p&gt;
&lt;div id=&#34;greedy-choice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 Greedy choice&lt;/h2&gt;
&lt;p&gt;Greedy choice is simple–choose the option with the highest expected value on each trial (i.e. pick the &lt;em&gt;greedy&lt;/em&gt; option):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[c_{t} = \underset{s \in S}{argmax}(V(s_{t}))\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt; indicates the choice made on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V(s_{t})\)&lt;/span&gt; represents the value associated with Slot machine &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Applying this logic to our example, this would equate to choosing the slot machine with the highest expected win probability. While this may sound like a good idea, it is important to remember that we do not know what the true win rate is for either slot machine. Instead, we estimate it after each outcome. With this in mind, a simple example (below) shows why the greedy choice method fails in practical applications.&lt;/p&gt;
&lt;p&gt;Imagine you are choosing between 2 slot machines, where machine A (&lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt;) has a true win rate of 0.9, and machine B (&lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt;) has a true win rate of 0.5. Obviously, &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt; is a better option, but this is something you need to learn by making a choice and updating your expectations. Assuming that you start off thinking that each slot machine has a win rate of 0 (which is typical in human decision making models), your first choice will be a random selection between the equivalent options. In our example, imagine that you randomly choose &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt;, the slot machine spins, and then you win! Great, so now (regardless of your learning rate), you will update your expectation of the win rate for &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt; to a positive, non-zero value. On the next trial, you are greedy, so you again choose &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt;–it has a higher expected win rate than &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt;. In fact, you will continue to choose &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt; on each trial without ever considering &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt;! In this case, even though &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt; it the optimal choice, you never allow yourself to &lt;em&gt;explore&lt;/em&gt; alternative choices. Here, we come across a classical problem in reward-learning paradigms–&lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0095693&#34;&gt;&lt;strong&gt;&lt;em&gt;the exploration-exploitation tradeoff&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;. The crux of this problem is this: how do you know that a “good” choice is better than other choices that you have yet to explore? Think of it like choosing a job. I chose to study psychology, and I continue to exploit this choice (i.e. I am not exploring other education). How do I know that psychology is for me, though? It is possible that I would have gained more from a computer science degree, but I did not explore that option. In the same way, this compromise exists in simple reinforcement learning paradigms such as choosing the best slot machine.
Below, we will explore two methods that address the exploration-exploitation problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;epsilon-greedy-choice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.2 &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-Greedy choice&lt;/h2&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-greedy method is a simple extention of the greedy method above. Instead of always choosing the option with the highest expected value, we sometimes (with probability &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;) randomly choose another option:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[c_{t} = \cases{
          \underset{s \in S}{argmax}(V(s_{t}))  &amp;amp; \text{with } Pr(1 - \epsilon) \cr
          \text{random choice} &amp;amp; \text{with } Pr(\epsilon)
                }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By choosing a random option with &lt;span class=&#34;math inline&#34;&gt;\(Pr(\epsilon)\)&lt;/span&gt;, we can avoid getting stuck choosing the non-optimal slot machine. While this method solves our dilemma, it suffers another problem–when randomly selecting options, it gives equal probabilities to each option. Intuitively, a better method would be to choose options &lt;em&gt;probabilistically with respect to their expected values&lt;/em&gt; (i.e. give high probability to relatively good options and &lt;em&gt;vice-versa&lt;/em&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;softmax-choice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.3 Softmax choice&lt;/h2&gt;
&lt;p&gt;Also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Luce%27s_choice_axiom&#34;&gt;Luce choice rule&lt;/a&gt;, the softmax allows choices to be probabilistic with weights respective to expected value:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(c_{t} = s \in S) = \frac{e^{V_{s}(t)}}{\sum_{s = 1}^S e^{V_{s}(t)}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(e^{V_{s}(t)}\)&lt;/span&gt; is the exponentiated expected value of slot machine &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{s = 1}^S e^{V_{s}(t)}\)&lt;/span&gt; is the summation of the exponentiated expected value of both slot machines (there are 2 in our example). When the expected values are entered, the softmax equation returns a probability of selecting each slot machine which we can then use to make an actual choice. In practice, the softmax function is used most often in decision making research–moving forward, we will use the softmax choice mechanism to model human decision making.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Implementation&lt;/h1&gt;
&lt;p&gt;We now have a full model describing each of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating an outcome,&lt;/li&gt;
&lt;li&gt;Updating previous representations of choice options, and&lt;/li&gt;
&lt;li&gt;Generating a probability of selecting each choice on the next trial.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This model is simple, but it provides the basic building blocks for more complex models that are found in neuroscience, cognitive science, and decision making literature today. In the next post, we will explore various methods which can be used to estimate the &lt;em&gt;free parameters&lt;/em&gt; in the model (e.g. the learning rate) when all we have are the person’s choices.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (1)</title>
      <link>http://haines-lab.com/post/2017-04-04-choice_rl_1/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/post/2017-04-04-choice_rl_1/</guid>
      <description>
&lt;script src=&#34;http://haines-lab.com/post/2017-04-04-choice_rl_1/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;short-history&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Short history&lt;/h1&gt;
&lt;p&gt;In 1972, Robert Rescorla and Allan Wagner developed a formal theory of associative learning, the process through which multiple stimuli are associated with one-another. The most widely used example (Fig. 1) of associative learning comes straight from Psychology 101–Pavlov’s dog.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/pavlov.jpeg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The idea is simple, and it’s something that we experience quite often in everyday life. In the same way that Pavlov’s dog begins to drool after hearing a bell, certain cognitive and/or biological processes are triggered when we are exposed to stimuli that we have been exposed to in the past. But how can this learning process be modeled? That is to say, what sort of &lt;em&gt;equation&lt;/em&gt; can we use to describe how an agent learns to associate multiple stimuli? To answer these questions, Rescorla and Wagner developed what is now know as the Rescorla-Wagner updating rule:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta V_{A} = \alpha_{A} \beta_{1} (\lambda_{1} - V_{AX})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta V_{X} = \alpha_{X} \beta_{1} (\lambda_{1} - V_{AX})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First off, note that the original Rescorla-Wagner rule was developed to explain &lt;em&gt;compound stimuli&lt;/em&gt; (e.g. presentation of a bell and light, followed by food). Here, &lt;span class=&#34;math inline&#34;&gt;\(\Delta V_{A}\)&lt;/span&gt; is the change in associative strength between stimulus &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (e.g. the bell) and the response (e.g. food). &lt;span class=&#34;math inline&#34;&gt;\(\Delta V_{X}\)&lt;/span&gt; has the same interpretation, but refers to stimulus &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (e.g. the light).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0 \leq \beta_{1} \leq 1\)&lt;/span&gt; is a free parameter (i.e. we estimate it from the data) referred to as the &lt;em&gt;learning rate&lt;/em&gt;. The learning rate controls how quickly updating takes place, where values near 0 and 1 reflect sluggish and rapid learning, respectively. Above, the learning rate is shared across stimuli.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0 \leq \alpha_{A} \leq 1\)&lt;/span&gt; is a free parameter which is determined by the salience of stimulus &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(0 \leq \alpha_{X} \leq 1\)&lt;/span&gt; for stimulus &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Unlike the learning rate, which is shared across stimuli, the salience parameter is specific to each stimulus. Put simply, this just means that learning can occur at different rates depending on the type of stimulus (e.g. I may associate a light with food more quickly than a tone).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt; is described as “the asymptote of associative strength”. This is the upper-limit on how strong the association strength can be. In this way, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt; reflects the value being updated toward by the learning rate (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;) and stimulus salience (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Lastly, &lt;span class=&#34;math inline&#34;&gt;\(V_{AX}\)&lt;/span&gt; is the total associative strength of the compound stimulus &lt;span class=&#34;math inline&#34;&gt;\(AX\)&lt;/span&gt;. Rescorla and Wagner assume that this is a simple sum of both stimuli strengths:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{AX} = V_{A} + V_{X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Interpretation of this model is actually quite simple–we update associative strength (&lt;span class=&#34;math inline&#34;&gt;\(V_{*}\)&lt;/span&gt;) for each stimulus by taking steps (&lt;span class=&#34;math inline&#34;&gt;\(\alpha \beta\)&lt;/span&gt;) toward the difference between the asymptote of learning (&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt;) and the current associative strength of the compund stimulus (&lt;span class=&#34;math inline&#34;&gt;\(V_{AX}\)&lt;/span&gt;). By continually exposing an agent to a tone or bell paired with a reward (or punishment), the agent learns the associative strength of the conditioned and unconditioned stimuli.&lt;/p&gt;
&lt;p&gt;While the original Rescorla-Wagner model was successful for explaining associative learning for classical conditioning paradigms, what of operant conditioning? What if we are interested in how people learn to make decisions? In most current research, we are not interested in knowing how people learn to associate lights or tones with some reward. Instead, we would like a model that can describe how people learn to select the &lt;em&gt;best choice&lt;/em&gt; among &lt;em&gt;multiple choices&lt;/em&gt;. This model would need to explain how people assign values to multiple options as well as how they decide which option to choose. In statistical terms, we want to know how people solve the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34;&gt;&lt;em&gt;multi-armed bandit&lt;/em&gt;&lt;/a&gt; problem. In the following section, we will begin to solve this problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;current-implementations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Current Implementations&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://haines-lab.com/media/slots.jpeg&#34; alt=&#34;&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Figure 1&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;As a motivating example, we will explore the simple problem of learning the probability that a slot machine will payoff (i.e. that you will win any amount after pulling the lever). To do so, the above equations only need minor modifications. Additionally, we will change the terminology–instead of learning an associative strength, we will now be learning the &lt;em&gt;probability of a winning outcome&lt;/em&gt;. To start, we take the first equation above and write it for a single stimulus &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, but exchange &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; with the probability of observing a win:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta Pr(win) = \alpha \beta (\lambda - Pr(win))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(\Delta Pr(win) = Pr(win)_{t+1} - Pr(win)_{t}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the current trial), we can re-write the above equation into an iterative form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = Pr(win)_{t} + \alpha \beta (\lambda_{t} - Pr(win)_{t})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since we are using this model to explain how people learn the probability of a binary outcome, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{t} \in [0, 1]\)&lt;/span&gt; now represents the outcome of slot machine roll on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Now, we drop the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; parameter to simplify the model further. Because we have a single choice option, estimating both &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; would lead to an unidentifiable model. This is because as either one increases, the other can decrease and lead to the same exact predictions. Even with multiple choice options, this problem is still apparent. In current applications of the Rescorla-Wagner rule, we do not include a “salience” parameter. Now, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = Pr(win)_{t} + \beta (\lambda_{t} - Pr(win)_{t})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model is now much easier to interpret. The term &lt;span class=&#34;math inline&#34;&gt;\((\lambda_{t} - Pr(win)_{t})\)&lt;/span&gt; can be thought of as the &lt;em&gt;prediction error&lt;/em&gt;–the difference between the actual value &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{t}\)&lt;/span&gt; revealed after the choice was made and the expected value of the choice &lt;span class=&#34;math inline&#34;&gt;\(Pr(win)_{t}\)&lt;/span&gt; for that trial. &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (the learning rate) then updates the current expectation &lt;span class=&#34;math inline&#34;&gt;\(Pr(win)_{t}\)&lt;/span&gt; in the direction of the prediction error &lt;span class=&#34;math inline&#34;&gt;\((\lambda_{t} - Pr(win)_{t})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. R Example&lt;/h1&gt;
&lt;p&gt;To see the Rescorla-Wagner rule in action, let’s generate some fake data using the binomial distribution and try to estimate the rate parameter using various different values for the learning rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For pretty images
library(ggplot2)

# Number of &amp;#39;trials&amp;#39;
num_trials &amp;lt;- 100

# The win rate is 0.7
payoff &amp;lt;- rbinom(n = num_trials, size = 1, prob = 0.7)

# Function that iterates the Rescorla-Wagner rule 
rw_update &amp;lt;- function(lambda, beta, init) {
  # To store expected value for each trial
  Pr_win &amp;lt;- vector(length=length(lambda))
  # Set initial value
  Pr_win[1] &amp;lt;- init
  for (t in 1:(length(lambda)-1)) {
    Pr_win[t+1] &amp;lt;- Pr_win[t] + beta * (lambda[t] - Pr_win[t])
  }
  return(Pr_win)
}

# With initial expectation = 0, try different learning rates
beta_05 &amp;lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_25 &amp;lt;- rw_update(lambda = payoff, beta = 0.25, init = 0)
beta_50 &amp;lt;- rw_update(lambda = payoff, beta = 0.50, init = 0)
beta_75 &amp;lt;- rw_update(lambda = payoff, beta = 0.75, init = 0)
beta_95 &amp;lt;- rw_update(lambda = payoff, beta = 0.95, init = 0)

# Store in data.frame for plotting
all_data &amp;lt;- stack(data.frame(beta_05 = beta_05,
                             beta_25 = beta_25,
                             beta_50 = beta_50,
                             beta_75 = beta_75,
                             beta_95 = beta_95))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:num_trials, 5)
names(all_data)[2]  &amp;lt;- &amp;quot;Beta&amp;quot;

# Plot results
p &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) + 
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&amp;quot;black&amp;quot;, linetype = &amp;quot;longdash&amp;quot;) + 
  ggtitle(&amp;quot;Expected Probability of Winning Outcome&amp;quot;) + 
  xlab(&amp;quot;Trial Number&amp;quot;) + 
  ylab(&amp;quot;Expected Pr(win)&amp;quot;)  
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-04-choice_rl_1/index.en_files/figure-html/2017-04-02_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that the higher learning rates (i.e. &amp;gt; 0.05) are jumping around the true win rate (0.7, dashed line) quite a bit, whereas setting &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.05\)&lt;/span&gt; allows for a more stable estimate. Let’s try again with learning rates closer to 0.05.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Learning rates closer to 0.05
beta_01 &amp;lt;- rw_update(lambda = payoff, beta = 0.01, init = 0)
beta_03 &amp;lt;- rw_update(lambda = payoff, beta = 0.03, init = 0)
beta_05 &amp;lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_07 &amp;lt;- rw_update(lambda = payoff, beta = 0.07, init = 0)
beta_09 &amp;lt;- rw_update(lambda = payoff, beta = 0.09, init = 0)

# Store in data.frame for plotting
all_data &amp;lt;- stack(data.frame(beta_01 = beta_01,
                             beta_03 = beta_03,
                             beta_05 = beta_05,
                             beta_07 = beta_07,
                             beta_09 = beta_09))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:num_trials, 5)
names(all_data)[2]  &amp;lt;- &amp;quot;Beta&amp;quot;

# Plot results
p2 &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) +
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&amp;quot;black&amp;quot;, linetype = &amp;quot;longdash&amp;quot;) + 
  ggtitle(&amp;quot;Expected Probability of Winning Outcome&amp;quot;) + 
  xlab(&amp;quot;Trial Number&amp;quot;) + 
  ylab(&amp;quot;Expected Pr(win)&amp;quot;)
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-04-choice_rl_1/index.en_files/figure-html/2017-04-02_fig2-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;These results look a bit better. However, it is apparent that setting &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; too low is making the updating very sluggish. If we run more trials, however, we should see the expectation converge.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of &amp;#39;trials&amp;#39;
num_trials &amp;lt;- 500

# The win rate is 0.7
payoff &amp;lt;- rbinom(n = num_trials, size = 1, prob = 0.7)

# Learning rates closer to 0.05
beta_01 &amp;lt;- rw_update(lambda = payoff, beta = 0.01, init = 0)
beta_03 &amp;lt;- rw_update(lambda = payoff, beta = 0.03, init = 0)
beta_05 &amp;lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_07 &amp;lt;- rw_update(lambda = payoff, beta = 0.07, init = 0)
beta_09 &amp;lt;- rw_update(lambda = payoff, beta = 0.09, init = 0)

# Store in data.frame for plotting
all_data &amp;lt;- stack(data.frame(beta_01 = beta_01,
                             beta_03 = beta_03,
                             beta_05 = beta_05,
                             beta_07 = beta_07,
                             beta_09 = beta_09))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:num_trials, 5)
names(all_data)[2]  &amp;lt;- &amp;quot;Beta&amp;quot;

# Plot results
p2 &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) +
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&amp;quot;black&amp;quot;, linetype = &amp;quot;longdash&amp;quot;) + 
  ggtitle(&amp;quot;Expected Probability of Winning Outcome&amp;quot;) + 
  xlab(&amp;quot;Trial Number&amp;quot;) + 
  ylab(&amp;quot;Expected Pr(win)&amp;quot;)
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-04-choice_rl_1/index.en_files/figure-html/2017-04-02_fig3-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Over 500 trials, the expected value for the win rate converges to the true win rate, 70%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Summary&lt;/h1&gt;
&lt;p&gt;In this post, we reviewed the original Rescorla-Wagner updating rule (a.k.a. the &lt;em&gt;Delta Rule&lt;/em&gt;) and explored its contemporary instantiation. I have shown that the Delta Rule can be used to estimate the win rate of a slot machine on a trial-by-trial basis. While this example may first appear trivial (e.g. why not just take the average of all past outcomes?), we will explore more practical usages in later posts. For now, try playing with the above code yourself! If you want a slightly more challenging problem, try finding the solution to the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;How should you change the learning rate so that the expected win rate is always the average of all past outcomes?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint –&amp;gt; With some simple algebra, the Delta Rule can be re-written as follows:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = (1 - \beta) \cdot Pr(win)_{t} + \beta \cdot \lambda_{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Happy solving! See the answer in the next post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package</title>
      <link>http://haines-lab.com/publication/ahn_haines_cpsy_2017/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/publication/ahn_haines_cpsy_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>http://haines-lab.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://haines-lab.com/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
