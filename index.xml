<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computational Psychology on Computational Psychology</title>
    <link>http://haines-lab.com/</link>
    <description>Recent content in Computational Psychology on Computational Psychology</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Nathaniel Haines</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Using computer-vision and machine learning to automate facial coding of positive and negative affect intensity</title>
      <link>http://haines-lab.com/publication/haines_plos_one_2019/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/publication/haines_plos_one_2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Outcome‐Representation Learning Model: A Novel Reinforcement Learning Model of the Iowa Gambling Task</title>
      <link>http://haines-lab.com/publication/haines_cog_sci_2018/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/publication/haines_cog_sci_2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (3)</title>
      <link>http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/</guid>
      <description>&lt;div id=&#34;goals-of-paramter-estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Goals of Paramter Estimation&lt;/h1&gt;
&lt;p&gt;When estimating paramters for a given model, we typically aim to make an inference on an individual’s underlying decision process. We may be inferring a variety of different factors, such as the rate at which someone updates their expectations, the way that someone subjectively values an outcome, or the amount of exploration versus exploitation that someone engages in. Once we estimate an individual’s parameters, we can compare then to other people or even other groups of people. Further, we can compare parameters within subjects after an experimental manipulation (e.g., &lt;em&gt;does drug X affect a person’s learning rate?&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;Below, we will explore multiple paremter estimation methods. Specifically, we will use: (1) maximum likelihood estimation, (2) maximum a posteriori estimation, (3) and fully Bayesian estimation. First, we will simulate data from models described in the previous post on a simple 2-armed bandit task. Importantly, we will simulate data using &lt;em&gt;known parameter values&lt;/em&gt;, which we will then try to recover from the simulated data. We will refer to the known paramters as the &lt;em&gt;true paramters&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Simulation&lt;/h1&gt;
&lt;p&gt;For our simulation, we will simulate choice from a model using delta-rule learning and softmax choice. To keep things simple, the learning rate will be the only free paramter in the model. Additionally, we will simulate choices in a task where there are two choices, where choice 1 has a mean payoff of 1 and choice 2 has a mean payoff of -1. Therefore, a learning agent should be able to learn that choice 1 is optimal and make selections accordingly. However, we will add noise to each choice payoff (&lt;em&gt;sigma&lt;/em&gt; below) to make things more realistic.&lt;/p&gt;
&lt;p&gt;The following R code simulates 100 trials using the model ans task described above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For pretty plots
library(ggplot2)
library(foreach)

# Simulation paramters
set.seed(1)        # Random seed for replication
mu    &amp;lt;- c(1, -1)  # Mean payoff for choices 1 and 2 
sigma &amp;lt;- 2         # SD of payoff distributions
n_tr  &amp;lt;- 100       # Number of trials 
beta  &amp;lt;- 0.1       # True learning rate

# Initial expected value
ev &amp;lt;- c(0, 0) 

# Softmax choice function
logsumexp &amp;lt;- function (x) {
  y &amp;lt;- max(x)
  y + log(sum(exp(x - y)))
}
softmax &amp;lt;- function (x) {
  exp(x - logsumexp(x))
}

# Simulate data
sim_dat &amp;lt;- foreach(t=1:n_tr, .combine = &amp;quot;rbind&amp;quot;) %do% {
  # Generate choice probability with softmax
  pr &amp;lt;- softmax(ev)
  
  # Use choice probability to sample choice
  choice &amp;lt;- sample(c(1,2), size = 1, prob = pr)
  
  # Generate outcome based on choice
  outcome &amp;lt;- rnorm(1, mean = mu[choice], sd = sigma)
  
  # Delta-rule learning
  ev[choice] &amp;lt;- ev[choice] + beta * (outcome - ev[choice])
  
  # Save data
  data.frame(Trial   = rep(t, 2),
             EV      = ev,
             Pr      = pr,
             Option  = paste(1:2),
             Choice  = rep(choice, 2),
             Outcome = rep(outcome, 2))
}

# Change in expected values across tirals
ggplot(sim_dat, aes(x = Trial, y = EV, geom = &amp;quot;line&amp;quot;, color = Option)) +
  geom_line() +
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) +
  ylab(&amp;quot;Expected Value&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;The above graph shows the simulated agent’s expected value (&lt;em&gt;EV&lt;/em&gt;) for options 1 and 2 across the 100 trials. We can also view the probability of selecting each option across trials:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Change in probability of selecting each option across tirals
ggplot(sim_dat, aes(x = Trial, y = Pr, geom = &amp;quot;line&amp;quot;, color = Option)) +
  geom_line() +
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;blue&amp;quot;)) +
  ylab(&amp;quot;Pr(Choice)&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig2-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the agent learns to prefer option 1 over option 2 across trials. Although we know the true learning rate (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true} = 0.1\)&lt;/span&gt;), we will explore the various parameter estimation techniques below to try and recover &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true}\)&lt;/span&gt; from the simulated data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimation-methods&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Parameter Estimation Methods&lt;/h1&gt;
&lt;div id=&#34;maximum-likelihood&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.1 Maximum Likelihood&lt;/h2&gt;
&lt;p&gt;The goal of maximum likelihood estimation (MLE) is to identify the single, most likely parameter value(s) that could have produced the observed data. For our purposes, MLE will allow us to estimate the learning rate that maximizes the probability of observing the simulated data. We refer to his estimate as &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; (pronounced beta-hat).&lt;/p&gt;
&lt;p&gt;Before moving on, it is worth introducing some new notation. If we refer to the observed data as &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and the parameters we aim to estimate as &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, we can refer to the likelihood function as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(X|\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In our case, &lt;span class=&#34;math inline&#34;&gt;\(\theta = \hat{\beta}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the vector of simulated choices from above. So then, how do we actually compute the probability of choices &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given learning rate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;? Simple! We use the model that we simulated the data from. Specifically, we:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Make a guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Look into &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and find out what choice and outcome the agent made/experienced on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Use our guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; to update the EV for the chosen option accoring to the model&lt;/li&gt;
&lt;li&gt;Enter the updated EVs into the softmax function to generate the probability of selecting each option for the next trial&lt;/li&gt;
&lt;li&gt;Store the model-estimated probability of selecting the choice that the agent actually made on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We iterate through these steps for each trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and then multiply the probabilities across all trials. In practice, we take the natural log of the probability on each trial and then sum across trials, which is equivalent to multiplying out the probabilities but is more numerically stable (computers don’t like really small numbers!). We can write out this &lt;em&gt;log-likelihood&lt;/em&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\sum_{t=1}^{T}{\text{ln } Pr(Choice_{t}|\hat{\beta})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We are not finished yet! Once we compute the above sum for a given guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;, we will run through all the steps again with a new guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;. We continue to make guesses and calculate the above sum until we find a value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; that gives us the maximum probability—this final value is the &lt;em&gt;maximum likelihood estimate (MLE)&lt;/em&gt;, written as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\beta}_{MLE} = \underset{\hat{\beta}}{\text{arg max}}\sum_{t=1}^{T}{\text{ln } Pr(Choice_{t}|\hat{\beta})}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to find the value for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; that maximizes the above sum. Now, we could accomplish this by sampling random learning rates between 0 and 1, computing the sum for each value, and then determining which value produces the highest log-likelihood. Alternatively, we could create a grid of values from 0 to 1 (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} \in \text{{0.01, 0.02, ..., 0.99}}\)&lt;/span&gt;) and select the MLE as the value with the highest log-likelihood. In the real world, however, we usually use some sort of optimization algorithm that makes our job much easier. Below, we will use the &lt;strong&gt;optim&lt;/strong&gt; function in R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define the log-likelihood function used for MLE
mle_bandit &amp;lt;- function(X, beta, outcomes)  {
  # Initialize expected value
  ev &amp;lt;- c(0, 0)
  # loop through each trial and compute log-likelihood
  ll &amp;lt;- foreach(t=seq_along(X), .combine = &amp;quot;c&amp;quot;) %do% {
    # Generate choice probability with softmax
    pr &amp;lt;- softmax(ev)
    
    # Delta-rule learning
    ev[X[t]] &amp;lt;- ev[X[t]] + beta * (outcomes[t] - ev[X[t]])
    
    # log probability of &amp;quot;true&amp;quot; simulated choice
    log(pr[X[t]])
  }
  
  # return the summed (minus) log-likelihood, because optim minimizes by default
  sum(-1*ll)
}

# Use optim to minimize the (minus) log-likelihood function
mle_results &amp;lt;- optim(par      = 0.5,             # Initial guess for beta
                     fn       = mle_bandit,      # Function we are minimizing
                     method   = &amp;quot;L-BFGS-B&amp;quot;,      # Specific algorithm used
                     lower    = 0,               # Lower bound for beta 
                     upper    = 1,               # Upper bound for beta
                     X        = sim_dat$Choice,  # Simulated choices
                     outcomes = sim_dat$Outcome) # Simulated choice outcomes

# Print results
cat(&amp;quot;The MLE for beta is: &amp;quot; , round(mle_results$par, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The MLE for beta is:  0.079&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Not bad! &lt;strong&gt;optim&lt;/strong&gt; returns a MLE of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(0.079\)&lt;/span&gt;. Given that &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true} = 0.1\)&lt;/span&gt; (since we determined the value for the simulations), our estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is not that far off. One potential downside of the MLE approach we used above is that we only receive a single value for the MLE, which makes it difficult to know how certain the estimate is. For example, our simulation was for 100 trials, but surely we would be more confident in the estimate if the simulation was for 1,000’s of trials! MLE alone does not offer an explicit measure of uncertainty in the parameter estimates without additional analyses (and additional assumptions), which is one reason that Bayesian methods are easier to interpret.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;maximum-a-poseriori-map-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.2 Maximum A Poseriori (MAP) Estimation&lt;/h2&gt;
&lt;p&gt;MAP estimation is a straightforward extention of MLE, which allows us to incorporate prior information about the parameter that we are trying to estimate into the estimation procedure. In our example, we may know from prior studies that learning rates for a given task typically fall in the range of 0.05-0.4. MAP estimation allows us to formalize this prior research in a very simple way! We simply parameterize a prior distribution (&lt;span class=&#34;math inline&#34;&gt;\(Pr(\beta)\)&lt;/span&gt;) that is consistent with estimates from prior studies. For example, a normal distribution with a &lt;span class=&#34;math inline&#34;&gt;\(\mu = .2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.5\)&lt;/span&gt; captures the above range nicely but does not constrain the values too much:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x &amp;lt;- seq(0, 1, length=1000)
y &amp;lt;- dnorm(x, mean = .2, sd = .5)
qplot(x = x, y = y, geom = &amp;quot;line&amp;quot;, xlab = expression(beta[prior]), ylab = &amp;quot;Density&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig4-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;So, how do we include this prior information? Easy! When we make a guess for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;, we will compute the likelihood just like we did for MLE, and we will multiple this value by the “likelihood” of the prior. Intuitively, this allows us to &lt;em&gt;weight&lt;/em&gt; the likelihood of each possible value for &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; by the prior for that same value of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;. This behavior results in a sort of trade off between the likelihood and prior distribution, which ends up &lt;strong&gt;regularizing&lt;/strong&gt; our MLE estimate by pulling it toward the center mass of the prior distribution. Formally, we represent this by adding the prior distribution (bolded) to the MLE function from above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{\beta}_{MAP} = \underset{\hat{\beta}}{\text{arg max}}\sum_{t=1}^{T}{\text{ln } Pr(Choice_{t}|\hat{\beta})\bf{Pr(\beta)}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s see what this looks like in R, and note how it affects estimation of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define the log-likelihood function used for MAP
map_bandit &amp;lt;- function(X, beta, outcomes)  {
  # Initialize expected value
  ev &amp;lt;- c(0, 0)
  # loop through each trial and compute log-likelihood
  ll &amp;lt;- foreach(t=seq_along(X), .combine = &amp;quot;c&amp;quot;) %do% {
    # Generate choice probability with softmax
    pr &amp;lt;- softmax(ev)
    
    # Delta-rule learning
    ev[X[t]] &amp;lt;- ev[X[t]] + beta * (outcomes[t] - ev[X[t]])
    
    # Probability/likelihood of &amp;quot;true&amp;quot; simulated choice
    like &amp;lt;- pr[X[t]]
    
    # Likelihood of current beta according to prior distribution
    prior &amp;lt;- dnorm(x = beta, mean = .2, sd = 0.5)
    
    # Log of like*prior
    log(like*prior)
  }
  
  # return the summed (minus) log-likelihood with prior information included
  sum(-1*ll)
}

# Use optim to minimize the (minus) log-likelihood function
map_results &amp;lt;- optim(par      = 0.5,             # Initial guess for beta
                     fn       = map_bandit,      # Function we are minimizing
                     method   = &amp;quot;L-BFGS-B&amp;quot;,      # Specific algorithm used
                     lower    = 0,               # Lower bound for beta 
                     upper    = 1,               # Upper bound for beta
                     X        = sim_dat$Choice,  # Simulated choices
                     outcomes = sim_dat$Outcome) # Simulated choice outcomes

# Print results
cat(&amp;quot;The MAP for beta is: &amp;quot; , round(map_results$par, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The MAP for beta is:  0.145&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Woah! The simple addition of prior information pushed our estimate of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(0.079\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} =\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(0.145\)&lt;/span&gt;. Notice that the MAP estimator was pulled toward the mean of the prior distribution. However, is this a good thing? When is this behavior beneficial? After all, both estimates are about equidistant from &lt;span class=&#34;math inline&#34;&gt;\(\beta_{true}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To demonstrate the benefit of prior information, let’s take the simulated data from our learner (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.1\)&lt;/span&gt;), but only fit the model using the first 10 trials worth of data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use MLE to fit the first 10 trials
mle_results_10tr &amp;lt;- optim(par      = 0.5,             
                          fn       = mle_bandit,      
                          method   = &amp;quot;L-BFGS-B&amp;quot;,      
                          lower    = 0,               
                          upper    = 1,               
                          X        = sim_dat$Choice[1:15],  # Only using first 10 trials
                          outcomes = sim_dat$Outcome[1:15]) 

# Use MAP to fit the first 10 trials
map_results_10tr &amp;lt;- optim(par      = 0.5,             
                          fn       = map_bandit,      
                          method   = &amp;quot;L-BFGS-B&amp;quot;,      
                          lower    = 0,               
                          upper    = 1,               
                          X        = sim_dat$Choice[1:15],  # Only using first 10 trials
                          outcomes = sim_dat$Outcome[1:15]) 

cat(&amp;quot;The MLE for beta with 10 trials is: &amp;quot; , round(mle_results_10tr$par, 3), &amp;quot;\n&amp;quot;, 
    &amp;quot;The MAP for beta with 10 trials is: &amp;quot; , round(map_results_10tr$par, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The MLE for beta with 10 trials is:  0.06 
##  The MAP for beta with 10 trials is:  0.099&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Look at that! MLE underestimates the learning rate, whereas MAP gives us a great estimate. There are two main take-aways from this toy example:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;MLE is prone to biased estimates in low data settings, which can push parameters to the edge of the parameter space (i.e. 0 when the range is from 0 to 1), and&lt;/li&gt;
&lt;li&gt;Introduction of our own, theory-based form of bias (i.e. regularization from the prior distribution) can help us avoid estimation problems—espectially in low data settings! (this will become clearer in future posts)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In fact, you may have realized through the above example (or math) that MLE is just a sepcial case of MAP estimation! If it is not already intuitive, think of this—what would happen to our MAP estimate if we assumed that the prior distribution was uniform (i.e. all values between 0 and 1 are equally likely for learning rate &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;)? Well, we would have to multiply &lt;span class=&#34;math inline&#34;&gt;\(Pr(Choice_{t}|\hat{\beta})\)&lt;/span&gt; by 1 for each guess of &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;! See this yourself by observing the likelihood of different values for &lt;code&gt;x&lt;/code&gt; drawn from a uniform distribution (code: &lt;code&gt;dunif(x = .2, min = 0, max = 1)&lt;/code&gt;). Therfore, MAP is analytically equivalent to MLE when we assume a uniform prior distibution. Of course, in many settings, we know that certain paremter values are very unlikely (e.g., a learning rate of .2 is more reasonable than of .99 in most settings). It follows that assuming a uniform distribution for the prior can be quite (mis)informative!&lt;/p&gt;
&lt;p&gt;Note that MAP, like MLE, only offers a point estimate. Again, we would ideally like a proper representation of uncertainty for our estimate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;markov-chain-monte-carlo-mcmc-estimation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3.3 Markov Chain Monte Carlo (MCMC) Estimation&lt;/h2&gt;
&lt;p&gt;We have finally arrived… MCMC builds on all of the above estimation methods, resulting in a powerful estimation procedure that gives as an entire distribution—rather than just a point estimate—to represent a parameter.&lt;/p&gt;
&lt;p&gt;To begin, we will first introduce Bayes’ Theorem; you have likely seen is before:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(\theta | X) = \frac{Pr(X | \theta)Pr(\theta)}{Pr(X)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In English, this translates to:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Posterior Distribution} = \frac{\text{Likelihood} \cdot \text{Prior Distribution}}{\text{Marginal Distribution}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You may notice that the numerator (i.e. &lt;span class=&#34;math inline&#34;&gt;\(Pr(X | \theta)Pr(\theta)\)&lt;/span&gt;) looks suspiciously like the term we were trying to maximize for MAP estimation (&lt;span class=&#34;math inline&#34;&gt;\(Pr(Choice_{t}|\hat{\beta})Pr(\beta)\)&lt;/span&gt;)…which is because MAP estimation is indeed derived from Bayes’ Theorem! In fact, the MAP estimate is the &lt;em&gt;mode of the posterior distribution&lt;/em&gt;, which explains why it is called &lt;strong&gt;maximum a posteriori&lt;/strong&gt; estimation.&lt;/p&gt;
&lt;p&gt;So then, we already know what the numerator corresponds to, but what of the denominator? Referring back to our simulation, the marginal distribution &lt;span class=&#34;math inline&#34;&gt;\(Pr(X)\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(Pr(Choices)\)&lt;/span&gt;, which is interpreted as the probability of the observed data—what exactly does this mean? Well, it turns out that for our purposes, it is not too important! &lt;span class=&#34;math inline&#34;&gt;\(Pr(Choices)\)&lt;/span&gt; is a constant term, and it does not depend on the model we are trying to estimate parameters for. Therefore, we often write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(\theta | X) \propto Pr(X | \theta)Pr(\theta)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which translates to “&lt;em&gt;the posterior distribution is &lt;strong&gt;proportional to&lt;/strong&gt; the likelihood times the prior distribution&lt;/em&gt;”. Intuitively, this means that it is the relative differences in &lt;span class=&#34;math inline&#34;&gt;\(Pr(X | \theta)Pr(\theta)\)&lt;/span&gt; across different values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that give us information about the posterior distribution. Importantly, we already know how to work with this numerator term (from doing MAP estimation)! Therefore, fully Bayesian estimation using MCMC only requires a small extention. Specifically, instead of using optimization (i.e. R’s &lt;code&gt;optim&lt;/code&gt; function) to find a single value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that maximizes &lt;span class=&#34;math inline&#34;&gt;\(Pr(X | \theta)Pr(\theta)\)&lt;/span&gt;, we want to use a method that tells us how likely all possible values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; are relative to each other (i.e. a distribution!). While there are many different algorithms that can be used to accomplish this, we will start with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm&#34;&gt;Metropolis&lt;/a&gt; algorithm. Referring back to our learning data, estimating &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; using the Metropolis algorithm proceeds with the steps outlined below.&lt;/p&gt;
&lt;p&gt;For &lt;span class=&#34;math inline&#34;&gt;\(n = 1, 2, ..., N:\)&lt;/span&gt;&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Propose a value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}&amp;#39;\)&lt;/span&gt; that is near your current guess &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Calculate the &lt;em&gt;acceptance ratio&lt;/em&gt;, defined by &lt;span class=&#34;math inline&#34;&gt;\(accept = \frac{Pr(Choices | \hat{\beta}&amp;#39;)Pr(\hat{\beta}&amp;#39;)}{Pr(Choices | \hat{\beta}_{n})Pr(\hat{\beta}_{n})}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Generate a uniform random number &lt;span class=&#34;math inline&#34;&gt;\(u\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\([0,1]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;If &lt;span class=&#34;math inline&#34;&gt;\(u \le accept\)&lt;/span&gt;, set &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n+1} = \hat{\beta}&amp;#39;\)&lt;/span&gt;, otherwise set &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n+1} = \hat{\beta}_{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Importantly, while iterating through all samples &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;, we store each value &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}_{n}\)&lt;/span&gt;. This sequence of values &lt;em&gt;is the posterior distribution&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(Pr(\hat{\beta}|Choices)\)&lt;/span&gt;. The R code below shows the Metropolis algorithm in action, and the resulting histogram of posterior samples:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that this takes a few minutes to run because it is not optimized in the least bit!&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Set number of samples N for the Metropolis algorithm
samples &amp;lt;- 5000

# Set initial guess for beta
beta_n &amp;lt;- 0.5

# Take what we did above for MAP estimation and make into a function
calc_like &amp;lt;- function(beta, X, outcomes) {
  # Initialize expected value
  ev &amp;lt;- c(0, 0)
  # loop through each trial and compute log-likelihood
  ll &amp;lt;- foreach(t=seq_along(X), .combine = &amp;quot;c&amp;quot;) %do% {
    # Generate choice probability with softmax
    pr &amp;lt;- softmax(ev)
    
    # Delta-rule learning
    ev[X[t]] &amp;lt;- ev[X[t]] + beta * (outcomes[t] - ev[X[t]])
    
    # Probability/likelihood of &amp;quot;true&amp;quot; simulated choice
    like &amp;lt;- pr[X[t]]
    
    # Likelihood of current beta according to prior distribution
    prior &amp;lt;- dnorm(x = beta, mean = .2, sd = 0.5)
    
    # log of like*prior
    log(like*prior)
  }
  
  # return the summed log-likelihood with prior information included
  sum(ll)
}

# Iterate through N samples and store each result
posterior &amp;lt;- foreach(n=1:samples, .combine = &amp;quot;c&amp;quot;) %do% {
  # Step 1: Generate random proposal value with normal distribution
  beta_proposal &amp;lt;- rnorm(1, mean = beta_n, sd = .01)
  
  # If proposal is outside of parameter bounds, keep current sample, else continue
  if (0 &amp;lt; beta_proposal &amp;amp; beta_proposal &amp;lt; 1) {
    # Step 2: Calculate acceptance ratio
    like_proposal &amp;lt;- exp(calc_like(beta_proposal, sim_dat$Choice, sim_dat$Outcome))
    like_current  &amp;lt;- exp(calc_like(beta_n, sim_dat$Choice, sim_dat$Outcome))
    accept &amp;lt;- like_proposal/like_current
    
    # Step 3: Generate uniform random number on [0,1]
    u &amp;lt;- runif(1, min = 0, max = 1)
    
    # Step 4: Accept or reject proposal
    if (u &amp;lt;= accept) {
      beta_n &amp;lt;- beta_proposal
    }
  }
  
  # Retern beta_n (either updated with proposal or remains the same)
  beta_n
}

# Plot time-series of posterior samples
qplot(x = 1:samples, y = posterior, geom = &amp;quot;line&amp;quot;) + 
  geom_hline(aes(yintercept= beta, linetype = &amp;quot;True Beta&amp;quot;), color= &amp;#39;red&amp;#39;) +
  scale_linetype_manual(name = &amp;quot;&amp;quot;, values = 2) +
  xlab(&amp;quot;Posterior Sample&amp;quot;) +
  ylab(expression(hat(beta))) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig7-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;This &lt;em&gt;traceplot&lt;/em&gt; shows each accepted proposal across all 5,000 samples that we drew from the posterior distribution. As you can see, the samples are all distributed around the true value once they converge to a stable estimate. The samples at the beginning (before reaching convergence) will be discarded (termed &lt;em&gt;burn-in&lt;/em&gt; samples) for further analyses because they do not represent the posterior distribution. Unlike for MLE or MAP estimation, all the posterior samples after burn-in can be used to represent the uncertainty in the learning rate parameter! We simply plot a histogram of the samples:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;qplot(posterior[200:5000], geom = &amp;quot;histogram&amp;quot;) +
  geom_vline(aes(xintercept= beta, linetype = &amp;quot;True Beta&amp;quot;), color= &amp;#39;red&amp;#39;) +
  scale_linetype_manual(name = &amp;quot;&amp;quot;, values = 2) +
  xlab(expression(hat(beta))) +
  ylab(&amp;quot;Frequency&amp;quot;) +
  theme_minimal(base_size = 20)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3_files/figure-html/2018-09-08_fig8-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Wrapping up&lt;/h1&gt;
&lt;p&gt;In this post, we covered three methods of parameter estimation including: (1) maximum likelihood estimation, (2) maximum a posteriori estimation, and (3) markov chain monte carlo estimation. In the future, we will use software packages (particularly &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt;) to do MCMC for us, which will allow us to more rapidly estimate parameters compared to our simplistic MCMC implementation above. In the next post, we will use Stan to estimate the same learning rate from the model above. Soon after, we will work towards fitting hierarchical models!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>FEAT</title>
      <link>http://haines-lab.com/project/feat/</link>
      <pubDate>Mon, 22 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/project/feat/</guid>
      <description>&lt;p&gt;This project is currently at the beginning stages of development. Come back in a few months for an update!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Easyml: Easily Build And Evaluate Machine Learning Models</title>
      <link>http://haines-lab.com/publication/easyml/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/publication/easyml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Indirect Effect of Emotion Regulation on Minority Stress and Problematic Substance Use in Lesbian, Gay, and Bisexual Individuals</title>
      <link>http://haines-lab.com/publication/minority_stress/</link>
      <pubDate>Wed, 25 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/publication/minority_stress/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Linking emotion to decision making through model-based facial expression analysis</title>
      <link>http://haines-lab.com/talk/math_psych_2017/</link>
      <pubDate>Sun, 23 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/talk/math_psych_2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (2)</title>
      <link>http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/</link>
      <pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/</guid>
      <description>&lt;div id=&#34;answer-to-post-1&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Answer to post 1&lt;/h1&gt;
&lt;p&gt;In the previous &lt;a href=&#34;http://haines-lab.com/2017/04/04/human-choice-and-reinforcement-learning-1/&#34;&gt;post&lt;/a&gt;, I reviewed the Rescorla-Wagner updating (Delta) rule and its contemporary instantiation. At the end, I asked the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;How should you change the learning rate so that the expected win rate is always the average of all past outcomes?&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will go over the answer to this question before progressing to the use of the Delta rule in modeling human choice. To begin, refer back to the Delta rule written in the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = (1 - \beta) \cdot Pr(win)_{t} + \beta \cdot \lambda_{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here, we see that in the Delta rule the expected win probability for the next trial is equal to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average&#34;&gt;&lt;em&gt;exponentially weighted moving average&lt;/em&gt;&lt;/a&gt; of the past expectation and the current outcome. It is easy to show this through a visualization of the expectation over time. For example, imagine that we have a vector of outcomes &lt;span class=&#34;math inline&#34;&gt;\(\lambda = [1,0,0,1,1,1,0,1,1,1]\)&lt;/span&gt;, where 0 and 1 represent losing and winning slot machine rolls, respectively. Note that in this example, the placement of these outcomes within the vector &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; indicates their temporal order (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}=1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{2}=0\)&lt;/span&gt;, etc.). Now, if we set an arbitrary learning rate such as &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.05\)&lt;/span&gt;, what is the expected win rate after iterating through outcomes &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;? The R code below demonstrates the use of the Delta rule and an alternative exponential weighting scheme–which takes the form of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Power_series#Examples&#34;&gt;&lt;em&gt;power series&lt;/em&gt;&lt;/a&gt;–to determine the expectation on each trial:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For pretty plots
library(ggplot2)

# Assign lambda (lambda[1] == first trial)
lambda &amp;lt;- c(1,0,0,1,1,1,0,1,1,1)

# Set learning rate
beta &amp;lt;- 0.05

### Iterative prediction error (Delta rule) approach ###

# Function that iterates the Rescorla-Wagner rule 
  # This function is slightly modified from the last post
  # to ensure that that final expectation is stored
rw_update &amp;lt;- function(lambda, beta, init) {
  # To store expected value for each trial
  Pr_win &amp;lt;- vector(length=length(lambda)+1)
  # Set initial value
  Pr_win[1] &amp;lt;- init
  for (t in 1:(length(lambda))) {
    Pr_win[t+1] &amp;lt;- Pr_win[t] + beta * (lambda[t] - Pr_win[t])
  }
  return(Pr_win)
}

# With initial expectation = 0, iterate Delta rule
delta_results &amp;lt;- rw_update(lambda = lambda, 
                           beta   = beta, 
                           init   = 0)[-1]
                          #             ^
                          # Remove initial value (0)

### Power series approach ###

# Direct exponential weighting (saving all expectations)
power_ser &amp;lt;- NULL
for (i in 1:10) {
  power_ser[i] &amp;lt;- beta * sum((1-beta)^(0:(i-1))*lambda[i:1])
}

### Comparison of both approaches ###
all(round(delta_results, 8) == round(power_ser, 8))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data.frame for ggplot
all_data &amp;lt;- stack(data.frame(delta = delta_results,
                             pow_s = power_ser))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:10, 2)
names(all_data)[2] &amp;lt;- &amp;quot;Approach&amp;quot;

# Visually
p &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Approach)) + 
  geom_line() +
  facet_grid(facets = &amp;quot;Approach ~ .&amp;quot;) + 
  ggtitle(&amp;quot;Comparison of approaches&amp;quot;) +
  xlab(&amp;quot;Trial Number&amp;quot;) +
  ylab(&amp;quot;Expected Win Probability&amp;quot;)
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2_files/figure-html/2017-04-07_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the plots, both the Delta rule and the power series approach yield the same exact expectations when iterated for each trial. However, the power series form requires each past observation while the Delta rule only requires the last expectation–this feature makes the Delta rule form of the equation much more plausible as a processes that people may use to estimate the value of a choice. This is because the computational cost does not increase with the number of past observations.&lt;/p&gt;
&lt;p&gt;Through this example, those familiar with &lt;a href=&#34;http://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_averages&#34;&gt;economics&lt;/a&gt; or &lt;a href=&#34;https://en.wikipedia.org/wiki/Signal_processing&#34;&gt;signal processing&lt;/a&gt; may find the Delta rule familiar. Essentially, we can think of the Delta rule as a smoothing function or a &lt;a href=&#34;https://en.wikipedia.org/wiki/High-pass_filter#Algorithmic_implementation&#34;&gt;high- or low-pass filter&lt;/a&gt;–albeit in the time as opposed to frequency domain–which effectively attenuates the effect of past or current outcomes, respectively. What makes this specific form interesting is again the fact that it can be iterated (i.e. it is recursive), making it a realistic approximation to the computations performed by the brain when estimating some value.&lt;/p&gt;
&lt;p&gt;With the above intuitions in mind, we will now get back to the question. How do we change the learning rate to ensure that the current expectation is always the simple average of all past outcomes? Since the above example showed that the Delta rule is really just a moving average where past outcomes are given exponentially decreasing weights, our goal is to make all outcomes equally represented. In other words, we want to weight past and current outcomes equally–this is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average&#34;&gt;&lt;em&gt;cumulative moving average&lt;/em&gt;&lt;/a&gt;. Using our slot machine example, the cumulative moving average formula (in its most common form) is written as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = \frac{\lambda_{t} + (t-1) \cdot Pr(win)_{t}}{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can re-write the above equation into one that is more familiar to us:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = (1-\frac{1}{t}) \cdot Pr(win)_{t} + \frac{1}{t} \cdot \lambda_{t} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the above form, you can see that the cumulative moving average can be computed using the Delta rule by setting the learning rate to &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{t}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the trial number. Looking at the equation, you will notice that as &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; increases, the weight &lt;span class=&#34;math inline&#34;&gt;\((1-\frac{1}{t})\)&lt;/span&gt; placed on the past probability estimate &lt;span class=&#34;math inline&#34;&gt;\(Pr(win)_{t}\)&lt;/span&gt; becomes larger while the weight &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{t}\)&lt;/span&gt; on the current outcome &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{t}\)&lt;/span&gt; shrinks. This behavior ensures that past outcomes are not discounted at a higher rate than current ones.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;choice-mechanisms&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Choice mechanisms&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;http://az616578.vo.msecnd.net/files/2016/08/19/636072180885324217301014601_shoes.jpg&#34; alt=&#34;Figure 1&#34; style=&#34;height: 100%; width: 100%; object-fit: contain&#34;/&gt;&lt;/p&gt;
&lt;p&gt;With the Delta rule, we can approximate how people with a certain learning rate may update their expectations about an outcome on a trial-by-trial basis, but how does this translate to choice? In the slot machine example, we only had a single choice (i.e. pull the lever and see if you win), so this question never applied to us. &lt;strong&gt;But what if we have 2 slot machines and we want to select the one that will win most frequently?&lt;/strong&gt; In this case, we can use the Delta rule to update win probability expectations for each slot machine separately, but what do we do with these values after they are computed? Below, I will describe three different methods that can be used to translate expected values to choice.&lt;/p&gt;
&lt;div id=&#34;greedy-choice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.1 Greedy choice&lt;/h2&gt;
&lt;p&gt;Greedy choice is simple–choose the option with the highest expected value on each trial (i.e. pick the &lt;em&gt;greedy&lt;/em&gt; option):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[c_{t} = \underset{s \in S}{argmax}(V(s_{t}))\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(c_{t}\)&lt;/span&gt; indicates the choice made on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V(s_{t})\)&lt;/span&gt; represents the value associated with Slot machine &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Applying this logic to our example, this would equate to choosing the slot machine with the highest expected win probability. While this may sound like a good idea, it is important to remember that we do not know what the true win rate is for either slot machine. Instead, we estimate it after each outcome. With this in mind, a simple example (below) shows why the greedy choice method fails in practical applications.&lt;/p&gt;
&lt;p&gt;Imagine you are choosing between 2 slot machines, where machine A (&lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt;) has a true win rate of 0.9, and machine B (&lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt;) has a true win rate of 0.5. Obviously, &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt; is a better option, but this is something you need to learn by making a choice and updating your expectations. Assuming that you start off thinking that each slot machine has a win rate of 0 (which is typical in human decision making models), your first choice will be a random selection between the equivalent options. In our example, imagine that you randomly choose &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt;, the slot machine spins, and then you win! Great, so now (regardless of your learning rate), you will update your expectation of the win rate for &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt; to a positive, non-zero value. On the next trial, you are greedy, so you again choose &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt;–it has a higher expected win rate than &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt;. In fact, you will continue to choose &lt;span class=&#34;math inline&#34;&gt;\(Slot_{B}\)&lt;/span&gt; on each trial without ever considering &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt;! In this case, even though &lt;span class=&#34;math inline&#34;&gt;\(Slot_{A}\)&lt;/span&gt; it the optimal choice, you never allow yourself to &lt;em&gt;explore&lt;/em&gt; alternative choices. Here, we come across a classical problem in reward-learning paradigms–&lt;a href=&#34;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0095693&#34;&gt;&lt;strong&gt;&lt;em&gt;the exploration-exploitation tradeoff&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;. The crux of this problem is this: how do you know that a “good” choice is better than other choices that you have yet to explore? Think of it like choosing a job. I chose to study psychology, and I continue to exploit this choice (i.e. I am not exploring other education). How do I know that psychology is for me, though? It is possible that I would have gained more from a computer science degree, but I did not explore that option. In the same way, this compromise exists in simple reinforcement learning paradigms such as choosing the best slot machine. Below, we will explore two methods that address the exploration-exploitation problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;epsilon-greedy-choice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.2 &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-Greedy choice&lt;/h2&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;-greedy method is a simple extention of the greedy method above. Instead of always choosing the option with the highest expected value, we sometimes (with probability &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;) randomly choose another option:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[c_{t} = \cases{
          \underset{s \in S}{argmax}(V(s_{t}))  &amp;amp; \text{with } Pr(1 - \epsilon) \cr
          \text{random choice} &amp;amp; \text{with } Pr(\epsilon)
                }
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By choosing a random option with &lt;span class=&#34;math inline&#34;&gt;\(Pr(\epsilon)\)&lt;/span&gt;, we can avoid getting stuck choosing the non-optimal slot machine. While this method solves our dilemma, it suffers another problem–when randomly selecting options, it gives equal probabilities to each option. Intuitively, a better method would be to choose options &lt;em&gt;probabilistically with respect to their expected values&lt;/em&gt; (i.e. give high probability to relatively good options and &lt;em&gt;vice-versa&lt;/em&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;softmax-choice&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2.3 Softmax choice&lt;/h2&gt;
&lt;p&gt;Also known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Luce%27s_choice_axiom&#34;&gt;Luce choice rule&lt;/a&gt;, the softmax allows choices to be probabilistic with weights respective to expected value:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(c_{t} = s \in S) = \frac{e^{V_{s}(t)}}{\sum_{s = 1}^S e^{V_{s}(t)}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(e^{V_{s}(t)}\)&lt;/span&gt; is the exponentiated expected value of slot machine &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sum_{s = 1}^S e^{V_{s}(t)}\)&lt;/span&gt; is the summation of the exponentiated expected value of both slot machines (there are 2 in our example). When the expected values are entered, the softmax equation returns a probability of selecting each slot machine which we can then use to make an actual choice. In practice, the softmax function is used most often in decision making research–moving forward, we will use the softmax choice mechanism to model human decision making.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;implementation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. Implementation&lt;/h1&gt;
&lt;p&gt;We now have a full model describing each of the following steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluating an outcome,&lt;/li&gt;
&lt;li&gt;Updating previous representations of choice options, and&lt;/li&gt;
&lt;li&gt;Generating a probability of selecting each choice on the next trial.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This model is simple, but it provides the basic building blocks for more complex models that are found in neuroscience, cognitive science, and decision making literature today. In the next post, we will explore various methods which can be used to estimate the &lt;em&gt;free parameters&lt;/em&gt; in the model (e.g. the learning rate) when all we have are the person’s choices.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (1)</title>
      <link>http://haines-lab.com/post/2017-04-04-choice_rl_1/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2017-04-04-choice_rl_1/</guid>
      <description>&lt;div id=&#34;short-history&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;1. Short history&lt;/h1&gt;
&lt;p&gt;In 1972, Robert Rescorla and Allan Wagner developed a formal theory of associative learning, the process through which multiple stimuli are associated with one-another. The most widely used example (Fig. 1) of associative learning comes straight from Psychology 101–Pavlov’s dog.&lt;/p&gt;
&lt;div id=&#34;figure-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Figure 1&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;http://www.savingstudentsmoney.org/psychimg/stangor-fig07_003.jpg&#34; alt=&#34;Figure 1&#34; style=&#34;height: 100%; width: 100%; object-fit: contain&#34;/&gt;&lt;/p&gt;
&lt;p&gt;The idea is simple, and it’s something that we experience quite often in everyday life. In the same way that Pavlov’s dog begins to drool after hearing a bell, certain cognitive and/or biological processes are triggered when we are exposed to stimuli that we have been exposed to in the past. But how can this learning process be modeled? That is to say, what sort of &lt;em&gt;equation&lt;/em&gt; can we use to describe how an agent learns to associate multiple stimuli? To answer these questions, Rescorla and Wagner developed what is now know as the Rescorla-Wagner updating rule:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta V_{A} = \alpha_{A} \beta_{1} (\lambda_{1} - V_{AX})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta V_{X} = \alpha_{X} \beta_{1} (\lambda_{1} - V_{AX})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;First off, note that the original Rescorla-Wagner rule was developed to explain &lt;em&gt;compound stimuli&lt;/em&gt; (e.g. presentation of a bell and light, followed by food). Here, &lt;span class=&#34;math inline&#34;&gt;\(\Delta V_{A}\)&lt;/span&gt; is the change in associative strength between stimulus &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; (e.g. the bell) and the response (e.g. food). &lt;span class=&#34;math inline&#34;&gt;\(\Delta V_{X}\)&lt;/span&gt; has the same interpretation, but refers to stimulus &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; (e.g. the light).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0 \leq \beta_{1} \leq 1\)&lt;/span&gt; is a free parameter (i.e. we estimate it from the data) referred to as the &lt;em&gt;learning rate&lt;/em&gt;. The learning rate controls how quickly updating takes place, where values near 0 and 1 reflect sluggish and rapid learning, respectively. Above, the learning rate is shared across stimuli.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0 \leq \alpha_{A} \leq 1\)&lt;/span&gt; is a free parameter which is determined by the salience of stimulus &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(0 \leq \alpha_{X} \leq 1\)&lt;/span&gt; for stimulus &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Unlike the learning rate, which is shared across stimuli, the salience parameter is specific to each stimulus. Put simply, this just means that learning can occur at different rates depending on the type of stimulus (e.g. I may associate a light with food more quickly than a tone).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt; is described as “the asymptote of associative strength”. This is the upper-limit on how strong the association strength can be. In this way, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt; reflects the value being updated toward by the learning rate (&lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;) and stimulus salience (&lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Lastly, &lt;span class=&#34;math inline&#34;&gt;\(V_{AX}\)&lt;/span&gt; is the total associative strength of the compound stimulus &lt;span class=&#34;math inline&#34;&gt;\(AX\)&lt;/span&gt;. Rescorla and Wagner assume that this is a simple sum of both stimuli strengths:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[V_{AX} = V_{A} + V_{X}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Interpretation of this model is actually quite simple–we update associative strength (&lt;span class=&#34;math inline&#34;&gt;\(V_{*}\)&lt;/span&gt;) for each stimulus by taking steps (&lt;span class=&#34;math inline&#34;&gt;\(\alpha \beta\)&lt;/span&gt;) toward the difference between the asymptote of learning (&lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt;) and the current associative strength of the compund stimulus (&lt;span class=&#34;math inline&#34;&gt;\(V_{AX}\)&lt;/span&gt;). By continually exposing an agent to a tone or bell paired with a reward (or punishment), the agent learns the associative strength of the conditioned and unconditioned stimuli.&lt;/p&gt;
&lt;p&gt;While the original Rescorla-Wagner model was successful for explaining associative learning for classical conditioning paradigms, what of operant conditioning? What if we are interested in how people learn to make decisions? In most current research, we are not interested in knowing how people learn to associate lights or tones with some reward. Instead, we would like a model that can describe how people learn to select the &lt;em&gt;best choice&lt;/em&gt; among &lt;em&gt;multiple choices&lt;/em&gt;. This model would need to explain how people assign values to multiple options as well as how they decide which option to choose. In statistical terms, we want to know how people solve the &lt;a href=&#34;https://en.wikipedia.org/wiki/Multi-armed_bandit&#34;&gt;&lt;em&gt;multi-armed bandit&lt;/em&gt;&lt;/a&gt; problem. In the following section, we will begin to solve this problem.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;current-implementations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;2. Current Implementations&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;http://777click.com/assets/templates/777/img/777banner.jpg&#34; alt=&#34;Figure 2&#34; style=&#34;height: 100%; width: 100%; object-fit: contain&#34;/&gt;&lt;/p&gt;
&lt;p&gt;As a motivating example, we will explore the simple problem of learning the probability that a slot machine will payoff (i.e. that you will win any amount after pulling the lever). To do so, the above equations only need minor modifications. Additionally, we will change the terminology–instead of learning an associative strength, we will now be learning the &lt;em&gt;probability of a winning outcome&lt;/em&gt;. To start, we take the first equation above and write it for a single stimulus &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, but exchange &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; with the probability of observing a win:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\Delta Pr(win) = \alpha \beta (\lambda - Pr(win))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;\(\Delta Pr(win) = Pr(win)_{t+1} - Pr(win)_{t}\)&lt;/span&gt; (where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the current trial), we can re-write the above equation into an iterative form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = Pr(win)_{t} + \alpha \beta (\lambda_{t} - Pr(win)_{t})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Since we are using this model to explain how people learn the probability of a binary outcome, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{t} \in [0, 1]\)&lt;/span&gt; now represents the outcome of slot machine roll on trial &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Now, we drop the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; parameter to simplify the model further. Because we have a single choice option, estimating both &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; would lead to an unidentifiable model. This is because as either one increases, the other can decrease and lead to the same exact predictions. Even with multiple choice options, this problem is still apparent. In current applications of the Rescorla-Wagner rule, we do not include a “salience” parameter. Now, we have:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = Pr(win)_{t} + \beta (\lambda_{t} - Pr(win)_{t})\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The model is now much easier to interpret. The term &lt;span class=&#34;math inline&#34;&gt;\((\lambda_{t} - Pr(win)_{t})\)&lt;/span&gt; can be thought of as the &lt;em&gt;prediction error&lt;/em&gt;–the difference between the actual value &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{t}\)&lt;/span&gt; revealed after the choice was made and the expected value of the choice &lt;span class=&#34;math inline&#34;&gt;\(Pr(win)_{t}\)&lt;/span&gt; for that trial. &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; (the learning rate) then updates the current expectation &lt;span class=&#34;math inline&#34;&gt;\(Pr(win)_{t}\)&lt;/span&gt; in the direction of the prediction error &lt;span class=&#34;math inline&#34;&gt;\((\lambda_{t} - Pr(win)_{t})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;r-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;3. R Example&lt;/h1&gt;
&lt;p&gt;To see the Rescorla-Wagner rule in action, let’s generate some fake data using the binomial distribution and try to estimate the rate parameter using various different values for the learning rate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# For pretty images
library(ggplot2)

# Number of &amp;#39;trials&amp;#39;
num_trials &amp;lt;- 100

# The win rate is 0.7
payoff &amp;lt;- rbinom(n = num_trials, size = 1, prob = 0.7)

# Function that iterates the Rescorla-Wagner rule 
rw_update &amp;lt;- function(lambda, beta, init) {
  # To store expected value for each trial
  Pr_win &amp;lt;- vector(length=length(lambda))
  # Set initial value
  Pr_win[1] &amp;lt;- init
  for (t in 1:(length(lambda)-1)) {
    Pr_win[t+1] &amp;lt;- Pr_win[t] + beta * (lambda[t] - Pr_win[t])
  }
  return(Pr_win)
}

# With initial expectation = 0, try different learning rates
beta_05 &amp;lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_25 &amp;lt;- rw_update(lambda = payoff, beta = 0.25, init = 0)
beta_50 &amp;lt;- rw_update(lambda = payoff, beta = 0.50, init = 0)
beta_75 &amp;lt;- rw_update(lambda = payoff, beta = 0.75, init = 0)
beta_95 &amp;lt;- rw_update(lambda = payoff, beta = 0.95, init = 0)

# Store in data.frame for plotting
all_data &amp;lt;- stack(data.frame(beta_05 = beta_05,
                             beta_25 = beta_25,
                             beta_50 = beta_50,
                             beta_75 = beta_75,
                             beta_95 = beta_95))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:num_trials, 5)
names(all_data)[2]  &amp;lt;- &amp;quot;Beta&amp;quot;

# Plot results
p &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) + 
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&amp;quot;black&amp;quot;, linetype = &amp;quot;longdash&amp;quot;) + 
  ggtitle(&amp;quot;Expected Probability of Winning Outcome&amp;quot;) + 
  xlab(&amp;quot;Trial Number&amp;quot;) + 
  ylab(&amp;quot;Expected Pr(win)&amp;quot;)  
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-04-choice_RL_1_files/figure-html/2017-04-02_fig1-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;It is easy to see that the higher learning rates (i.e. &amp;gt; 0.05) are jumping around the true win rate (0.7, dashed line) quite a bit, whereas setting &lt;span class=&#34;math inline&#34;&gt;\(\beta = 0.05\)&lt;/span&gt; allows for a more stable estimate. Let’s try again with learning rates closer to 0.05.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Learning rates closer to 0.05
beta_01 &amp;lt;- rw_update(lambda = payoff, beta = 0.01, init = 0)
beta_03 &amp;lt;- rw_update(lambda = payoff, beta = 0.03, init = 0)
beta_05 &amp;lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_07 &amp;lt;- rw_update(lambda = payoff, beta = 0.07, init = 0)
beta_09 &amp;lt;- rw_update(lambda = payoff, beta = 0.09, init = 0)

# Store in data.frame for plotting
all_data &amp;lt;- stack(data.frame(beta_01 = beta_01,
                             beta_03 = beta_03,
                             beta_05 = beta_05,
                             beta_07 = beta_07,
                             beta_09 = beta_09))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:num_trials, 5)
names(all_data)[2]  &amp;lt;- &amp;quot;Beta&amp;quot;

# Plot results
p2 &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) +
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&amp;quot;black&amp;quot;, linetype = &amp;quot;longdash&amp;quot;) + 
  ggtitle(&amp;quot;Expected Probability of Winning Outcome&amp;quot;) + 
  xlab(&amp;quot;Trial Number&amp;quot;) + 
  ylab(&amp;quot;Expected Pr(win)&amp;quot;)
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-04-choice_RL_1_files/figure-html/2017-04-02_fig2-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;These results look a bit better. However, it is apparent that setting &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; too low is making the updating very sluggish. If we run more trials, however, we should see the expectation converge.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Number of &amp;#39;trials&amp;#39;
num_trials &amp;lt;- 500

# The win rate is 0.7
payoff &amp;lt;- rbinom(n = num_trials, size = 1, prob = 0.7)

# Learning rates closer to 0.05
beta_01 &amp;lt;- rw_update(lambda = payoff, beta = 0.01, init = 0)
beta_03 &amp;lt;- rw_update(lambda = payoff, beta = 0.03, init = 0)
beta_05 &amp;lt;- rw_update(lambda = payoff, beta = 0.05, init = 0)
beta_07 &amp;lt;- rw_update(lambda = payoff, beta = 0.07, init = 0)
beta_09 &amp;lt;- rw_update(lambda = payoff, beta = 0.09, init = 0)

# Store in data.frame for plotting
all_data &amp;lt;- stack(data.frame(beta_01 = beta_01,
                             beta_03 = beta_03,
                             beta_05 = beta_05,
                             beta_07 = beta_07,
                             beta_09 = beta_09))
# Add trial 
all_data[[&amp;quot;trial&amp;quot;]] &amp;lt;- rep(1:num_trials, 5)
names(all_data)[2]  &amp;lt;- &amp;quot;Beta&amp;quot;

# Plot results
p2 &amp;lt;- ggplot(all_data, aes(x = trial, y = values, color = Beta)) +
  geom_line() + 
  geom_hline(yintercept = 0.7, colour=&amp;quot;black&amp;quot;, linetype = &amp;quot;longdash&amp;quot;) + 
  ggtitle(&amp;quot;Expected Probability of Winning Outcome&amp;quot;) + 
  xlab(&amp;quot;Trial Number&amp;quot;) + 
  ylab(&amp;quot;Expected Pr(win)&amp;quot;)
p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://haines-lab.com/post/2017-04-04-choice_RL_1_files/figure-html/2017-04-02_fig3-1.svg&#34; width=&#34;672&#34; style=&#39;height: 100%; width: 100%; object-fit: contain&#39; /&gt;&lt;/p&gt;
&lt;p&gt;Over 500 trials, the expected value for the win rate converges to the true win rate, 70%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;4. Summary&lt;/h1&gt;
&lt;p&gt;In this post, we reviewed the original Rescorla-Wagner updating rule (a.k.a. the &lt;em&gt;Delta Rule&lt;/em&gt;) and explored its contemporary instantiation. I have shown that the Delta Rule can be used to estimate the win rate of a slot machine on a trial-by-trial basis. While this example may first appear trivial (e.g. why not just take the average of all past outcomes?), we will explore more practical usages in later posts. For now, try playing with the above code yourself! If you want a slightly more challenging problem, try finding the solution to the following question:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;How should you change the learning rate so that the expected win rate is always the average of all past outcomes?&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hint –&amp;gt; With some simple algebra, the Delta Rule can be re-written as follows:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Pr(win)_{t+1} = (1 - \beta) \cdot Pr(win)_{t} + \beta \cdot \lambda_{t}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Happy solving! See the answer in the next post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Revealing Neurocomputational Mechanisms of Reinforcement Learning and Decision-Making With the hBayesDM Package</title>
      <link>http://haines-lab.com/publication/hbayesdm/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/publication/hbayesdm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A new reinforcement learning model of the Iowa Gambling Task</title>
      <link>http://haines-lab.com/talk/cog_brown_1/</link>
      <pubDate>Wed, 30 Nov 2016 12:10:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/talk/cog_brown_1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>easyml</title>
      <link>http://haines-lab.com/project/easyml/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/project/easyml/</guid>
      <description>&lt;p&gt;Currently under construction.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hBayesDM</title>
      <link>http://haines-lab.com/project/hbayesdm/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/project/hbayesdm/</guid>
      <description>&lt;p&gt;Currently under construction.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
