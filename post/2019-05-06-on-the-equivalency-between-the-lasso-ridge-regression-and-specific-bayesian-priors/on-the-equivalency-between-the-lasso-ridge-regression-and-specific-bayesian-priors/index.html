<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.4.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Nathaniel Haines" />

  
  
  
    
  
  <meta name="description" content="Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:" />

  
  <link rel="alternate" hreflang="en-us" href="http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/" />

  
  
  
    <meta name="theme-color" content="hsl(339, 90%, 68%)" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'" disabled>
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'">
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.483823b49802e3bfc12a727dbc71499e.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106994238-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-106994238-1', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Computational Psychology" />
  <meta property="og:url" content="http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/" />
  <meta property="og:title" content="On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression | Computational Psychology" />
  <meta property="og:description" content="Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:" /><meta property="og:image" content="http://haines-lab.com/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="http://haines-lab.com/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2019-05-06T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2019-05-06T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/"
  },
  "headline": "On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression",
  
  "datePublished": "2019-05-06T00:00:00Z",
  "dateModified": "2019-05-06T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Nathaniel Haines"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Computational Psychology",
    "logo": {
      "@type": "ImageObject",
      "url": "http://haines-lab.com/media/icon_hu88408da40bfce0858e3c9c463b923e62_76530_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:"
}
</script>

  

  

  

  





  <title>On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression | Computational Psychology</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper  dark " data-wc-page-id="2993693cf47b8f747b7625e8212fca91" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.b6ac29faab89ee14db88f0bed7aa6622.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Computational Psychology</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Computational Psychology</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#featured"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://twitter.com/nate__haines" data-toggle="tooltip" data-placement="bottom" title="Follow me on Twitter" target="_blank" rel="noopener" aria-label="Follow me on Twitter">
                <i class="fab fa-twitter" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    May 6, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    28 min read
  </span>
  

  
  
  
  
  
  
    <span class="middot-divider"></span>
    <a href="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/statistical-modeling/">Statistical Modeling</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/plotly-binding/plotly.js"></script>
<script src="/rmarkdown-libs/typedarray/typedarray.min.js"></script>
<script src="/rmarkdown-libs/jquery/jquery.min.js"></script>
<link href="/rmarkdown-libs/crosstalk/css/crosstalk.min.css" rel="stylesheet" />
<script src="/rmarkdown-libs/crosstalk/js/crosstalk.min.js"></script>
<link href="/rmarkdown-libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="/rmarkdown-libs/plotly-main/plotly-latest.min.js"></script>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>In this post, we will explore frequentist and Bayesian analogues of <em>regularized/penalized linear regression</em> models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:</p>
<p><span class="math display">\[y = \beta_{0}+X\beta + \epsilon\tag{1}\]</span>
where <span class="math inline">\(\epsilon\)</span> is the error, which is normally distributed as:</p>
<p><span class="math display">\[\epsilon \sim  \mathcal{N}(0, \sigma)\tag{2}\]</span>
Unlike these traditional linear regression models, regularized linear regression models produce <strong>biased estimates</strong> for the <span class="math inline">\(\beta\)</span> weights. Specifically, both frequentist and Bayesian regularized linear regression models pool information across <span class="math inline">\(\beta\)</span> weights, resulting in regression toward a common mean. When the common mean is centered at 0, this pooling of information produces more conservative estimates for each <span class="math inline">\(\beta\)</span> weight (they are biased toward 0). In contrast, traditional linear regression models assume that <span class="math inline">\(\beta\)</span> weights share no group-level information (i.e. they are independent), which leads to so-called <strong>unbiased estimates</strong>.</p>
<p><strong>So then, why are these models—which produce biased estimates—becoming increasingly popular throughout the social and behavioral sciences?</strong></p>
<div id="learning-objectives" class="section level2">
<h2>Learning Objectives</h2>
<p>The current post seeks to show that we actually want biased estimates in many contexts. In doing so, we will also explore associations between frequentist and Bayesian regularization. Therefore, the <strong>learning objectives</strong> of the current post are to develop an understanding of:</p>
<ol style="list-style-type: decimal">
<li>why so-called <strong>biased</strong> estimates are actually good for science,</li>
<li>the difference between traditional and regularized linear regression models, and</li>
<li>the correspondence between frequentist regularization and hierarchical Bayesian regression</li>
</ol>
</div>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<p>For this post, we will use a college-admissions dataset that is freely available from <a href="https://www.kaggle.com/mohansacharya/graduate-admissions">Kaggle</a>. Make sure to download the .csv file named <code>Admission_Predict_Ver1.1.csv</code>. This dataset contains 500 observations (i.e. rows) and 9 total variables (i.e. columns). 1 of these columns is a subject ID, which we will note use for modeling. Taken directly from the link:</p>
<p><em>“The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are : 1. GRE Scores ( out of 340 ) 2. TOEFL Scores ( out of 120 ) 3. University Rating ( out of 5 ) 4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 ) 5. Undergraduate GPA ( out of 10 ) 6. Research Experience ( either 0 or 1 ) 7. Chance of Admit ( ranging from 0 to 1 )”</em></p>
<p>Our goal is to predict the likelihood of being admitted to graduate school (<em>Chance of Admit</em>), given the other variables, which we will now refer to as <strong>predictors</strong>.</p>
<div id="getting-started" class="section level2">
<h2>Getting Started</h2>
<p>First off, let’s load the libraries that we will use:</p>
<pre class="r"><code># For traditional LASSO/Ridge regression
library(glmnet)</code></pre>
<pre><code>## Warning: package &#39;glmnet&#39; was built under R version 4.1.1</code></pre>
<pre class="r"><code># For Bayesian modeling
library(rstan)
# For data wrangling/plotting
library(dplyr)
library(tidyr)
library(foreach)
library(ggplot2)
library(bayesplot) # Visualizing posteriors 
library(akima)     # for 3D plotting</code></pre>
<pre><code>## Warning: package &#39;akima&#39; was built under R version 4.1.1</code></pre>
<pre class="r"><code>library(plotly)    # for 3D plotting</code></pre>
<pre><code>## Warning: package &#39;plotly&#39; was built under R version 4.1.1</code></pre>
<p>Next, we need to read in the data. Assuming that you have already downloaded the data from the link above, we can read it into R as follows:</p>
<pre class="r"><code># I was lazy and just left this in the downloads folder...
grad_dat &lt;- read.csv(&quot;~/Downloads/Admission_Predict_Ver1.1.csv&quot;)

# View first few observations 
tbl_df(grad_dat)</code></pre>
<pre><code>## Warning: `tbl_df()` was deprecated in dplyr 1.0.0.
## Please use `tibble::as_tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## # A tibble: 500 × 9
##    Serial.No. GRE.Score TOEFL.Score University.Rating   SOP   LOR  CGPA Research
##         &lt;int&gt;     &lt;int&gt;       &lt;int&gt;             &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;int&gt;
##  1          1       337         118                 4   4.5   4.5  9.65        1
##  2          2       324         107                 4   4     4.5  8.87        1
##  3          3       316         104                 3   3     3.5  8           1
##  4          4       322         110                 3   3.5   2.5  8.67        1
##  5          5       314         103                 2   2     3    8.21        0
##  6          6       330         115                 5   4.5   3    9.34        1
##  7          7       321         109                 3   3     4    8.2         1
##  8          8       308         101                 2   3     4    7.9         0
##  9          9       302         102                 1   2     1.5  8           0
## 10         10       323         108                 3   3.5   3    8.6         0
## # … with 490 more rows, and 1 more variable: Chance.of.Admit &lt;dbl&gt;</code></pre>
</div>
<div id="creating-training-and-test-sets" class="section level2">
<h2>Creating Training and Test Sets</h2>
<p>Before really looking at the data, we need to separate out the training and testing portions. The original competition version of data uploaded to Kaggle only included the first 400 observations, and competitors had to make predictions on the remaining 100 observations before the actual outcomes (i.e. likelihood of getting into graduate school) were released.</p>
<p>To show off the benefits of regularized over traditional methods, we will only train our models on <strong>the first 20 observations, and make predictions on the remaining 480</strong>. In these low data settings—which are common to many areas of social and behavioral science (e.g., psychology, neuroscience, human ecology, etc.)—regularized regression models show clear advantages over traditional regression models. In fact, I will go as far as claiming that <strong>regularized models should be the default choice in most areas of science</strong>, and the following examples should explain why.</p>
<pre class="r"><code># Training data (used to fit model)
train_dat &lt;- grad_dat[1:20,] # Only first 20 for training

# Testing data (used to test model generalizability)
test_dat &lt;- grad_dat[21:500,] # Testing on the rest</code></pre>
<p>Now that we have the data read in and separated, it would be useful to visualize our training data to get a sense of what we are working with. A correlation matrix should do a good job here:</p>
<pre class="r"><code># Plotting correlation matrix
cor(train_dat) %&gt;%
  as.data.frame() %&gt;%
  mutate(Var1 = factor(row.names(.), levels=row.names(.))) %&gt;% # For nice order
  gather(Var2, Correlation, 1:9) %&gt;%
  ggplot(aes(reorder(Var2, Correlation), # Reorder to visualize
             reorder(Var1, -Correlation), fill = Correlation)) +
  geom_tile() +
  scale_fill_continuous(type = &quot;viridis&quot;) +
  xlab(&quot;Variable&quot;) +
  ylab(&quot;Variable&quot;) +
  theme_minimal(base_size = 15) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig0-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>This correlation matrix shows us that <code>Serial.No</code> is not really correlated with any of the other 8 variables. This makes sense, given that <code>Serial.No</code> is the subject ID (we will not use that in our models). Otherwise, it appears that there is a moderate amount of collinearity amoung the predictor and outcome variables.</p>
</div>
<div id="traditional-regression" class="section level2">
<h2>Traditional Regression</h2>
<p>Traditional linear regression models seek to mimimize the squared error between predicted and actual observations. Formally, we can represent this with a <em>loss function</em> of the following form:</p>
<p><span class="math display">\[\underset{\boldsymbol{\beta}}{argmin}\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2\tag{3}\]</span></p>
<p>where <span class="math inline">\(y_{i}\)</span> is the outcome variable (probability of acceptance in our example) for observation <span class="math inline">\(i\)</span>, <span class="math inline">\(n\)</span> is the total number of observations (20 students in our training data example), <span class="math inline">\(p\)</span> is the number of predictors (7 in our example), <span class="math inline">\(\beta_{0}\)</span> is the intercept of the model, and <span class="math inline">\(\beta_{1,2,...,j}\)</span> are the weights (i.e. slopes) for each of the <span class="math inline">\(j\)</span> predictor variables. Under the assumption of normality, both ordinary least squares and maximum likelihood estimation of <span class="math inline">\(\beta\)</span> weights in equation 3 will offer the same results—from here on, we will refer to these methods of minimizing equation 3 as <em>traditional linear regression</em>.</p>
<p>Below, we will fit a traditional linear regression model on the training set (which contains 20 observations total), and we will see how well it predicts the graduate school acceptance probability (<span class="math inline">\(\text{Pr}(Acceptance)\)</span>) for each of the remaining 480 observations. We begin by scaling all variables—that is, we mean-center and divide each column by its own standard deviation (SD). Then, we apply the same standardization to the test data, followed by fitting the traditional model and making test-set predictions.</p>
<pre class="r"><code># Scale training data (and get rid of ID)
train_scale &lt;- scale(train_dat[,2:9])

# Find means and SDs of training data variables
means &lt;- attributes(train_scale)$`scaled:center`
SDs &lt;- attributes(train_scale)$`scaled:scale`

# Scale test data using training data summary stats (no cheating!)
test_scale &lt;- scale(test_dat[,-1], center = means, scale = SDs)

# Fit linear regression
fit_lr &lt;- lm(Chance.of.Admit ~ ., data = as.data.frame(train_scale))

# Generate test-set predictions with linear regression
y_pred_lr &lt;- predict(fit_lr, newdata = as.data.frame(test_scale[,-8]))

# Plot cor(predicted, actual)
qplot(x = y_pred_lr, y = test_scale[,8],
      main = paste0(&quot;Traditional Linear Regression\n&quot;, 
                    &quot;r = &quot;, round(cor(test_scale[,8], y_pred_lr), 2))) +
  xlab(&quot;Model Predicted Pr(Acceptance)&quot;) +
  ylab(&quot;Actual Pr(Acceptance)&quot;) +
  theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig1-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Wow, not bad at all! Even with only 20 observations, the correlation between predicted and actual probability of acceptance for the remaining 480 observations is $r = $ <span class="math inline">\(0.76\)</span>. This suggests that probability of acceptance is rather well described as a simple linear combination of the predictors.</p>
</div>
<div id="regularized-regression" class="section level2">
<h2>Regularized Regression</h2>
<p>As described above, regularized linear regression models aim to estimate more conservative values for the <span class="math inline">\(\beta\)</span> weights in a model, and this is true for both frequentist and Bayesian versions of regularization. While there are many methods that can be used to regularize your estimation procedure, we will focus specifically on two popular forms—namely, ridge and LASSO regression. We start below by describing each regression generally, and then proceed to implement both the frequentist and Bayesian versions.</p>
<div id="ridge-regression" class="section level3">
<h3>Ridge Regression</h3>
<p>The extention from traditional to ridge regression is actually very straightforward! Specifically, we modify the loss function (equation 3) to include a <strong>penalty term</strong> for model complexity, where model complexity is operationalized as the sum of squared <span class="math inline">\(\beta\)</span> weights:</p>
<p><span class="math display">\[\underbrace{\underset{\boldsymbol{\beta}}{argmin}\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2}_{\text{Traditional Loss Function}} + \underbrace{\lambda\sum_{j=1}^{p}\beta_{j}^2}_{\text{Ridge Penalty}}\tag{4}\]</span>
Here, <span class="math inline">\(\lambda~(0&lt;\lambda&lt;\infty)\)</span> is a penalty parameter, which controls how much regularization we would like in the model. To gain an intuition for the behavior of this model, think of what happens at the very extremes of <span class="math inline">\(\lambda\)</span>. For example, when <span class="math inline">\(\lambda = 0\)</span>, what happens to equation 4? Well, the model simply reduces to equation 3, and we are back to traditional regression! What about as <span class="math inline">\(\lambda \rightarrow \infty\)</span>? In that case, any non-zero values for <span class="math inline">\(\beta\)</span> will lead to an infinitely large penalty—then, the only solution to equation 4 is for all <span class="math inline">\(\beta\)</span> weights to be equal to 0, indicating no learning from the data at all.</p>
<p>Therefore, we can think of <span class="math inline">\(\lambda\)</span> as a parameter that <strong>controls how much we learn from the data</strong>, with smaller and larger values leading to more and less learning, respectively. Note that parameters that control how much we learn from data are typically called <strong>hyper-parameters</strong>. Framed in these terms, we can view traditional regression methods as those that maximally learn from the data, and regularized regression models as those that restrict learning from the data. It is in this way that traditional regression produces <em>unbiased</em> estimate of the <span class="math inline">\(\beta\)</span> weights. That is, <span class="math inline">\(\beta\)</span> weights are unbiased with respect to the information available in the training data. However, when data are noisy (i.e. contain large amounts of variability), such <em>unbiased</em> estimates will be unduly influenced by noise. Thus, although traditional regression offers <em>unbiased</em> estimates, it is also succeptable to estimating large magnitude <span class="math inline">\(\beta\)</span> weights based purely on noise within the data. Ridge regression minimizes the potential learning from noise by penalizing the model for the squared sum of all the <span class="math inline">\(\beta\)</span> weights. The squaring of the <span class="math inline">\(\beta\)</span> weights encodes our knowledge that large-magnitude <span class="math inline">\(\beta\)</span> weights are much less likely than small-magnitude <span class="math inline">\(\beta\)</span> weights (given that all our variables are on the same scale after standardization). Practically, this means that if a traditional regression would give us a very large magnitude <span class="math inline">\(\beta\)</span> weight, when data are highly variable, ridge regression will <em>bias such large <span class="math inline">\(\beta\)</span> estimates toward 0</em>.</p>
<p>We can actually visualize the effects of the ridge penalty on <span class="math inline">\(\beta\)</span> weights by plotting out the ridge penalty from equation 4. Specifically, the below plot shows the resulting penalty (where 0 is no penalty and increasingly negative numbers are stronger penalties) for the 2-dimensional case, where we only have 2 predictors. Importantly, the plot also includes the penalty functions for varying settings of <span class="math inline">\(\lambda \in \{0, .5, 1.5\}\)</span>. Note that the flat surface is when <span class="math inline">\(\lambda = 0\)</span>, which leads to no penalization.</p>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"visdat":{"15060540265fc":["function () ","plotlyVisDat"]},"cur_data":"15060540265fc","attrs":{"15060540265fc":{"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"opacity":0.5,"inherit":true},"15060540265fc.1":{"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"inherit":true},"15060540265fc.2":{"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Beta_1"},"yaxis":{"title":"Beta_2"},"zaxis":{"title":"Penalty"}},"title":"Ridge Penalty Contour","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"ridge_p1$z<br />ridge_p2$z<br />ridge_p3$z","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,null,null,null,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0,0,0,0]],"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"opacity":0.5,"frame":null},{"colorbar":{"title":"ridge_p1$z<br />ridge_p2$z<br />ridge_p3$z","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[9,8.55038461538462,8.12442307692308,7.72211538461538,7.34346153846154,6.98846153846154,6.65711538461538,6.34942307692308,6.06538461538462,5.805,5.56826923076923,5.35519230769231,5.16576923076923,5,4.85807692307692,4.73980769230769,4.64519230769231,4.57423076923077,4.52692307692308,4.50326923076923,4.50326923076923,4.52692307692308,4.57423076923077,4.64519230769231,4.73980769230769,4.85807692307692,5,5.16576923076923,5.35519230769231,5.56826923076923,5.805,6.06538461538462,6.34942307692308,6.65711538461539,6.98846153846154,7.34346153846154,7.72211538461538,8.12442307692308,8.55038461538462,9],[8.55038461538462,8.10076923076923,7.67480769230769,7.2725,6.89384615384615,6.53884615384615,6.2075,5.89980769230769,5.61576923076923,5.35538461538462,5.11865384615385,4.90557692307692,4.71615384615385,4.55038461538462,4.40846153846154,4.29019230769231,4.19557692307692,4.12461538461539,4.07730769230769,4.05365384615385,4.05365384615385,4.07730769230769,4.12461538461539,4.19557692307692,4.29019230769231,4.40846153846154,4.55038461538462,4.71615384615385,4.90557692307692,5.11865384615385,5.35538461538462,5.61576923076923,5.89980769230769,6.2075,6.53884615384615,6.89384615384616,7.2725,7.67480769230769,8.10076923076923,8.55038461538462],[8.12442307692308,7.67480769230769,7.24884615384615,6.84653846153846,6.46788461538462,6.11288461538462,5.78153846153846,5.47384615384615,5.18980769230769,4.92942307692308,4.69269230769231,4.47961538461538,4.29019230769231,4.12442307692308,3.9825,3.86423076923077,3.76961538461539,3.69865384615385,3.65134615384615,3.62769230769231,3.62769230769231,3.65134615384615,3.69865384615385,3.76961538461539,3.86423076923077,3.9825,4.12442307692308,4.29019230769231,4.47961538461539,4.69269230769231,4.92942307692308,5.18980769230769,5.47384615384616,5.78153846153846,6.11288461538462,6.46788461538462,6.84653846153846,7.24884615384615,7.67480769230769,8.12442307692308],[7.72211538461538,7.2725,6.84653846153846,6.44423076923077,6.06557692307692,5.71057692307692,5.37923076923077,5.07153846153846,4.7875,4.52711538461539,4.29038461538462,4.07730769230769,3.88788461538461,3.72211538461538,3.58019230769231,3.46192307692308,3.36730769230769,3.29634615384615,3.24903846153846,3.22538461538462,3.22538461538462,3.24903846153846,3.29634615384615,3.36730769230769,3.46192307692308,3.58019230769231,3.72211538461538,3.88788461538462,4.07730769230769,4.29038461538462,4.52711538461539,4.7875,5.07153846153846,5.37923076923077,5.71057692307692,6.06557692307692,6.44423076923077,6.84653846153846,7.2725,7.72211538461538],[7.34346153846154,6.89384615384615,6.46788461538462,6.06557692307692,5.68692307692308,5.33192307692308,5.00057692307692,4.69288461538462,4.40884615384615,4.14846153846154,3.91173076923077,3.69865384615385,3.50923076923077,3.34346153846154,3.20153846153846,3.08326923076923,2.98865384615385,2.91769230769231,2.87038461538462,2.84673076923077,2.84673076923077,2.87038461538462,2.91769230769231,2.98865384615385,3.08326923076923,3.20153846153846,3.34346153846154,3.50923076923077,3.69865384615385,3.91173076923077,4.14846153846154,4.40884615384615,4.69288461538462,5.00057692307692,5.33192307692308,5.68692307692308,6.06557692307692,6.46788461538462,6.89384615384615,7.34346153846154],[6.98846153846154,6.53884615384615,6.11288461538462,5.71057692307692,5.33192307692308,4.97692307692308,4.64557692307692,4.33788461538462,4.05384615384615,3.79346153846154,3.55673076923077,3.34365384615385,3.15423076923077,2.98846153846154,2.84653846153846,2.72826923076923,2.63365384615385,2.56269230769231,2.51538461538462,2.49173076923077,2.49173076923077,2.51538461538462,2.56269230769231,2.63365384615385,2.72826923076923,2.84653846153846,2.98846153846154,3.15423076923077,3.34365384615385,3.55673076923077,3.79346153846154,4.05384615384615,4.33788461538462,4.64557692307693,4.97692307692308,5.33192307692308,5.71057692307692,6.11288461538462,6.53884615384615,6.98846153846154],[6.65711538461538,6.2075,5.78153846153846,5.37923076923077,5.00057692307692,4.64557692307692,4.31423076923077,4.00653846153846,3.7225,3.46211538461538,3.22538461538461,3.01230769230769,2.82288461538461,2.65711538461538,2.51519230769231,2.39692307692308,2.30230769230769,2.23134615384615,2.18403846153846,2.16038461538461,2.16038461538461,2.18403846153846,2.23134615384615,2.30230769230769,2.39692307692308,2.51519230769231,2.65711538461538,2.82288461538462,3.01230769230769,3.22538461538461,3.46211538461538,3.7225,4.00653846153846,4.31423076923077,4.64557692307692,5.00057692307692,5.37923076923077,5.78153846153846,6.2075,6.65711538461538],[6.34942307692308,5.89980769230769,5.47384615384615,5.07153846153846,4.69288461538462,4.33788461538462,4.00653846153846,3.69884615384615,3.41480769230769,3.15442307692308,2.91769230769231,2.70461538461538,2.51519230769231,2.34942307692308,2.2075,2.08923076923077,1.99461538461538,1.92365384615385,1.87634615384615,1.85269230769231,1.85269230769231,1.87634615384615,1.92365384615385,1.99461538461538,2.08923076923077,2.2075,2.34942307692308,2.51519230769231,2.70461538461539,2.91769230769231,3.15442307692308,3.41480769230769,3.69884615384615,4.00653846153846,4.33788461538462,4.69288461538462,5.07153846153846,5.47384615384615,5.89980769230769,6.34942307692308],[6.06538461538462,5.61576923076923,5.18980769230769,4.7875,4.40884615384615,4.05384615384615,3.7225,3.41480769230769,3.13076923076923,2.87038461538462,2.63365384615385,2.42057692307692,2.23115384615385,2.06538461538462,1.92346153846154,1.80519230769231,1.71057692307692,1.63961538461538,1.59230769230769,1.56865384615385,1.56865384615385,1.59230769230769,1.63961538461538,1.71057692307692,1.80519230769231,1.92346153846154,2.06538461538462,2.23115384615385,2.42057692307692,2.63365384615385,2.87038461538462,3.13076923076923,3.41480769230769,3.7225,4.05384615384615,4.40884615384615,4.7875,5.18980769230769,5.61576923076923,6.06538461538462],[5.805,5.35538461538462,4.92942307692308,4.52711538461539,4.14846153846154,3.79346153846154,3.46211538461538,3.15442307692308,2.87038461538462,2.61,2.37326923076923,2.16019230769231,1.97076923076923,1.805,1.66307692307692,1.54480769230769,1.45019230769231,1.37923076923077,1.33192307692308,1.30826923076923,1.30826923076923,1.33192307692308,1.37923076923077,1.45019230769231,1.54480769230769,1.66307692307692,1.805,1.97076923076923,2.16019230769231,2.37326923076923,2.61,2.87038461538462,3.15442307692308,3.46211538461539,3.79346153846154,4.14846153846154,4.52711538461538,4.92942307692308,5.35538461538462,5.805],[5.56826923076923,5.11865384615385,4.69269230769231,4.29038461538462,3.91173076923077,3.55673076923077,3.22538461538461,2.91769230769231,2.63365384615385,2.37326923076923,2.13653846153846,1.92346153846154,1.73403846153846,1.56826923076923,1.42634615384615,1.30807692307692,1.21346153846154,1.1425,1.09519230769231,1.07153846153846,1.07153846153846,1.09519230769231,1.1425,1.21346153846154,1.30807692307692,1.42634615384615,1.56826923076923,1.73403846153846,1.92346153846154,2.13653846153846,2.37326923076923,2.63365384615385,2.91769230769231,3.22538461538462,3.55673076923077,3.91173076923077,4.29038461538462,4.69269230769231,5.11865384615385,5.56826923076923],[5.35519230769231,4.90557692307692,4.47961538461539,4.07730769230769,3.69865384615385,3.34365384615385,3.01230769230769,2.70461538461538,2.42057692307692,2.16019230769231,1.92346153846154,1.71038461538461,1.52096153846154,1.35519230769231,1.21326923076923,1.095,1.00038461538461,0.929423076923077,0.882115384615384,0.858461538461538,0.858461538461538,0.882115384615384,0.929423076923077,1.00038461538462,1.095,1.21326923076923,1.35519230769231,1.52096153846154,1.71038461538462,1.92346153846154,2.16019230769231,2.42057692307692,2.70461538461538,3.01230769230769,3.34365384615385,3.69865384615385,4.07730769230769,4.47961538461538,4.90557692307692,5.35519230769231],[5.16576923076923,4.71615384615385,4.29019230769231,3.88788461538461,3.50923076923077,3.15423076923077,2.82288461538461,2.51519230769231,2.23115384615385,1.97076923076923,1.73403846153846,1.52096153846154,1.33153846153846,1.16576923076923,1.02384615384615,0.905576923076923,0.810961538461538,0.74,0.692692307692308,0.669038461538461,0.669038461538461,0.692692307692307,0.74,0.810961538461539,0.905576923076923,1.02384615384615,1.16576923076923,1.33153846153846,1.52096153846154,1.73403846153846,1.97076923076923,2.23115384615385,2.51519230769231,2.82288461538462,3.15423076923077,3.50923076923077,3.88788461538461,4.29019230769231,4.71615384615385,5.16576923076923],[5,4.55038461538462,4.12442307692308,3.72211538461538,3.34346153846154,2.98846153846154,2.65711538461538,2.34942307692308,2.06538461538462,1.805,1.56826923076923,1.35519230769231,1.16576923076923,1,0.858076923076923,0.739807692307692,0.645192307692308,0.574230769230769,0.526923076923077,0.503269230769231,0.503269230769231,0.526923076923077,0.574230769230769,0.645192307692308,0.739807692307692,0.858076923076923,1,1.16576923076923,1.35519230769231,1.56826923076923,1.805,2.06538461538462,2.34942307692308,2.65711538461539,2.98846153846154,3.34346153846154,3.72211538461538,4.12442307692308,4.55038461538462,5],[4.85807692307692,4.40846153846154,3.9825,3.58019230769231,3.20153846153846,2.84653846153846,2.51519230769231,2.2075,1.92346153846154,1.66307692307692,1.42634615384615,1.21326923076923,1.02384615384615,0.858076923076923,0.716153846153846,0.597884615384615,0.50326923076923,0.432307692307692,0.385,0.361346153846154,0.361346153846154,0.385,0.432307692307692,0.503269230769231,0.597884615384615,0.716153846153846,0.858076923076923,1.02384615384615,1.21326923076923,1.42634615384615,1.66307692307692,1.92346153846154,2.2075,2.51519230769231,2.84653846153846,3.20153846153846,3.58019230769231,3.9825,4.40846153846154,4.85807692307692],[4.73980769230769,4.29019230769231,3.86423076923077,3.46192307692308,3.08326923076923,2.72826923076923,2.39692307692308,2.08923076923077,1.80519230769231,1.54480769230769,1.30807692307692,1.095,0.905576923076923,0.739807692307692,0.597884615384615,0.479615384615384,0.385,0.314038461538461,0.266730769230769,0.243076923076923,0.243076923076923,0.266730769230769,0.314038461538461,0.385,null,0.597884615384615,0.739807692307692,0.905576923076923,1.095,1.30807692307692,1.54480769230769,1.80519230769231,2.08923076923077,2.39692307692308,2.72826923076923,3.08326923076923,3.46192307692308,null,4.29019230769231,4.73980769230769],[4.64519230769231,4.19557692307692,3.76961538461539,3.36730769230769,2.98865384615385,2.63365384615385,2.30230769230769,1.99461538461538,1.71057692307692,1.45019230769231,1.21346153846154,1.00038461538461,0.810961538461538,0.645192307692308,0.50326923076923,0.385,0.290384615384615,0.219423076923077,0.172115384615385,0.148461538461538,0.148461538461538,0.172115384615385,0.219423076923077,0.290384615384615,0.385,0.503269230769231,0.645192307692308,0.810961538461539,1.00038461538462,1.21346153846154,1.45019230769231,1.71057692307692,1.99461538461539,2.30230769230769,2.63365384615385,2.98865384615385,3.36730769230769,3.76961538461538,4.19557692307693,4.64519230769231],[4.57423076923077,4.12461538461539,3.69865384615385,3.29634615384615,2.91769230769231,2.56269230769231,2.23134615384615,1.92365384615385,1.63961538461538,1.37923076923077,1.1425,0.929423076923077,0.74,0.574230769230769,0.432307692307692,0.314038461538461,0.219423076923077,0.148461538461538,0.101153846153846,0.0775,0.0775,0.101153846153846,0.148461538461539,0.219423076923077,0.314038461538462,0.432307692307692,0.574230769230769,0.74,0.929423076923078,1.1425,1.37923076923077,1.63961538461538,1.92365384615385,2.23134615384616,2.56269230769231,2.91769230769231,3.29634615384615,3.69865384615385,4.12461538461539,4.57423076923077],[4.52692307692308,4.07730769230769,3.65134615384615,3.24903846153846,2.87038461538462,2.51538461538462,2.18403846153846,1.87634615384615,1.59230769230769,1.33192307692308,1.09519230769231,0.882115384615384,0.692692307692308,0.526923076923077,0.385,0.266730769230769,0.172115384615385,0.101153846153846,0.0538461538461539,0.0301923076923077,0.0301923076923077,0.0538461538461539,0.101153846153846,0.172115384615385,0.266730769230769,0.385,0.526923076923077,0.692692307692308,0.882115384615386,1.09519230769231,1.33192307692308,1.59230769230769,1.87634615384615,2.18403846153846,2.51538461538462,2.87038461538462,3.24903846153846,3.65134615384615,4.07730769230769,4.52692307692308],[4.50326923076923,4.05365384615385,3.62769230769231,3.22538461538462,2.84673076923077,2.49173076923077,2.16038461538461,1.85269230769231,1.56865384615385,1.30826923076923,1.07153846153846,0.858461538461538,0.669038461538461,0.503269230769231,0.361346153846154,0.243076923076923,0.148461538461538,0.0775,0.0301923076923077,0.00653846153846149,0.00653846153846153,0.0301923076923077,0.0775000000000001,0.148461538461539,0.243076923076923,0.361346153846154,0.503269230769231,0.669038461538462,0.858461538461539,1.07153846153846,1.30826923076923,1.56865384615385,1.85269230769231,2.16038461538462,2.49173076923077,2.84673076923077,3.22538461538461,3.62769230769231,4.05365384615385,4.50326923076923],[4.50326923076923,4.05365384615385,3.62769230769231,3.22538461538462,2.84673076923077,2.49173076923077,2.16038461538461,1.85269230769231,1.56865384615385,1.30826923076923,1.07153846153846,0.858461538461538,0.669038461538461,0.503269230769231,0.361346153846154,0.243076923076923,0.148461538461538,0.0775,0.0301923076923077,0.00653846153846153,0.00653846153846156,0.0301923076923077,0.0775000000000002,0.148461538461539,0.243076923076923,0.361346153846154,0.503269230769231,0.669038461538462,0.858461538461539,1.07153846153846,1.30826923076923,1.56865384615385,1.85269230769231,2.16038461538462,2.49173076923077,2.84673076923077,3.22538461538462,3.62769230769231,4.05365384615385,4.50326923076923],[4.52692307692308,4.07730769230769,3.65134615384615,3.24903846153846,2.87038461538462,2.51538461538462,2.18403846153846,1.87634615384615,1.59230769230769,1.33192307692308,1.09519230769231,0.882115384615384,0.692692307692308,0.526923076923077,0.385,0.266730769230769,0.172115384615385,0.101153846153846,0.0538461538461539,0.0301923076923077,0.0301923076923077,0.0538461538461539,0.101153846153846,0.172115384615385,0.266730769230769,0.385,0.526923076923077,0.692692307692308,0.882115384615386,1.09519230769231,1.33192307692308,1.59230769230769,1.87634615384615,2.18403846153846,2.51538461538462,2.87038461538462,3.24903846153846,3.65134615384615,4.07730769230769,4.52692307692308],[4.57423076923077,4.12461538461539,3.69865384615385,3.29634615384615,2.91769230769231,2.56269230769231,2.23134615384615,1.92365384615385,1.63961538461538,1.37923076923077,1.1425,0.929423076923077,0.74,0.574230769230769,0.432307692307692,0.314038461538462,0.219423076923077,0.148461538461539,0.101153846153846,0.0775000000000001,0.0775000000000002,0.101153846153846,0.148461538461539,0.219423076923077,0.314038461538462,0.432307692307693,0.574230769230769,0.740000000000001,0.929423076923078,1.1425,1.37923076923077,1.63961538461538,1.92365384615385,2.23134615384616,2.56269230769231,2.91769230769231,3.29634615384615,3.69865384615385,4.12461538461539,4.57423076923077],[4.64519230769231,4.19557692307692,3.76961538461539,3.36730769230769,2.98865384615385,2.63365384615385,2.30230769230769,1.99461538461538,1.71057692307692,1.45019230769231,1.21346153846154,1.00038461538462,0.810961538461539,0.645192307692308,0.503269230769231,0.385,0.290384615384615,0.219423076923077,0.172115384615385,0.148461538461539,0.148461538461539,0.172115384615385,0.219423076923077,0.290384615384616,0.385,0.503269230769231,0.645192307692308,0.810961538461539,1.00038461538462,1.21346153846154,1.45019230769231,1.71057692307692,1.99461538461539,2.30230769230769,2.63365384615385,2.98865384615385,3.36730769230769,3.76961538461539,4.19557692307693,4.64519230769231],[4.73980769230769,4.29019230769231,3.86423076923077,3.46192307692308,3.08326923076923,2.72826923076923,2.39692307692308,2.08923076923077,1.80519230769231,1.54480769230769,1.30807692307692,1.095,0.905576923076923,0.739807692307692,0.597884615384615,null,0.385,0.314038461538462,0.266730769230769,0.243076923076923,0.243076923076923,0.266730769230769,0.314038461538462,0.385,0.479615384615385,0.597884615384616,0.739807692307692,0.905576923076924,1.095,1.30807692307692,1.54480769230769,1.80519230769231,2.08923076923077,2.39692307692308,2.72826923076923,3.08326923076923,3.46192307692308,3.86423076923077,4.29019230769231,4.73980769230769],[4.85807692307692,4.40846153846154,3.9825,3.58019230769231,3.20153846153846,2.84653846153846,2.51519230769231,2.2075,1.92346153846154,1.66307692307692,1.42634615384615,1.21326923076923,1.02384615384615,0.858076923076923,0.716153846153846,0.597884615384615,0.503269230769231,0.432307692307692,0.385,0.361346153846154,0.361346153846154,0.385,0.432307692307693,0.503269230769231,0.597884615384616,0.716153846153846,0.858076923076923,null,1.21326923076923,1.42634615384615,1.66307692307692,1.92346153846154,2.2075,2.51519230769231,2.84653846153846,3.20153846153846,3.58019230769231,3.9825,4.40846153846154,4.85807692307692],[5,4.55038461538462,4.12442307692308,3.72211538461538,3.34346153846154,2.98846153846154,2.65711538461538,2.34942307692308,2.06538461538462,1.805,1.56826923076923,1.35519230769231,1.16576923076923,1,0.858076923076923,0.739807692307692,0.645192307692308,0.574230769230769,0.526923076923077,0.503269230769231,0.503269230769231,0.526923076923077,0.574230769230769,0.645192307692308,0.739807692307692,0.858076923076923,1,1.16576923076923,1.35519230769231,1.56826923076923,1.805,2.06538461538462,2.34942307692308,2.65711538461539,2.98846153846154,3.34346153846154,3.72211538461538,4.12442307692308,4.55038461538462,5],[5.16576923076923,4.71615384615385,4.29019230769231,3.88788461538462,3.50923076923077,3.15423076923077,2.82288461538461,2.51519230769231,2.23115384615385,1.97076923076923,1.73403846153846,1.52096153846154,1.33153846153846,1.16576923076923,1.02384615384615,0.905576923076923,0.810961538461539,0.74,0.692692307692308,0.669038461538462,0.669038461538462,0.692692307692308,0.740000000000001,0.810961538461539,0.905576923076924,null,1.16576923076923,1.33153846153846,1.52096153846154,1.73403846153846,1.97076923076923,2.23115384615385,2.51519230769231,2.82288461538462,3.15423076923077,3.50923076923077,3.88788461538462,4.29019230769231,null,5.16576923076923],[5.35519230769231,4.90557692307692,4.47961538461539,4.07730769230769,3.69865384615385,3.34365384615385,3.01230769230769,2.70461538461539,2.42057692307692,2.16019230769231,1.92346153846154,1.71038461538462,1.52096153846154,1.35519230769231,1.21326923076923,1.095,1.00038461538462,0.929423076923078,0.882115384615386,0.858461538461539,0.858461538461539,0.882115384615386,0.929423076923078,1.00038461538462,1.095,1.21326923076923,1.35519230769231,1.52096153846154,1.71038461538462,1.92346153846154,2.16019230769231,2.42057692307692,2.70461538461539,3.01230769230769,3.34365384615385,3.69865384615385,4.07730769230769,4.47961538461539,4.90557692307693,5.35519230769231],[5.56826923076923,5.11865384615385,4.69269230769231,4.29038461538462,3.91173076923077,3.55673076923077,3.22538461538461,2.91769230769231,2.63365384615385,2.37326923076923,2.13653846153846,1.92346153846154,1.73403846153846,1.56826923076923,1.42634615384615,1.30807692307692,1.21346153846154,1.1425,1.09519230769231,1.07153846153846,1.07153846153846,1.09519230769231,1.1425,1.21346153846154,1.30807692307692,1.42634615384615,1.56826923076923,1.73403846153846,1.92346153846154,2.13653846153846,2.37326923076923,2.63365384615385,2.91769230769231,3.22538461538462,3.55673076923077,3.91173076923077,4.29038461538461,4.69269230769231,5.11865384615385,5.56826923076923],[5.805,5.35538461538462,4.92942307692308,4.52711538461539,4.14846153846154,3.79346153846154,3.46211538461538,3.15442307692308,2.87038461538462,2.61,2.37326923076923,2.16019230769231,1.97076923076923,1.805,1.66307692307692,1.54480769230769,1.45019230769231,1.37923076923077,1.33192307692308,1.30826923076923,1.30826923076923,1.33192307692308,1.37923076923077,1.45019230769231,1.54480769230769,1.66307692307692,1.805,1.97076923076923,2.16019230769231,2.37326923076923,2.61,2.87038461538462,3.15442307692308,3.46211538461539,3.79346153846154,4.14846153846154,4.52711538461539,4.92942307692308,5.35538461538462,5.805],[6.06538461538462,5.61576923076923,5.18980769230769,4.7875,4.40884615384615,4.05384615384615,3.7225,3.41480769230769,3.13076923076923,2.87038461538462,2.63365384615385,2.42057692307692,2.23115384615385,2.06538461538462,1.92346153846154,1.80519230769231,1.71057692307692,1.63961538461538,1.59230769230769,1.56865384615385,1.56865384615385,1.59230769230769,1.63961538461538,1.71057692307692,1.80519230769231,1.92346153846154,2.06538461538462,2.23115384615385,2.42057692307692,2.63365384615385,2.87038461538462,3.13076923076923,3.41480769230769,3.7225,4.05384615384615,4.40884615384615,4.7875,5.18980769230769,5.61576923076923,6.06538461538462],[6.34942307692308,5.89980769230769,5.47384615384616,5.07153846153846,4.69288461538462,4.33788461538462,4.00653846153846,3.69884615384615,3.41480769230769,3.15442307692308,2.91769230769231,2.70461538461538,2.51519230769231,2.34942307692308,2.2075,2.08923076923077,1.99461538461539,1.92365384615385,1.87634615384615,1.85269230769231,1.85269230769231,1.87634615384615,1.92365384615385,1.99461538461539,2.08923076923077,2.2075,2.34942307692308,2.51519230769231,2.70461538461539,2.91769230769231,3.15442307692308,3.41480769230769,3.69884615384615,4.00653846153846,4.33788461538462,4.69288461538462,5.07153846153846,5.47384615384616,5.89980769230769,6.34942307692308],[6.65711538461539,6.2075,5.78153846153846,5.37923076923077,5.00057692307692,4.64557692307692,4.31423076923077,4.00653846153846,3.7225,3.46211538461539,3.22538461538462,3.01230769230769,2.82288461538462,2.65711538461539,2.51519230769231,2.39692307692308,2.30230769230769,2.23134615384616,2.18403846153846,2.16038461538462,2.16038461538462,2.18403846153846,2.23134615384616,2.30230769230769,2.39692307692308,2.51519230769231,2.65711538461539,2.82288461538462,3.01230769230769,3.22538461538462,3.46211538461539,3.7225,4.00653846153846,4.31423076923077,4.64557692307692,5.00057692307693,5.37923076923077,5.78153846153846,6.2075,null],[6.98846153846154,6.53884615384615,6.11288461538462,5.71057692307692,5.33192307692308,4.97692307692308,4.64557692307692,4.33788461538462,4.05384615384615,3.79346153846154,3.55673076923077,3.34365384615385,3.15423076923077,2.98846153846154,2.84653846153846,2.72826923076923,2.63365384615385,2.56269230769231,2.51538461538462,2.49173076923077,2.49173076923077,2.51538461538462,2.56269230769231,2.63365384615385,2.72826923076923,2.84653846153846,2.98846153846154,3.15423076923077,3.34365384615385,3.55673076923077,3.79346153846154,4.05384615384615,4.33788461538462,4.64557692307692,4.97692307692308,5.33192307692308,5.71057692307692,6.11288461538462,null,null],[7.34346153846154,6.89384615384616,6.46788461538462,6.06557692307692,5.68692307692308,5.33192307692308,5.00057692307692,4.69288461538462,4.40884615384615,4.14846153846154,3.91173076923077,3.69865384615385,3.50923076923077,3.34346153846154,3.20153846153846,3.08326923076923,2.98865384615385,2.91769230769231,2.87038461538462,2.84673076923077,2.84673076923077,2.87038461538462,2.91769230769231,2.98865384615385,3.08326923076923,3.20153846153846,3.34346153846154,3.50923076923077,3.69865384615385,3.91173076923077,4.14846153846154,4.40884615384615,4.69288461538462,5.00057692307693,5.33192307692308,5.68692307692308,6.06557692307692,null,null,7.34346153846154],[7.72211538461538,7.2725,6.84653846153846,6.44423076923077,6.06557692307692,5.71057692307692,5.37923076923077,5.07153846153846,4.7875,4.52711538461538,4.29038461538461,4.07730769230769,3.88788461538461,3.72211538461538,3.58019230769231,3.46192307692308,3.36730769230769,3.29634615384615,3.24903846153846,3.22538461538462,3.22538461538462,3.24903846153846,3.29634615384615,3.36730769230769,3.46192307692308,3.58019230769231,3.72211538461538,3.88788461538462,4.07730769230769,4.29038461538462,4.52711538461539,4.7875,5.07153846153846,5.37923076923077,5.71057692307692,6.06557692307692,null,null,null,7.72211538461538],[8.12442307692308,7.67480769230769,7.24884615384616,6.84653846153846,6.46788461538462,6.11288461538462,5.78153846153846,5.47384615384615,5.18980769230769,4.92942307692308,4.69269230769231,4.47961538461539,4.29019230769231,4.12442307692308,3.9825,null,3.76961538461539,3.69865384615385,3.65134615384615,3.62769230769231,3.62769230769231,3.65134615384615,3.69865384615385,3.76961538461539,3.86423076923077,3.9825,4.12442307692308,4.29019230769231,4.47961538461539,4.69269230769231,4.92942307692308,5.18980769230769,5.47384615384616,5.78153846153846,6.11288461538462,null,null,null,7.67480769230769,8.12442307692308],[8.55038461538462,8.10076923076923,7.67480769230769,7.2725,6.89384615384616,6.53884615384615,6.2075,5.89980769230769,5.61576923076923,5.35538461538462,5.11865384615385,4.90557692307692,4.71615384615385,4.55038461538462,4.40846153846154,4.29019230769231,4.19557692307692,4.12461538461539,4.07730769230769,4.05365384615385,4.05365384615385,4.07730769230769,4.12461538461539,4.19557692307692,4.29019230769231,4.40846153846154,4.55038461538462,null,4.90557692307693,5.11865384615385,5.35538461538462,5.61576923076923,5.89980769230769,null,null,null,7.2725,7.67480769230769,8.10076923076923,8.55038461538462],[9,8.55038461538462,8.12442307692308,7.72211538461538,7.34346153846154,6.98846153846154,6.65711538461538,6.34942307692308,6.06538461538462,5.805,5.56826923076923,5.35519230769231,5.16576923076923,5,4.85807692307692,4.73980769230769,4.64519230769231,4.57423076923077,4.52692307692308,4.50326923076923,4.50326923076923,4.52692307692308,4.57423076923077,4.64519230769231,4.73980769230769,4.85807692307692,5,5.16576923076923,5.35519230769231,5.56826923076923,5.805,6.06538461538462,6.34942307692308,6.65711538461539,null,null,7.72211538461538,8.12442307692308,8.55038461538462,9]],"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"frame":null},{"colorbar":{"title":"ridge_p1$z<br />ridge_p2$z<br />ridge_p3$z","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[27,25.6511538461538,24.3732692307692,23.1663461538462,22.0303846153846,20.9653846153846,19.9713461538462,19.0482692307692,18.1961538461538,17.415,16.7048076923077,16.0655769230769,15.4973076923077,15,14.5742307692308,14.2194230769231,13.9355769230769,13.7226923076923,13.5807692307692,13.5098076923077,13.5098076923077,13.5807692307692,13.7226923076923,13.9355769230769,14.2194230769231,14.5742307692308,15,15.4973076923077,16.0655769230769,16.7048076923077,17.415,18.1961538461538,19.0482692307692,19.9713461538462,20.9653846153846,22.0303846153846,23.1663461538462,24.3732692307692,25.6511538461538,27],[25.6511538461538,24.3023076923077,23.0244230769231,21.8175,20.6815384615385,19.6165384615385,18.6225,17.6994230769231,16.8473076923077,16.0661538461538,15.3559615384615,14.7167307692308,14.1484615384615,13.6511538461538,13.2253846153846,12.8705769230769,12.5867307692308,12.3738461538462,12.2319230769231,12.1609615384615,12.1609615384615,12.2319230769231,12.3738461538462,12.5867307692308,12.8705769230769,13.2253846153846,13.6511538461538,14.1484615384615,14.7167307692308,15.3559615384615,16.0661538461538,16.8473076923077,17.6994230769231,18.6225,19.6165384615385,20.6815384615385,21.8175,23.0244230769231,24.3023076923077,25.6511538461538],[24.3732692307692,23.0244230769231,21.7465384615385,20.5396153846154,19.4036538461538,18.3386538461538,17.3446153846154,16.4215384615385,15.5694230769231,14.7882692307692,14.0780769230769,13.4388461538462,12.8705769230769,12.3732692307692,11.9475,11.5926923076923,11.3088461538462,11.0959615384615,10.9540384615385,10.8830769230769,10.8830769230769,10.9540384615385,11.0959615384615,11.3088461538462,11.5926923076923,11.9475,12.3732692307692,12.8705769230769,13.4388461538462,14.0780769230769,14.7882692307692,15.5694230769231,16.4215384615385,17.3446153846154,18.3386538461538,19.4036538461539,20.5396153846154,21.7465384615385,23.0244230769231,24.3732692307692],[23.1663461538462,21.8175,20.5396153846154,19.3326923076923,18.1967307692308,17.1317307692308,16.1376923076923,15.2146153846154,14.3625,13.5813461538462,12.8711538461538,12.2319230769231,11.6636538461538,11.1663461538462,10.7405769230769,10.3857692307692,10.1019230769231,9.88903846153846,9.74711538461539,9.67615384615385,9.67615384615384,9.74711538461538,9.88903846153846,10.1019230769231,10.3857692307692,10.7405769230769,11.1663461538462,11.6636538461538,12.2319230769231,12.8711538461538,13.5813461538462,14.3625,15.2146153846154,16.1376923076923,17.1317307692308,18.1967307692308,19.3326923076923,20.5396153846154,21.8175,23.1663461538462],[22.0303846153846,20.6815384615385,19.4036538461539,18.1967307692308,17.0607692307692,15.9957692307692,15.0017307692308,14.0786538461538,13.2265384615385,12.4453846153846,11.7351923076923,11.0959615384615,10.5276923076923,10.0303846153846,9.60461538461538,9.24980769230769,8.96596153846154,8.75307692307692,8.61115384615385,8.54019230769231,8.54019230769231,8.61115384615385,8.75307692307692,8.96596153846154,9.24980769230769,9.60461538461538,10.0303846153846,10.5276923076923,11.0959615384615,11.7351923076923,12.4453846153846,13.2265384615385,14.0786538461538,15.0017307692308,15.9957692307692,17.0607692307692,18.1967307692308,19.4036538461539,20.6815384615385,22.0303846153846],[20.9653846153846,19.6165384615385,18.3386538461538,17.1317307692308,15.9957692307692,14.9307692307692,13.9367307692308,13.0136538461538,12.1615384615385,11.3803846153846,10.6701923076923,10.0309615384615,9.46269230769231,8.96538461538462,8.53961538461538,8.18480769230769,7.90096153846154,7.68807692307692,7.54615384615385,7.47519230769231,7.47519230769231,7.54615384615385,7.68807692307692,7.90096153846154,8.18480769230769,8.53961538461539,8.96538461538461,9.46269230769231,10.0309615384615,10.6701923076923,11.3803846153846,12.1615384615385,13.0136538461538,13.9367307692308,14.9307692307692,15.9957692307692,17.1317307692308,18.3386538461538,19.6165384615385,20.9653846153846],[19.9713461538462,18.6225,17.3446153846154,16.1376923076923,15.0017307692308,13.9367307692308,12.9426923076923,12.0196153846154,11.1675,10.3863461538462,9.67615384615384,9.03692307692308,8.46865384615385,7.97134615384615,7.54557692307692,7.19076923076923,6.90692307692308,6.69403846153846,6.55211538461538,6.48115384615384,6.48115384615384,6.55211538461538,6.69403846153846,6.90692307692308,7.19076923076923,7.54557692307692,7.97134615384615,8.46865384615385,9.03692307692308,9.67615384615384,10.3863461538462,11.1675,12.0196153846154,12.9426923076923,13.9367307692308,15.0017307692308,16.1376923076923,17.3446153846154,18.6225,19.9713461538462],[19.0482692307692,17.6994230769231,16.4215384615385,15.2146153846154,14.0786538461538,13.0136538461538,12.0196153846154,11.0965384615385,10.2444230769231,9.46326923076923,8.75307692307692,8.11384615384615,7.54557692307692,7.04826923076923,6.6225,6.26769230769231,5.98384615384615,5.77096153846154,5.62903846153846,5.55807692307692,5.55807692307692,5.62903846153846,5.77096153846154,5.98384615384615,6.26769230769231,6.6225,7.04826923076923,7.54557692307692,8.11384615384616,8.75307692307692,9.46326923076923,10.2444230769231,11.0965384615385,12.0196153846154,13.0136538461538,14.0786538461539,15.2146153846154,16.4215384615385,17.6994230769231,19.0482692307692],[18.1961538461538,16.8473076923077,15.5694230769231,14.3625,13.2265384615385,12.1615384615385,11.1675,10.2444230769231,9.39230769230769,8.61115384615385,7.90096153846154,7.26173076923077,6.69346153846154,6.19615384615385,5.77038461538461,5.41557692307692,5.13173076923077,4.91884615384615,4.77692307692308,4.70596153846154,4.70596153846154,4.77692307692308,4.91884615384615,5.13173076923077,5.41557692307692,5.77038461538462,6.19615384615385,6.69346153846154,7.26173076923077,7.90096153846154,8.61115384615385,9.39230769230769,10.2444230769231,11.1675,12.1615384615385,13.2265384615385,14.3625,15.5694230769231,16.8473076923077,18.1961538461538],[17.415,16.0661538461538,14.7882692307692,13.5813461538462,12.4453846153846,11.3803846153846,10.3863461538462,9.46326923076923,8.61115384615385,7.83,7.11980769230769,6.48057692307692,5.91230769230769,5.415,4.98923076923077,4.63442307692308,4.35057692307692,4.13769230769231,3.99576923076923,3.92480769230769,3.92480769230769,3.99576923076923,4.13769230769231,4.35057692307692,4.63442307692308,4.98923076923077,5.415,5.91230769230769,6.48057692307693,7.11980769230769,7.83,8.61115384615385,9.46326923076923,10.3863461538462,11.3803846153846,12.4453846153846,13.5813461538462,14.7882692307692,16.0661538461538,17.415],[16.7048076923077,15.3559615384615,14.0780769230769,12.8711538461538,11.7351923076923,10.6701923076923,9.67615384615384,8.75307692307692,7.90096153846154,7.11980769230769,6.40961538461538,5.77038461538461,5.20211538461538,4.70480769230769,4.27903846153846,3.92423076923077,3.64038461538462,3.4275,3.28557692307692,3.21461538461538,3.21461538461538,3.28557692307692,3.4275,3.64038461538462,3.92423076923077,4.27903846153846,4.70480769230769,5.20211538461539,5.77038461538462,6.40961538461538,7.11980769230769,7.90096153846154,8.75307692307692,9.67615384615385,10.6701923076923,11.7351923076923,12.8711538461538,14.0780769230769,15.3559615384615,16.7048076923077],[16.0655769230769,14.7167307692308,13.4388461538462,12.2319230769231,11.0959615384615,10.0309615384615,9.03692307692307,8.11384615384615,7.26173076923077,6.48057692307692,5.77038461538461,5.13115384615384,4.56288461538461,4.06557692307692,3.63980769230769,3.285,3.00115384615385,2.78826923076923,2.64634615384615,2.57538461538461,2.57538461538461,2.64634615384615,2.78826923076923,3.00115384615385,3.285,3.63980769230769,4.06557692307692,4.56288461538462,5.13115384615385,5.77038461538462,6.48057692307693,7.26173076923077,8.11384615384615,9.03692307692308,10.0309615384615,11.0959615384615,12.2319230769231,13.4388461538462,14.7167307692308,16.0655769230769],[15.4973076923077,14.1484615384615,12.8705769230769,11.6636538461538,10.5276923076923,9.46269230769231,8.46865384615384,7.54557692307692,6.69346153846154,5.91230769230769,5.20211538461538,4.56288461538461,3.99461538461538,3.49730769230769,3.07153846153846,2.71673076923077,2.43288461538461,2.22,2.07807692307692,2.00711538461538,2.00711538461538,2.07807692307692,2.22,2.43288461538462,2.71673076923077,3.07153846153846,3.49730769230769,3.99461538461539,4.56288461538462,5.20211538461538,5.91230769230769,6.69346153846154,7.54557692307692,8.46865384615385,9.46269230769231,10.5276923076923,11.6636538461538,12.8705769230769,14.1484615384615,15.4973076923077],[15,13.6511538461538,12.3732692307692,11.1663461538462,10.0303846153846,8.96538461538462,7.97134615384615,7.04826923076923,6.19615384615385,5.415,4.70480769230769,4.06557692307692,3.49730769230769,3,2.57423076923077,2.21942307692308,1.93557692307692,1.72269230769231,1.58076923076923,1.50980769230769,1.50980769230769,1.58076923076923,1.72269230769231,1.93557692307692,2.21942307692308,2.57423076923077,3,3.49730769230769,4.06557692307693,4.70480769230769,5.415,6.19615384615385,7.04826923076923,7.97134615384616,8.96538461538462,10.0303846153846,11.1663461538462,12.3732692307692,13.6511538461538,15],[14.5742307692308,13.2253846153846,11.9475,10.7405769230769,9.60461538461538,8.53961538461538,7.54557692307692,6.6225,5.77038461538461,4.98923076923077,4.27903846153846,3.63980769230769,3.07153846153846,2.57423076923077,2.14846153846154,1.79365384615384,1.50980769230769,1.29692307692308,1.155,1.08403846153846,1.08403846153846,1.155,1.29692307692308,1.50980769230769,1.79365384615385,2.14846153846154,2.57423076923077,3.07153846153846,3.63980769230769,4.27903846153846,4.98923076923077,5.77038461538461,6.6225,7.54557692307693,8.53961538461538,9.60461538461539,10.7405769230769,11.9475,13.2253846153846,14.5742307692308],[14.2194230769231,12.8705769230769,11.5926923076923,10.3857692307692,9.24980769230769,8.18480769230769,7.19076923076923,6.26769230769231,5.41557692307692,4.63442307692308,3.92423076923077,3.285,2.71673076923077,2.21942307692308,1.79365384615384,1.43884615384615,1.155,0.942115384615384,0.800192307692307,0.729230769230769,0.729230769230769,0.800192307692307,0.942115384615385,1.155,null,1.79365384615385,2.21942307692308,2.71673076923077,3.285,3.92423076923077,4.63442307692308,5.41557692307692,6.26769230769231,7.19076923076923,8.18480769230769,9.24980769230769,10.3857692307692,null,12.8705769230769,14.2194230769231],[13.9355769230769,12.5867307692308,11.3088461538462,10.1019230769231,8.96596153846154,7.90096153846154,6.90692307692308,5.98384615384615,5.13173076923077,4.35057692307692,3.64038461538462,3.00115384615384,2.43288461538461,1.93557692307692,1.50980769230769,1.155,0.871153846153846,0.65826923076923,0.516346153846154,0.445384615384615,0.445384615384615,0.516346153846154,0.658269230769231,0.871153846153846,1.155,1.50980769230769,1.93557692307692,2.43288461538462,3.00115384615385,3.64038461538462,4.35057692307693,5.13173076923077,5.98384615384616,6.90692307692308,7.90096153846154,8.96596153846154,10.1019230769231,11.3088461538462,12.5867307692308,13.9355769230769],[13.7226923076923,12.3738461538462,11.0959615384615,9.88903846153846,8.75307692307692,7.68807692307692,6.69403846153846,5.77096153846154,4.91884615384615,4.13769230769231,3.4275,2.78826923076923,2.22,1.72269230769231,1.29692307692308,0.942115384615384,0.65826923076923,0.445384615384615,0.303461538461539,0.2325,0.2325,0.303461538461538,0.445384615384616,0.658269230769231,0.942115384615385,1.29692307692308,1.72269230769231,2.22,2.78826923076923,3.4275,4.13769230769231,4.91884615384615,5.77096153846154,6.69403846153847,7.68807692307692,8.75307692307693,9.88903846153846,11.0959615384615,12.3738461538462,13.7226923076923],[13.5807692307692,12.2319230769231,10.9540384615385,9.74711538461538,8.61115384615385,7.54615384615385,6.55211538461538,5.62903846153846,4.77692307692308,3.99576923076923,3.28557692307692,2.64634615384615,2.07807692307692,1.58076923076923,1.155,0.800192307692307,0.516346153846154,0.303461538461538,0.161538461538462,0.0905769230769231,0.0905769230769232,0.161538461538462,0.303461538461539,0.516346153846154,0.800192307692308,1.155,1.58076923076923,2.07807692307692,2.64634615384616,3.28557692307692,3.99576923076923,4.77692307692308,5.62903846153846,6.55211538461539,7.54615384615385,8.61115384615385,9.74711538461538,10.9540384615385,12.2319230769231,13.5807692307692],[13.5098076923077,12.1609615384615,10.8830769230769,9.67615384615385,8.54019230769231,7.47519230769231,6.48115384615384,5.55807692307692,4.70596153846154,3.92480769230769,3.21461538461538,2.57538461538461,2.00711538461538,1.50980769230769,1.08403846153846,0.729230769230769,0.445384615384615,0.2325,0.0905769230769231,0.0196153846153845,0.0196153846153846,0.0905769230769231,0.2325,0.445384615384616,0.72923076923077,1.08403846153846,1.50980769230769,2.00711538461539,2.57538461538462,3.21461538461539,3.92480769230769,4.70596153846154,5.55807692307692,6.48115384615385,7.47519230769231,8.54019230769231,9.67615384615384,10.8830769230769,12.1609615384615,13.5098076923077],[13.5098076923077,12.1609615384615,10.8830769230769,9.67615384615385,8.54019230769231,7.47519230769231,6.48115384615384,5.55807692307692,4.70596153846154,3.92480769230769,3.21461538461538,2.57538461538461,2.00711538461538,1.50980769230769,1.08403846153846,0.729230769230769,0.445384615384615,0.2325,0.0905769230769232,0.0196153846153846,0.0196153846153847,0.0905769230769232,0.232500000000001,0.445384615384616,0.72923076923077,1.08403846153846,1.50980769230769,2.00711538461539,2.57538461538462,3.21461538461539,3.9248076923077,4.70596153846154,5.55807692307692,6.48115384615385,7.47519230769231,8.54019230769231,9.67615384615384,10.8830769230769,12.1609615384615,13.5098076923077],[13.5807692307692,12.2319230769231,10.9540384615385,9.74711538461538,8.61115384615385,7.54615384615385,6.55211538461538,5.62903846153846,4.77692307692308,3.99576923076923,3.28557692307692,2.64634615384615,2.07807692307692,1.58076923076923,1.155,0.800192307692307,0.516346153846154,0.303461538461538,0.161538461538462,0.0905769230769231,0.0905769230769232,0.161538461538462,0.303461538461539,0.516346153846154,0.800192307692308,1.155,1.58076923076923,2.07807692307692,2.64634615384616,3.28557692307692,3.99576923076923,4.77692307692308,5.62903846153846,6.55211538461539,7.54615384615385,8.61115384615385,9.74711538461538,10.9540384615385,12.2319230769231,13.5807692307692],[13.7226923076923,12.3738461538462,11.0959615384615,9.88903846153846,8.75307692307692,7.68807692307692,6.69403846153846,5.77096153846154,4.91884615384615,4.13769230769231,3.4275,2.78826923076923,2.22,1.72269230769231,1.29692307692308,0.942115384615385,0.658269230769231,0.445384615384616,0.303461538461539,0.2325,0.232500000000001,0.303461538461539,0.445384615384616,0.658269230769232,0.942115384615386,1.29692307692308,1.72269230769231,2.22,2.78826923076923,3.4275,4.13769230769231,4.91884615384615,5.77096153846154,6.69403846153847,7.68807692307692,8.75307692307693,9.88903846153846,11.0959615384615,12.3738461538462,13.7226923076923],[13.9355769230769,12.5867307692308,11.3088461538462,10.1019230769231,8.96596153846154,7.90096153846154,6.90692307692308,5.98384615384615,5.13173076923077,4.35057692307692,3.64038461538462,3.00115384615385,2.43288461538462,1.93557692307692,1.50980769230769,1.155,0.871153846153847,0.658269230769231,0.516346153846154,0.445384615384616,0.445384615384616,0.516346153846154,0.658269230769232,0.871153846153847,1.155,1.50980769230769,1.93557692307692,2.43288461538462,3.00115384615385,3.64038461538462,4.35057692307693,5.13173076923077,5.98384615384616,6.90692307692308,7.90096153846154,8.96596153846154,10.1019230769231,11.3088461538462,12.5867307692308,13.9355769230769],[14.2194230769231,12.8705769230769,11.5926923076923,10.3857692307692,9.24980769230769,8.18480769230769,7.19076923076923,6.26769230769231,5.41557692307692,4.63442307692308,3.92423076923077,3.285,2.71673076923077,2.21942307692308,1.79365384615385,null,1.155,0.942115384615385,0.800192307692308,0.72923076923077,0.72923076923077,0.800192307692308,0.942115384615386,1.155,1.43884615384615,1.79365384615385,2.21942307692308,2.71673076923077,3.285,3.92423076923077,4.63442307692308,5.41557692307692,6.26769230769231,7.19076923076923,8.18480769230769,9.2498076923077,10.3857692307692,11.5926923076923,12.8705769230769,14.2194230769231],[14.5742307692308,13.2253846153846,11.9475,10.7405769230769,9.60461538461538,8.53961538461539,7.54557692307692,6.6225,5.77038461538462,4.98923076923077,4.27903846153846,3.63980769230769,3.07153846153846,2.57423076923077,2.14846153846154,1.79365384615385,1.50980769230769,1.29692307692308,1.155,1.08403846153846,1.08403846153846,1.155,1.29692307692308,1.50980769230769,1.79365384615385,2.14846153846154,2.57423076923077,null,3.6398076923077,4.27903846153846,4.98923076923077,5.77038461538462,6.6225,7.54557692307693,8.53961538461539,9.60461538461539,10.7405769230769,11.9475,13.2253846153846,14.5742307692308],[15,13.6511538461538,12.3732692307692,11.1663461538462,10.0303846153846,8.96538461538462,7.97134615384615,7.04826923076923,6.19615384615385,5.415,4.70480769230769,4.06557692307692,3.49730769230769,3,2.57423076923077,2.21942307692308,1.93557692307692,1.72269230769231,1.58076923076923,1.50980769230769,1.50980769230769,1.58076923076923,1.72269230769231,1.93557692307692,2.21942307692308,2.57423076923077,3,3.49730769230769,4.06557692307693,4.70480769230769,5.415,6.19615384615385,7.04826923076923,7.97134615384616,8.96538461538462,10.0303846153846,11.1663461538462,12.3732692307692,13.6511538461538,15],[15.4973076923077,14.1484615384615,12.8705769230769,11.6636538461538,10.5276923076923,9.46269230769231,8.46865384615385,7.54557692307692,6.69346153846154,5.91230769230769,5.20211538461539,4.56288461538462,3.99461538461538,3.49730769230769,3.07153846153846,2.71673076923077,2.43288461538462,2.22,2.07807692307692,2.00711538461539,2.00711538461539,2.07807692307692,2.22,2.43288461538462,2.71673076923077,null,3.49730769230769,3.99461538461539,4.56288461538462,5.20211538461539,5.9123076923077,6.69346153846154,7.54557692307693,8.46865384615385,9.46269230769231,10.5276923076923,11.6636538461538,12.8705769230769,null,15.4973076923077],[16.0655769230769,14.7167307692308,13.4388461538462,12.2319230769231,11.0959615384615,10.0309615384615,9.03692307692308,8.11384615384616,7.26173076923077,6.48057692307693,5.77038461538462,5.13115384615385,4.56288461538462,4.06557692307693,3.63980769230769,3.285,3.00115384615385,2.78826923076923,2.64634615384616,2.57538461538462,2.57538461538462,2.64634615384616,2.78826923076923,3.00115384615385,3.285,3.63980769230769,4.06557692307693,4.56288461538462,5.13115384615385,5.77038461538462,6.48057692307693,7.26173076923077,8.11384615384616,9.03692307692308,10.0309615384615,11.0959615384615,12.2319230769231,13.4388461538462,14.7167307692308,16.0655769230769],[16.7048076923077,15.3559615384615,14.0780769230769,12.8711538461538,11.7351923076923,10.6701923076923,9.67615384615384,8.75307692307692,7.90096153846154,7.11980769230769,6.40961538461538,5.77038461538462,5.20211538461538,4.70480769230769,4.27903846153846,3.92423076923077,3.64038461538462,3.4275,3.28557692307692,3.21461538461539,3.21461538461539,3.28557692307692,3.4275,3.64038461538462,3.92423076923077,4.27903846153846,4.70480769230769,5.20211538461539,5.77038461538462,6.40961538461539,7.1198076923077,7.90096153846154,8.75307692307693,9.67615384615385,10.6701923076923,11.7351923076923,12.8711538461538,14.0780769230769,15.3559615384615,16.7048076923077],[17.415,16.0661538461538,14.7882692307692,13.5813461538462,12.4453846153846,11.3803846153846,10.3863461538462,9.46326923076923,8.61115384615385,7.83,7.11980769230769,6.48057692307693,5.91230769230769,5.415,4.98923076923077,4.63442307692308,4.35057692307693,4.13769230769231,3.99576923076923,3.92480769230769,3.9248076923077,3.99576923076923,4.13769230769231,4.35057692307693,4.63442307692308,4.98923076923077,5.415,5.9123076923077,6.48057692307693,7.1198076923077,7.83,8.61115384615385,9.46326923076923,10.3863461538462,11.3803846153846,12.4453846153846,13.5813461538462,14.7882692307692,16.0661538461539,17.415],[18.1961538461538,16.8473076923077,15.5694230769231,14.3625,13.2265384615385,12.1615384615385,11.1675,10.2444230769231,9.39230769230769,8.61115384615385,7.90096153846154,7.26173076923077,6.69346153846154,6.19615384615385,5.77038461538461,5.41557692307692,5.13173076923077,4.91884615384615,4.77692307692308,4.70596153846154,4.70596153846154,4.77692307692308,4.91884615384615,5.13173076923077,5.41557692307692,5.77038461538462,6.19615384615385,6.69346153846154,7.26173076923077,7.90096153846154,8.61115384615385,9.39230769230769,10.2444230769231,11.1675,12.1615384615385,13.2265384615385,14.3625,15.5694230769231,16.8473076923077,18.1961538461538],[19.0482692307692,17.6994230769231,16.4215384615385,15.2146153846154,14.0786538461538,13.0136538461538,12.0196153846154,11.0965384615385,10.2444230769231,9.46326923076923,8.75307692307693,8.11384615384615,7.54557692307692,7.04826923076923,6.6225,6.26769230769231,5.98384615384616,5.77096153846154,5.62903846153846,5.55807692307692,5.55807692307692,5.62903846153846,5.77096153846154,5.98384615384616,6.26769230769231,6.6225,7.04826923076923,7.54557692307693,8.11384615384616,8.75307692307693,9.46326923076924,10.2444230769231,11.0965384615385,12.0196153846154,13.0136538461538,14.0786538461539,15.2146153846154,16.4215384615385,17.6994230769231,19.0482692307692],[19.9713461538462,18.6225,17.3446153846154,16.1376923076923,15.0017307692308,13.9367307692308,12.9426923076923,12.0196153846154,11.1675,10.3863461538462,9.67615384615385,9.03692307692308,8.46865384615385,7.97134615384616,7.54557692307693,7.19076923076923,6.90692307692308,6.69403846153847,6.55211538461539,6.48115384615385,6.48115384615385,6.55211538461539,6.69403846153847,6.90692307692308,7.19076923076924,7.54557692307693,7.97134615384616,8.46865384615385,9.03692307692308,9.67615384615385,10.3863461538462,11.1675,12.0196153846154,12.9426923076923,13.9367307692308,15.0017307692308,16.1376923076923,17.3446153846154,18.6225,null],[20.9653846153846,19.6165384615385,18.3386538461538,17.1317307692308,15.9957692307692,14.9307692307692,13.9367307692308,13.0136538461538,12.1615384615385,11.3803846153846,10.6701923076923,10.0309615384615,9.46269230769231,8.96538461538462,8.53961538461538,8.18480769230769,7.90096153846154,7.68807692307692,7.54615384615385,7.47519230769231,7.47519230769231,7.54615384615385,7.68807692307692,7.90096153846154,8.18480769230769,8.53961538461539,8.96538461538462,9.46269230769231,10.0309615384615,10.6701923076923,11.3803846153846,12.1615384615385,13.0136538461538,13.9367307692308,14.9307692307692,15.9957692307692,17.1317307692308,18.3386538461538,null,null],[22.0303846153846,20.6815384615385,19.4036538461538,18.1967307692308,17.0607692307692,15.9957692307692,15.0017307692308,14.0786538461538,13.2265384615385,12.4453846153846,11.7351923076923,11.0959615384615,10.5276923076923,10.0303846153846,9.60461538461539,9.24980769230769,8.96596153846154,8.75307692307693,8.61115384615385,8.54019230769231,8.54019230769231,8.61115384615385,8.75307692307693,8.96596153846154,9.2498076923077,9.60461538461539,10.0303846153846,10.5276923076923,11.0959615384615,11.7351923076923,12.4453846153846,13.2265384615385,14.0786538461539,15.0017307692308,15.9957692307692,17.0607692307692,18.1967307692308,null,null,22.0303846153846],[23.1663461538462,21.8175,20.5396153846154,19.3326923076923,18.1967307692308,17.1317307692308,16.1376923076923,15.2146153846154,14.3625,13.5813461538462,12.8711538461538,12.2319230769231,11.6636538461538,11.1663461538462,10.7405769230769,10.3857692307692,10.1019230769231,9.88903846153846,9.74711538461538,9.67615384615385,9.67615384615385,9.74711538461538,9.88903846153846,10.1019230769231,10.3857692307692,10.7405769230769,11.1663461538462,11.6636538461538,12.2319230769231,12.8711538461538,13.5813461538462,14.3625,15.2146153846154,16.1376923076923,17.1317307692308,18.1967307692308,null,null,null,23.1663461538462],[24.3732692307692,23.0244230769231,21.7465384615385,20.5396153846154,19.4036538461538,18.3386538461538,17.3446153846154,16.4215384615385,15.5694230769231,14.7882692307692,14.0780769230769,13.4388461538462,12.8705769230769,12.3732692307692,11.9475,null,11.3088461538462,11.0959615384615,10.9540384615385,10.8830769230769,10.8830769230769,10.9540384615385,11.0959615384615,11.3088461538462,11.5926923076923,11.9475,12.3732692307692,12.8705769230769,13.4388461538462,14.0780769230769,14.7882692307692,15.5694230769231,16.4215384615385,17.3446153846154,18.3386538461538,null,null,null,23.0244230769231,24.3732692307692],[25.6511538461538,24.3023076923077,23.0244230769231,21.8175,20.6815384615385,19.6165384615385,18.6225,17.6994230769231,16.8473076923077,16.0661538461538,15.3559615384615,14.7167307692308,14.1484615384615,13.6511538461538,13.2253846153846,12.8705769230769,12.5867307692308,12.3738461538462,12.2319230769231,12.1609615384615,12.1609615384615,12.2319230769231,12.3738461538462,12.5867307692308,12.8705769230769,13.2253846153846,13.6511538461538,null,14.7167307692308,15.3559615384615,16.0661538461539,16.8473076923077,17.6994230769231,null,null,null,21.8175,23.0244230769231,24.3023076923077,25.6511538461538],[27,25.6511538461538,24.3732692307692,23.1663461538462,22.0303846153846,20.9653846153846,19.9713461538462,19.0482692307692,18.1961538461538,17.415,16.7048076923077,16.0655769230769,15.4973076923077,15,14.5742307692308,14.2194230769231,13.9355769230769,13.7226923076923,13.5807692307692,13.5098076923077,13.5098076923077,13.5807692307692,13.7226923076923,13.9355769230769,14.2194230769231,14.5742307692308,15,15.4973076923077,16.0655769230769,16.7048076923077,17.415,18.1961538461538,19.0482692307692,19.9713461538462,null,null,23.1663461538462,24.3732692307692,25.6511538461538,27]],"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>The goal of ridge regression is to find the optimal combination of <span class="math inline">\(\beta\)</span> weights that minimizes the traditional loss function, while also accounting for the added “loss” incurred by the penalty term. Looking at the plot above, you can see that as the <span class="math inline">\(\beta\)</span> weights both move away from 0, the ridge penalty function shows a slow increase near 0, which becomes progressively steeper as we move away from the origin. Further, as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the penalty function becomes steeper and steeper. Altogether, ridge regression does not prefer particular values for <span class="math inline">\(\beta\)</span> weights—it only prefers for all <span class="math inline">\(\beta\)</span> weights to be closer to 0.</p>
<div id="frequentist-ridge-regression" class="section level4">
<h4>Frequentist Ridge Regression</h4>
<p>One of the problems that arises from equation 4 is that we need to select a value for <span class="math inline">\(\lambda\)</span>, the parameter that controls how much we learn from our training data. But how should we select an optimal <span class="math inline">\(\lambda\)</span>? Of course, we could just pick a value based on our intuition, but that would likely lead to non-optimal solutions in many cases. Therefore, <strong>frequentist ridge regression typically relies on cross-validation (CV) to select</strong> <span class="math inline">\(\lambda\)</span> (see <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">here</a> for more details on hyper-parameter tuning/optimization). CV proceeds by:</p>
<ol style="list-style-type: decimal">
<li>generating a grid of values for our hyper-parameter (<span class="math inline">\(\lambda\)</span>)</li>
<li>splitting the training data into k equivalently sized training sets (<em>k folds</em>),</li>
<li>fitting a model to k-1 folds and testing prediction accuracy on the left-out fold,</li>
<li>repeat step 3 until each fold has been left out exactly once, and</li>
<li>iterating steps 2-4 for each hyper-parameter in the grid defined by step 1</li>
</ol>
<p>When we are finished, we select the value of <span class="math inline">\(\lambda\)</span> in the grid that minimizes the CV error (step 3). The R code below proceeds through the above steps, using the R <code>glmnet</code> package to fit the model:</p>
<pre class="r"><code># Penalty term values to test
lambdas &lt;- 10^seq(3, -2, by = -.1)

# Fit and cross-validate
fit_ridge &lt;- glmnet(x = train_scale[,-8], y = train_scale[,8], 
                    alpha = 0, lambda = lambdas, intercept = F)
cv_ridge &lt;- cv.glmnet(x = train_scale[,-8], y = train_scale[,8], 
                      alpha = 0, lambda = lambdas, intercept = F, 
                      grouped = FALSE)

# Find optimal penalty term (lambda)
opt_lambda &lt;- cv_ridge$lambda.min

# Generate predictions with opitmal lambda model from cross-validation
y_pred &lt;- predict(fit_ridge, s = opt_lambda, newx = test_scale[,-8])

# Plot cor(predicted, actual)
qplot(x = y_pred[,1], y = test_scale[,8],
      main = paste0(&quot;Frequentist Ridge Regression:\nEstimating &quot;, expression(lambda), 
                    &quot; with CV\nr = &quot;, round(cor(test_scale[,8], y_pred[,1]), 2))) +
  #geom_point(aes(x = , y = train_scale[,8]))
  xlab(&quot;Model Predicted Pr(Acceptance)&quot;) +
  ylab(&quot;Actual Pr(Acceptance)&quot;) +
  theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig3-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Woah, much better than traditional linear regression! Before diving into these results, however, let’s first explore Bayesian ridge regression.</p>
</div>
<div id="bayesian-ridge-regression" class="section level4">
<h4>Bayesian Ridge Regression</h4>
<p>Bayesian Ridge regression differs from the frequentist variant in only one way, and it is with how we think of the <span class="math inline">\(\lambda\)</span> penalty term. In the frequentist perspective, we showed that <span class="math inline">\(\lambda\)</span> effectively tells our model how much it is allowed to learn from the data. <em>In the Bayesian world, we can capture such an effect in the form of a prior distribution over our</em> <span class="math inline">\(\beta\)</span> <em>weights</em>. To reveal the extraordinary power hiding behind this simple idea, let’s first discuss Bayesian linear regression.</p>
<p>Bayesian models view estimation as a problem of integrating prior information with information gained from data, which we formalize using probability distributions. This differs from the frequntist view, which treats regression as an opimization problem that results in a point estimate (e.g., minimizing squared error). A Bayesian regression model takes the form of:</p>
<p><span class="math display">\[y_{i} | \beta_{0},\boldsymbol{x_{i}},\boldsymbol{\beta},\sigma \sim \mathcal{N}(\beta_{0} + \sum_{j=1}^{p}x_{ij}\beta_{j}, \sigma) \tag{5}\]</span>
Importantly, <em>Bayesian models require us to specify a <strong>prior distribution</strong> for each parameter we seek to estimate</em>. Therefore, we need to specify a prior on the intercept (<span class="math inline">\(\beta_{0}\)</span>), slopes (<span class="math inline">\(\boldsymbol{\beta}\)</span>), and error variance (<span class="math inline">\(\sigma\)</span>) in equation 5. Since we are standardizing all of our predictors and outcome variable(s), we will ignore the intercept term. Then, we are left with <span class="math inline">\(\boldsymbol{\beta}\)</span> and <span class="math inline">\(\sigma\)</span>. <strong>Crucially, our choice of prior distribution on <span class="math inline">\(\boldsymbol{\beta}\)</span> is what determines how much information we learn from the data, analagous to the penalty term <span class="math inline">\(\lambda\)</span> used for frequentist regularization.</strong></p>
<p>Like before, you can gain an intuition for this behavior by imagnining extreme cases. For example, suppose that we assumed that <span class="math inline">\(\boldsymbol{\beta} \sim \mathcal{U}(-\infty,+\infty)\)</span>. That is, we assume that <span class="math inline">\(\boldsymbol{\beta}\)</span> can take on any real-valued number, and every value is equally likely. In this case, the mode of the posterior distribution on each <span class="math inline">\(\beta\)</span> weight will be equivalent to the maximum likelihood estimate of the respective <span class="math inline">\(\beta\)</span> weight, and by extention the OLS estimate under the normality assumption (see our prior <a href="http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/">post on parameter estimation</a> for more details). If an unbounded uniform distribution on <span class="math inline">\(\boldsymbol{\beta}\)</span> produces the same behavior as traditional linear regression, it follows from our discussions above that it also allows us to maximally learn from the data.</p>
<p>Now, what can we do to restrict learning in a Bayesian model? Well, we can use a prior distribution that pulls the <span class="math inline">\(\beta\)</span> weights toward 0 (unlike the unbounded uniform distribution), similar to the <span class="math inline">\(\lambda\)</span> penalty parameter in frequentist regularization! Specifically, specifying the following prior distribution on <span class="math inline">\(\boldsymbol{\beta}\)</span> is mathematically equivalent in expectation (see <a href="https://stats.stackexchange.com/questions/163388/l2-regularization-is-equivalent-to-gaussian-prior">here</a>) to using the ridge penalty in the frequentist model:</p>
<p><span class="math display">\[\boldsymbol{\beta} \sim \mathcal{N}(0, \sigma_{\beta})\tag{6}\]</span></p>
<p>There are two important points that the prior distribution in equation 6 conveys:</p>
<ol style="list-style-type: decimal">
<li>The normal distribution places very little prior probability on large-magnitude <span class="math inline">\(\beta\)</span> weights (i.e. far from 0), while placing high prior probability on small-magnitude weights (i.e. near 0), and</li>
<li><span class="math inline">\(\sigma_{\beta}\)</span> controls how wide the normal distribution is, thus controlling the specific amount of prior probability placed on small- to large-magnitude <span class="math inline">\(\beta\)</span> weights.</li>
</ol>
<p>The Bayesian version differs most in that we jointly estimate the “penalization term” <span class="math inline">\(\sigma_{\beta}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> in a single model, as opposed to using CV to select an optimal <span class="math inline">\(\lambda\)</span> for frequentist regression. Because we are jointly estimating <span class="math inline">\(\sigma_{\beta}\)</span> along with individual-level <span class="math inline">\(\beta\)</span> weights, we can actually view Bayesian ridge regression as a simple hierarchical Bayesian model, where <span class="math inline">\(\sigma_{\beta}\)</span> is interpreted as a group-level scaling parameter that is estimated from pooled information across individual <span class="math inline">\(\beta\)</span> weights.</p>
<p>Below is the Stan code that specifies the Bayesian variant of ridge regression. Note that this code is based on <a href="https://osf.io/cg8fq/">Erp, Oberski, &amp; Mulder (2019)</a>:</p>
<pre class="stan"><code>data{
    int N_train;             // &quot;# training observations&quot;
    int N_test;              // &quot;# test observations&quot;
    int N_pred;              // &quot;# predictor variables&quot;
    vector[N_train] y_train; // &quot;training outcomes&quot;
    matrix[N_train, N_pred] X_train; // &quot;training data&quot;
    matrix[N_test, N_pred] X_test;   // &quot;testing data&quot;
}
parameters{
    real&lt;lower=0&gt; sigma;   // &quot;error SD&quot;
    real&lt;lower=0&gt; sigma_B; // &quot;hierarchical SD across betas&quot;
    vector[N_pred] beta;   // &quot;regression beta weights&quot;
}
model{
  // &quot;group-level (hierarchical) SD across betas&quot;
  sigma_B ~ cauchy(0, 1);
  
  // &quot;model error SD&quot;
  sigma ~ normal(0, 1);
  
  // &quot;beta prior (provides &#39;ridge&#39; regularization)&quot;
  beta ~ normal(0, sigma_B);
    
  // &quot;model likelihood&quot;
    y_train ~ normal(X_train*beta, sigma);
}
generated quantities{ 
    real y_test[N_test]; // &quot;test data predictions&quot;
    for(i in 1:N_test){
        y_test[i] = normal_rng(X_test[i,] * beta, sigma);
    }
}</code></pre>
<p>The important parts of the above code are in the <code>model{...}</code> section. While the parameterization looks different from the frequentist model (equation 4), the behavior of the model is equivalent. In particular, we have a <span class="math inline">\(\sigma_{\beta}\)</span> “penalty” term, can shrink the variation between <span class="math inline">\(\beta\)</span> weights toward 0. Intuitively, as <span class="math inline">\(\sigma_{\beta} \rightarrow 0\)</span>, all (<span class="math inline">\(\beta\)</span>) weights are shrunk toward 0 (no learning from data). Conversely, as <span class="math inline">\(\sigma_{\beta} \rightarrow \infty\)</span>, prior on <span class="math inline">\(\sigma_{\beta}\)</span> becomes uniform. A uniform prior denotes no penalty at all, and we are left with traditional, non-regularized regression (albeit we get a joint posterior distribution as opposed to point estimates). It is worth noting that the mode of the resulting posterior distribution over <span class="math inline">\(\boldsymbol{\beta}\)</span> will be equivalent to the maximum likelihood estimate when the prior on <span class="math inline">\(\boldsymbol{\beta}\)</span> is uniform (for details, see MLE/MAP estimation described in a <a href="http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/">previous post</a>).</p>
<p>Now, let’s try fitting the model!</p>
<pre class="r"><code># First, prepare data for Stan
stan_dat &lt;- list(N_train = nrow(train_scale),
                 N_test  = nrow(test_scale),
                 N_pred  = ncol(train_scale)-1,
                 y_train = train_scale[,8],
                 X_train = train_scale[,-8],
                 X_test  = test_scale[,-8])

# Fit the model using Stan&#39;s NUTS HMC sampler
fit_bayes_ridge &lt;- sampling(bayes_ridge, stan_dat, iter = 2000, 
                            warmup = 500, chains = 3, cores = 3)</code></pre>
<pre><code>## Warning: There were 1 divergent transitions after warmup. See
## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
## to find out why this is a problem and how to eliminate them.</code></pre>
<pre><code>## Warning: Examine the pairs() plot to diagnose sampling problems</code></pre>
<pre class="r"><code># Extract posterior distribution (parameters and predictions)
post_ridge &lt;- rstan::extract(fit_bayes_ridge)

# Compute mean of the posterior predictive distribution over test set predictors,
# which integrates out uncertainty in parameter estimates
y_pred_bayes &lt;- apply(post_ridge$y_test, 2, mean)

# Plot correlation between posterior predicted mean and actual Pr(Acceptance)
qplot(x = y_pred_bayes, y = test_scale[,8],
      main = paste0(&quot;Bayesian Ridge Regression:\nEstimating &quot;, expression(lambda), 
                    &quot; Hierarchically\nr = &quot;, round(cor(test_scale[,8], y_pred_bayes), 2))) +
    xlab(&quot;Model Predicted Pr(Acceptance)&quot;) +
    ylab(&quot;Actual Pr(Acceptance)&quot;) +
    theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig4-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>It is clear that the Bayesian ridge regression is giving results very similar (almost identical) to frequentist ridge regression as outlined above. Importantly, the Bayesian method also offers straightforward measures of uncertainty for both parameter estimates and predictions on the test data. The plot above integrates over the prediction uncertainty (i.e. integrating over the posterior predictive distribution). To really appreciate the prediction uncertainty, we compute the correlation between predicted and actual Pr(Acceptance) for each posterior sample, and then visualize the resulting distribution of correlations:</p>
<pre class="r"><code># Iterate through each posterior prediction and compute cor(predicted, actual),
# which preserves uncertainty in parameter estimates
y_pred_bayes2 &lt;- apply(post_ridge$y_test, 1, function(x) cor(x, test_scale[,8]))

# Plot posterior predictive distribution of cor(predicted, actual)
qplot(x = y_pred_bayes2, geom = &quot;histogram&quot;, bins = 50,
      xlab = &quot;Correlation between\nPredicted and Actual Pr(Acceptance)&quot;) +
  theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig5-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Clearly, there is good amount of uncertainty contained within the posterior predictive distribution, and the above plot shows it well. Importantly, this uncertainty reduce in proportion to how much data (i.e. observations) we have. Given that we only used 20 observations, these results are actually pretty good!</p>
<p>Another method is to visualize the scatterplot like above, but to include prediction intervals around the predicted mean estimates for each observation in the test set:</p>
<pre class="r"><code># `bayesplot` has many convenience functions for working with posteriors
color_scheme_set(scheme = &quot;darkgray&quot;)
ppc_intervals(x = colMeans(post_ridge$y_test), y = test_scale[,8],
              yrep = post_ridge$y_test, prob = 0.95) +
  ggtitle(&quot;95% Posterior Prediction Intervals&quot;) +
  xlab(&quot;Model Predicted Pr(Acceptance)&quot;) +
  ylab(&quot;Actual Pr(Acceptance)&quot;) +
  theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig6-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Looks great! We see that the 95% posterior prediction intervals contain the actual values in virtually all cases. Importantly, this visualization also makes it clear just how much uncertainty there is for each individual observation.</p>
<p>Next up, we will explore Traditional and Bayesian LASSO regressions, followed by a discussion of the benefits of Bayesian over frequentist regularization more broadly.</p>
</div>
</div>
<div id="lasso-regression" class="section level3">
<h3>LASSO Regression</h3>
<p>Now that we have covered ridge regression, LASSO regression only involves a minor revision to the loss function. Specifically, as opposed to penalizing the model based on the sum of squared <span class="math inline">\(\beta\)</span> weights, we will penalize the model by the sum of the absolute value of <span class="math inline">\(\beta\)</span> weights:</p>
<p><span class="math display">\[\underbrace{\underset{\boldsymbol{\beta}}{argmin}\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2}_{\text{Traditional Loss Function}} + \underbrace{\lambda\sum_{j=1}^{p}|\beta_{j}|}_{\text{LASSO Penalty}}\tag{7}\]</span></p>
<p>That’s it! But what difference does this make? At this point, it is useful to visualize the effect of the LASSO penalty on <span class="math inline">\(\beta\)</span> weight estimates, as we did above for the ridge penalty. As before, we are looking at penalty functions for varying settings of <span class="math inline">\(\lambda \in \{0, .5, 1.5\}\)</span>:</p>
<div id="htmlwidget-2" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-2">{"x":{"visdat":{"1506065ae472e":["function () ","plotlyVisDat"]},"cur_data":"1506065ae472e","attrs":{"1506065ae472e":{"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"opacity":0.5,"inherit":true},"1506065ae472e.1":{"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"inherit":true},"1506065ae472e.2":{"showscale":false,"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Beta_1"},"yaxis":{"title":"Beta_2"},"zaxis":{"title":"Penalty"}},"title":"LASSO Penalty Contour","hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"lasso_p1$z<br />lasso_p2$z<br />lasso_p3$z","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,null,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,0,0,0,0,0,null,null,null,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,null,null,0,0,0,0]],"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"opacity":0.5,"frame":null},{"colorbar":{"title":"lasso_p1$z<br />lasso_p2$z<br />lasso_p3$z","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[3,2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077,2.84615384615385,2.92307692307692,3],[2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077,2.84615384615385,2.92307692307692],[2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077,2.84615384615385],[2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769,2.76923076923077],[2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,2.53846153846154,2.61538461538462,2.69230769230769],[2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,2.61538461538462],[2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154],[2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846],[2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538],[2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231],[2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923],[2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615],[2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308],[2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.538461538461539,0.615384615384615,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2],[1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.461538461538461,0.538461538461538,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692],[1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615384,0.384615384615385,0.461538461538461,0.538461538461539,0.615384615384615,null,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,null,1.76923076923077,1.84615384615385],[1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692307,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077],[1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307692,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769],[1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.153846153846154,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307693,0.76923076923077,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462],[1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615384,0.307692307692307,0.230769230769231,0.153846153846154,0.0769230769230766,0.0769230769230769,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538461,0.538461538461538,0.615384615384615,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154],[1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461539,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.153846153846154,0.0769230769230769,0.0769230769230771,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154],[1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.461538461538461,0.384615384615385,0.307692307692308,0.230769230769231,0.153846153846154,0.153846153846154,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384615,0.692307692307693,0.76923076923077,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462],[1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307693,0.615384615384615,0.538461538461539,0.461538461538462,0.384615384615385,0.307692307692308,0.230769230769231,0.230769230769231,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.76923076923077,0.846153846153847,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769],[1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461539,0.461538461538462,0.384615384615385,0.307692307692308,0.307692307692308,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076924,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077],[1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,null,0.615384615384615,0.538461538461539,0.461538461538462,0.384615384615385,0.384615384615385,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385],[1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461539,0.461538461538461,0.461538461538462,0.538461538461539,0.615384615384616,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,null,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692],[2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307692,0.615384615384615,0.538461538461538,0.538461538461539,0.615384615384615,0.692307692307693,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2],[2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.692307692307693,0.615384615384615,0.615384615384616,0.692307692307693,0.76923076923077,0.846153846153846,0.923076923076923,null,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,null,2.07692307692308],[2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461539,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.76923076923077,0.692307692307693,0.692307692307693,0.76923076923077,0.846153846153847,0.923076923076924,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615],[2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.769230769230769,0.769230769230769,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923],[2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.846153846153846,0.846153846153846,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231],[2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,0.923076923076923,0.923076923076923,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538],[2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1,1,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846],[2.53846153846154,2.46153846153846,2.38461538461539,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.07692307692308,1.07692307692308,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,null],[2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.15384615384615,1.15384615384615,1.23076923076923,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,null,null],[2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461539,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.23076923076923,1.23076923076923,1.30769230769231,1.38461538461539,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,null,null,2.69230769230769],[2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.30769230769231,1.30769230769231,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,null,null,null,2.76923076923077],[2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,null,1.61538461538462,1.53846153846154,1.46153846153846,1.38461538461538,1.38461538461538,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,2.46153846153846,null,null,null,2.76923076923077,2.84615384615385],[2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.46153846153846,1.46153846153846,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,null,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461539,null,null,null,2.69230769230769,2.76923076923077,2.84615384615385,2.92307692307692],[3,2.92307692307692,2.84615384615385,2.76923076923077,2.69230769230769,2.61538461538462,2.53846153846154,2.46153846153846,2.38461538461538,2.30769230769231,2.23076923076923,2.15384615384615,2.07692307692308,2,1.92307692307692,1.84615384615385,1.76923076923077,1.69230769230769,1.61538461538462,1.53846153846154,1.53846153846154,1.61538461538462,1.69230769230769,1.76923076923077,1.84615384615385,1.92307692307692,2,2.07692307692308,2.15384615384615,2.23076923076923,2.30769230769231,2.38461538461538,2.46153846153846,2.53846153846154,null,null,2.76923076923077,2.84615384615385,2.92307692307692,3]],"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"frame":null},{"colorbar":{"title":"lasso_p1$z<br />lasso_p2$z<br />lasso_p3$z","ticklen":2},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":false,"z":[[9,8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538462,7.84615384615385,8.07692307692308,8.30769230769231,8.53846153846154,8.76923076923077,9],[8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461538,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538461,7.84615384615385,8.07692307692308,8.30769230769231,8.53846153846154,8.76923076923077],[8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,7.61538461538462,7.84615384615385,8.07692307692308,8.30769230769231,8.53846153846154],[8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461538,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538461,7.84615384615385,8.07692307692308,8.30769230769231],[8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461538,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461538,7.61538461538461,7.84615384615385,8.07692307692308],[7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538462,7.84615384615385],[7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538461],[7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539],[7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615],[6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692],[6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769],[6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846],[6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923],[6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6],[5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.38461538461538,1.61538461538461,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077],[5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,1.15384615384615,1.38461538461538,1.61538461538462,1.84615384615385,null,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538462,4.84615384615385,null,5.30769230769231,5.53846153846154],[5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076922,0.923076923076923,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231],[5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307692,0.692307692307693,0.923076923076923,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461538,4.61538461538462,4.84615384615385,5.07692307692308],[4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307693,0.461538461538461,0.461538461538462,0.692307692307693,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385],[4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076922,0.692307692307692,0.461538461538461,0.23076923076923,0.230769230769231,0.461538461538461,0.692307692307693,0.923076923076923,1.15384615384615,1.38461538461538,1.61538461538461,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461],[4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307693,0.461538461538462,0.230769230769231,0.230769230769231,0.461538461538462,0.692307692307693,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462],[4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.38461538461538,1.15384615384615,0.923076923076923,0.692307692307693,0.461538461538461,0.461538461538462,0.692307692307693,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385],[5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461539,1.15384615384615,0.923076923076924,0.692307692307693,0.692307692307693,0.923076923076924,1.15384615384616,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308],[5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384616,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461539,1.15384615384615,0.923076923076923,0.923076923076924,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231],[5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,null,1.84615384615385,1.61538461538462,1.38461538461539,1.15384615384615,1.15384615384615,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154],[5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538462,1.38461538461538,1.38461538461539,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,null,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077],[6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.61538461538461,1.61538461538462,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6],[6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,1.84615384615385,1.84615384615385,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,null,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,null,6.23076923076923],[6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.07692307692308,2.07692307692308,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307693,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846],[6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.30769230769231,2.30769230769231,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769],[6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384616,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.53846153846154,2.53846153846154,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692],[7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,2.76923076923077,2.76923076923077,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615],[7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3,3,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539],[7.61538461538462,7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.23076923076923,3.23076923076923,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,null],[7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.69230769230769,3.46153846153846,3.46153846153846,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538461,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,null,null],[8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,3.92307692307692,3.69230769230769,3.69230769230769,3.92307692307692,4.15384615384616,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,null,null,8.07692307692308],[8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.38461538461538,4.15384615384615,3.92307692307692,3.92307692307692,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,null,null,null,8.30769230769231],[8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,null,4.84615384615385,4.61538461538462,4.38461538461539,4.15384615384615,4.15384615384615,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,7.38461538461539,null,null,null,8.30769230769231,8.53846153846154],[8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538462,7.38461538461539,7.15384615384616,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538462,4.38461538461539,4.38461538461539,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,null,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384616,null,null,null,8.07692307692308,8.30769230769231,8.53846153846154,8.76923076923077],[9,8.76923076923077,8.53846153846154,8.30769230769231,8.07692307692308,7.84615384615385,7.61538461538461,7.38461538461539,7.15384615384615,6.92307692307692,6.69230769230769,6.46153846153846,6.23076923076923,6,5.76923076923077,5.53846153846154,5.30769230769231,5.07692307692308,4.84615384615385,4.61538461538461,4.61538461538462,4.84615384615385,5.07692307692308,5.30769230769231,5.53846153846154,5.76923076923077,6,6.23076923076923,6.46153846153846,6.69230769230769,6.92307692307692,7.15384615384615,7.38461538461539,7.61538461538462,null,null,8.30769230769231,8.53846153846154,8.76923076923077,9]],"type":"surface","x":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"y":[-3,-2.84615384615385,-2.69230769230769,-2.53846153846154,-2.38461538461538,-2.23076923076923,-2.07692307692308,-1.92307692307692,-1.76923076923077,-1.61538461538462,-1.46153846153846,-1.30769230769231,-1.15384615384615,-1,-0.846153846153846,-0.692307692307692,-0.538461538461538,-0.384615384615385,-0.230769230769231,-0.0769230769230766,0.0769230769230771,0.230769230769231,0.384615384615385,0.538461538461539,0.692307692307693,0.846153846153846,1,1.15384615384615,1.30769230769231,1.46153846153846,1.61538461538462,1.76923076923077,1.92307692307692,2.07692307692308,2.23076923076923,2.38461538461539,2.53846153846154,2.69230769230769,2.84615384615385,3],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Interesting! We can see here that unlike for the ridge penalty, there are sharp corners in the geometry, which correspond to when either <span class="math inline">\(\beta_{1}\)</span> or <span class="math inline">\(\beta_{2}\)</span> equal 0. Further, the LASSO penalty shows a linear increase as we move away from the origin. Together, the sharp corners on 0-valued <span class="math inline">\(\beta\)</span> weights and the linear increase in penalty make the LASSO penalty favor solutions that set one (or more) <span class="math inline">\(\beta\)</span> weights to exactly 0.</p>
<div id="frequentist-lasso-regression" class="section level4">
<h4>Frequentist LASSO Regression</h4>
<p>The code below is very similar to what we had for traditional ridge regression, except that we will set <span class="math inline">\(\alpha = 1\)</span> as opposed to <span class="math inline">\(\alpha = 0\)</span>:</p>
<pre class="r"><code># Scale training data (get rid of ID)
train_scale &lt;- scale(train_dat[,2:9])

# Penalty term values to test
lambdas &lt;- 10^seq(3, -2, by = -.1)

# Fit and cross-validate
fit_lasso &lt;- glmnet(x = train_scale[,-8], y = train_scale[,8], 
                    alpha = 1, lambda = lambdas, intercept = F)
cv_lasso &lt;- cv.glmnet(x = train_scale[,-8], y = train_scale[,8], 
                      alpha = 1, lambda = lambdas, intercept = F, 
                      grouped = FALSE)

# Find optimal penalty term (lambda)
opt_lambda_lasso &lt;- cv_lasso$lambda.min

# Generate predictions on the test set
y_pred_lasso &lt;- predict(fit_lasso, s = opt_lambda_lasso, newx = test_scale[,-8])

# Plot cor(predicted, actual)
qplot(x = y_pred_lasso[,1], y = test_scale[,8],
      main = paste0(&quot;r = &quot;, round(cor(test_scale[,8], y_pred_lasso[,1]), 2))) +
  xlab(&quot;Model Predicted Pr(Acceptance)&quot;) +
  ylab(&quot;Actual Pr(Acceptance)&quot;) +
  theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig8-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Clearly, there is not much of a performance difference between frequentist ridge and LASSO regression in this dataset. However, it is worth noting that the LASSO is often more useful when we expect the solution to be sparse—that is, we expect that many of the predictors in our model are mostly noise, and are goal is to identify more robust effects. In such a case, the LASSO does a good job of setting <span class="math inline">\(\beta\)</span> weights on noisy predictors to exactly 0.</p>
</div>
<div id="bayesian-lasso-regression" class="section level4">
<h4>Bayesian LASSO Regression</h4>
<p>Recall that for Bayesian ridge regression, we only needed to specifiy a normal prior distribution to the <span class="math inline">\(\beta\)</span> weights that we were aiming to regularize. For Bayesian LASSO regression, the only difference is in the form of the prior distribution. Specifically, <strong>setting a Laplace (i.e. double-exponential) prior on the <span class="math inline">\(\beta\)</span> weights is mathematically equivalent in expectation to the frequentist LASSO penalty</strong> (see e.g., <a href="https://stats.stackexchange.com/questions/182098/why-is-lasso-penalty-equivalent-to-the-double-exponential-laplace-prior">here</a>):</p>
<p><span class="math display">\[\boldsymbol{\beta} \sim \text{double-exponential}(0, \tau_{\beta})\tag{8}\]</span>
Here, <span class="math inline">\(\tau_{\beta}~(0 &lt; \tau_{\beta} &lt; \infty)\)</span> is a scale parameter that controls how peaked the prior distribution is around the center (0 in this case). As <span class="math inline">\(\tau_{\beta} \rightarrow 0\)</span>, then the model assigns infinite weight on 0-valued <span class="math inline">\(\beta\)</span> weights (i.e. no learning from data). Conversely, as <span class="math inline">\(\tau_{\beta} \rightarrow \infty\)</span>, the prior reduces to a uniform prior, thus leading to no regularization at all (i.e traditional regression, albeit with posterior distributions rather than point estimates). It can be useful to visualize what this prior distribution looks like over a single <span class="math inline">\(\beta\)</span> weight. For example, centered at 0 with <span class="math inline">\(\tau_{\beta} = 0.5\)</span>:</p>
<pre class="r"><code>qplot(x = seq(-3, 3, .1), y = rmutil::dlaplace(seq(-3, 3, .1), 0, .5), geom = &quot;line&quot;) +
  ggtitle(&quot;Laplace Prior with tau = 0.5&quot;) +
  xlab(expression(beta[1])) +
  ylab(&quot;Density&quot;) +
  theme_minimal(base_size = 20) </code></pre>
<pre><code>## Registered S3 method overwritten by &#39;rmutil&#39;:
##   method         from
##   print.response httr</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig9-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Compared to the ridge prior, which is a normal distribution, it is clear that the Laplace distribiution places much more probability mass directly on 0, which produces the variable selection effect specific to LASSO regression. Note also that such peakedness explains why there are sharp corners in the frequentist penalty function (see the LASSO contour plot above).</p>
<p>Below is the Stan code that specifies this Bayesian variant of LASSO regression. Like for the Bayesian ridge regression above, this code is loosely based on <a href="https://osf.io/cg8fq/">Erp, Oberski, &amp; Mulder (2019)</a>:</p>
<pre class="stan"><code>data{
    int N_train;             // &quot;# training observations&quot;
    int N_test;              // &quot;# test observations&quot;
    int N_pred;              // &quot;# predictor variables&quot;
    vector[N_train] y_train; // &quot;training outcomes&quot;
    matrix[N_train, N_pred] X_train; // &quot;training data&quot;
    matrix[N_test, N_pred] X_test;   // &quot;testing data&quot;
}
parameters{
    real&lt;lower=0&gt; sigma;   // &quot;error SD&quot;
    real&lt;lower=0&gt; sigma_B; // &quot;(hierarchical) SD across betas&quot;
    vector[N_pred] beta;   // &quot;regression beta weights&quot;
}
model{
  // &quot;group-level (hierarchical) SD across betas&quot;
  sigma_B ~ cauchy(0, 1);
  
  // &quot;Prior on SD&quot;
  sigma ~ normal(0, 1);
  
  // &quot;beta prior (Note this is the only change!)&quot;
  beta ~ double_exponential(0, sigma_B); 
    
  // &quot;model likelihood&quot;
    y_train ~ normal(X_train*beta, sigma);
}
generated quantities{ 
    real y_test[N_test]; // &quot;test data predictions&quot;
    for(i in 1:N_test){
        y_test[i] = normal_rng(X_test[i,] * beta, sigma);
    }
}</code></pre>
<p>Time to fit the model and visualize results:</p>
<pre class="r"><code># Fit the model using Stan&#39;s NUTS HMC sampler
fit_bayes_lasso &lt;- sampling(bayes_lasso, stan_dat, iter = 2000, 
                            warmup = 500, chains = 3, cores = 3)

# Extract posterior distribution (parameters and predictions)
post_lasso &lt;- rstan::extract(fit_bayes_lasso)

# Compute mean of the posterior predictive distribution over test set predictors,
# which integrates out uncertainty in parameter estimates
y_pred_bayes_lasso &lt;- apply(post_lasso$y_test, 2, mean)

# Plot correlation between posterior predicted mean and actual Pr(Acceptance)
qplot(x = y_pred_bayes_lasso, y = test_scale[,8],
      main = paste0(&quot;r = &quot;, round(cor(test_scale[,8], y_pred_bayes_lasso), 2))) +
    xlab(&quot;Model Predicted Pr(Acceptance)&quot;) +
    ylab(&quot;Actual Pr(Acceptance)&quot;) +
    theme_minimal(base_size = 20)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig10-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Like before, the Bayesian LASSO is producing very similar results compared to the frequentist version.</p>
</div>
</div>
</div>
<div id="comparing-the-models" class="section level2">
<h2>Comparing the Models</h2>
<p>So far, we have described and fit both the frequentist and Bayesian versions of ridge and LASSO regression to our training data, and we have shown that we can make pretty outstanding predictions on our held-out test set! However, we have not explored the parameters that each model has estimated. Here, we will begin to probe our models.</p>
<p>First off, let’s look at the ridge regression model parameters.</p>
<pre class="r"><code># Extract beta weights from traditional regression model
betas_trad &lt;- fit_lr$coefficients[-1]

# Extract optimal ridge beta weights from CV
betas_ridge &lt;- fit_ridge$beta[,which(cv_ridge$lambda==cv_ridge$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p1 &lt;- mcmc_areas(as.array(fit_bayes_ridge), pars = paste0(&quot;beta[&quot;, 1:7, &quot;]&quot;), 
           prob = 0.8, prob_outer = 0.99, point_est = &quot;mean&quot;) +
  geom_point(x = rep(betas_ridge, 1024), y = rep(7:1, 1024), color = I(&quot;#c61d29&quot;), size = 2) + # rep() here is an annoying work-around to this code breaking in an update...
  geom_point(x = rep(betas_trad, 1024), y = rep(7:1, 1024), color = I(&quot;black&quot;), size = 2) +
  ggtitle(&quot;Ridge Penalty&quot;) +
  theme_minimal(base_size = 20)

# Extract optimal LASSO beta weights from CV
betas_lasso &lt;- fit_lasso$beta[,which(cv_lasso$lambda==cv_lasso$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p2 &lt;- mcmc_areas(as.array(fit_bayes_lasso), pars = paste0(&quot;beta[&quot;, 1:7, &quot;]&quot;), 
           prob = 0.8, prob_outer = 0.99) +
  geom_point(x = rep(betas_lasso, 1024), y = rep(7:1, 1024), color = I(&quot;#c61d29&quot;), size = 2) +
  geom_point(x = rep(betas_trad, 1024), y = rep(7:1, 1024), color = I(&quot;black&quot;), size = 2) +
  ggtitle(&quot;LASSO Penalty&quot;) +
  theme_minimal(base_size = 20)

# Plot in grid
cowplot::plot_grid(p1, p2, ncol = 2)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig11-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Above, the disributions reflect the Bayesian posteriors for each <span class="math inline">\(\beta\)</span> weight (shading represents the 80% central interval, based on quantiles; bar representing the posterior mean), the red points indicate the regularized frequentist point estimates, and black points are estimates from the trditional, non-regularized frequentist regression model (shown on each plot for comparison purposes). Importantly, the traditional regression estimates are far from 0 in many cases, whereas both frequentist and Bayesian regularization estimates are much closer to 0. <strong>It is this conservatism that allows <em>biased</em> estimates (from regularization/penalization) to outperform traditional models that offer <em>unbiased estimates</em> when tested on external data</strong>.</p>
<p>Thus, the benefit of regularization is that we get better parameter estimates (translating to better model performance) in low and/or noisy data settings. Crucially, as we collect more and more data, the effects of regularization become less apparent. We can demonstrate this by fitting all the models above to 400—as opposed to only 20—observations and compare the coefficients to the above plot.</p>
<p>To get started, we first go through our pre-processing steps once again:</p>
<pre class="r"><code># Training data
train_dat2 &lt;- grad_dat[1:400,] # Use 400 observations now

# Testing data
test_dat2 &lt;- grad_dat[401:500,] # Only using last 100 now

# Scale training data (and get rid of ID)
train_scale2 &lt;- scale(train_dat2[,2:9])

# Find means and SDs of training data variables
means2 &lt;- attributes(train_scale2)$`scaled:center`
SDs2 &lt;- attributes(train_scale2)$`scaled:scale`

# Scale test data using training data summary stats (no cheating!)
test_scale2 &lt;- scale(test_dat2[,-1], center = means2, scale = SDs2)

# Prepare data for Stan
stan_dat2 &lt;- list(N_train = nrow(train_scale2),
                  N_test  = nrow(test_scale2),
                  N_pred  = ncol(train_scale2)-1,
                  y_train = train_scale2[,8],
                  X_train = train_scale2[,-8],
                  X_test  = test_scale2[,-8])</code></pre>
<p>Then, we re-fit all the models:</p>
<pre class="r"><code># Fit linear regression
fit_lr2 &lt;- lm(Chance.of.Admit ~ ., data = as.data.frame(train_scale2))

# Fit and cross-validate ridge
fit_ridge2 &lt;- glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                     alpha = 0, lambda = lambdas, intercept = F)
cv_ridge2 &lt;- cv.glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                       alpha = 0, lambda = lambdas, intercept = F)

# Fit the model using Stan&#39;s NUTS HMC sampler
fit_bayes_ridge2 &lt;- sampling(bayes_ridge, stan_dat2, iter = 2000, 
                             warmup = 500, chains = 3, cores = 3)
# Fit and cross-validate LASSO
fit_lasso2 &lt;- glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                     alpha = 1, lambda = lambdas, intercept = F)
cv_lasso2 &lt;- cv.glmnet(x = train_scale2[,-8], y = train_scale2[,8], 
                       alpha = 1, lambda = lambdas, intercept = F)
# Fit the model using Stan&#39;s NUTS HMC sampler
fit_bayes_lasso2 &lt;- sampling(bayes_lasso, stan_dat2, iter = 2000, 
                             warmup = 500, chains = 3, cores = 3)</code></pre>
<p>And time to plot!</p>
<pre class="r"><code># Extract beta weights from traditional regression model
betas_trad2 &lt;- fit_lr2$coefficients[-1]

# Extract optimal ridge beta weights from CV
betas_ridge2 &lt;- fit_ridge2$beta[,which(cv_ridge2$lambda==cv_ridge2$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p3 &lt;- mcmc_areas(as.array(fit_bayes_ridge2), pars = paste0(&quot;beta[&quot;, 1:7, &quot;]&quot;), 
           prob = 0.8, prob_outer = 0.99, point_est = &quot;mean&quot;) +
  geom_point(x = rep(betas_ridge2, 1024), y = rep(7:1, 1024), color = I(&quot;#c61d29&quot;), size = 2) +
  geom_point(x = rep(betas_trad2, 1024), y = rep(7:1, 1024), color = I(&quot;black&quot;), size = 2, alpha = .6) +
  ggtitle(&quot;Ridge Penalty&quot;) +
  theme_minimal(base_size = 20)

# Extract optimal LASSO beta weights from CV
betas_lasso2 &lt;- fit_lasso2$beta[,which(cv_lasso2$lambda==cv_lasso2$lambda.min)]

# Plot posterior distributions and frequentist point estimates
p4 &lt;- mcmc_areas(as.array(fit_bayes_lasso2), pars = paste0(&quot;beta[&quot;, 1:7, &quot;]&quot;), 
           prob = 0.8, prob_outer = 0.99) +
  geom_point(x = rep(betas_lasso2, 1024), y = rep(7:1, 1024), color = I(&quot;#c61d29&quot;), size = 2) +
  geom_point(x = rep(betas_trad2, 1024), y = rep(7:1, 1024), color = I(&quot;black&quot;), size = 2, alpha = .6) +
  ggtitle(&quot;LASSO Penalty&quot;) +
  theme_minimal(base_size = 20)

cowplot::plot_grid(p3, p4, ncol = 2)</code></pre>
<p><img src="/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors_files/figure-html/2019-05-06_fig12-1.svg" width="672" style='height: 100%; width: 100%; object-fit: contain' /></p>
<p>Woah! Unlike before, all of our estimates pretty much fall right on top of each other. Because this dataset has both: (1) many observations, and (2) a relatively high signal-to-noise ratio, both traditional and regularized regression methods offer the same conclusion when we make use the full training dataset.</p>
</div>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>In this post, we learned about the benefits of using regularized/penalized regression models over traditional regression. We determined that in low and/or noisy data settings, the so-called <em>unbiased</em> estimates given by traditional regression modeling actually lead to worse-off model performance. Importantly, we learned that this occurs because being ubiased allows a model to learn a lot from the data, including learning patterns of noise. Then, we learned that <em>biased</em> methods such as ridge and LASSO regression restrict the amount of learning that we get from data, which leads to better estimates in low and/or noisy data settings.</p>
<p>Finally, we saw that hierarchical Bayesian models actually contain frequentist ridge and LASSO regression as a special case—namely, we can choose a prior distribution across the <span class="math inline">\(\beta\)</span> weights that gives us a solution that is equivalent to that of the frequentist ridge or LASSO methods! Not only that, but Bayesian regression gives us a full posterior distribution for each parameter, thus circumventing problems with frequentist regularization that require the use of bootstrapping to estimate confidence intervals.</p>
<div id="so-what" class="section level2">
<h2>So What?</h2>
<p><strong>Psychology</strong>—and many other areas of social and behavoiral science—<strong>is in the midst of a replication crisis</strong>. While many factors are at play here, one problem is that the methods we use to draw inference from our data are too liberal. Taken with the high measurement error, low sample sizes, and lack of mechanistic models in our research areas, we are bound to overestimate the <a href="https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/">magnitude (and even miss the sign</a>) of effects that we are interested in if we continue to use methods that are prone to overfitting. <strong><em>Regularization provides an elegant solution to this problem of “learning too much” from our data</em></strong>.</p>
<p>Moving forward, I hope that you consider regularizing your models! (and if you are feeling really fancy, go Bayes! Embrace the uncertainty!)</p>
</div>
</div>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/r/">R</a>
  
  <a class="badge badge-light" href="/tag/machine-learning/">Machine Learning</a>
  
  <a class="badge badge-light" href="/tag/regularization/">Regularization</a>
  
  <a class="badge badge-light" href="/tag/bayesian-statistics/">Bayesian Statistics</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/&amp;text=On%20the%20equivalency%20between%20frequentist%20Ridge%20%28and%20LASSO%29%20regression%20and%20hierarchial%20Bayesian%20regression" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/&amp;t=On%20the%20equivalency%20between%20frequentist%20Ridge%20%28and%20LASSO%29%20regression%20and%20hierarchial%20Bayesian%20regression" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=On%20the%20equivalency%20between%20frequentist%20Ridge%20%28and%20LASSO%29%20regression%20and%20hierarchial%20Bayesian%20regression&amp;body=http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/&amp;title=On%20the%20equivalency%20between%20frequentist%20Ridge%20%28and%20LASSO%29%20regression%20and%20hierarchial%20Bayesian%20regression" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=On%20the%20equivalency%20between%20frequentist%20Ridge%20%28and%20LASSO%29%20regression%20and%20hierarchial%20Bayesian%20regression%20http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=http://haines-lab.com/post/2019-05-06-on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/&amp;title=On%20the%20equivalency%20between%20frequentist%20Ridge%20%28and%20LASSO%29%20regression%20and%20hierarchial%20Bayesian%20regression" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="http://haines-lab.com/"><img class="avatar mr-3 avatar-circle" src="/author/nathaniel-haines/avatar_hud6e4cbd359920a21ae5755d0b163666c_1382354_270x270_fill_q75_lanczos_center.jpg" alt="Nathaniel Haines"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="http://haines-lab.com/">Nathaniel Haines</a></h5>
      <h6 class="card-subtitle">Computational Psychologist &amp; Data Scientist, PhD</h6>
      <p class="card-text">An academic Bayesian who is currently exploring the high dimensional posterior distribution of life</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/nate__haines" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=lg741SgAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/Nathaniel-Haines" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/nathaniel-haines-216049101/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  

  
  
  

  
  <section id="comments">
    
<div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "haines-lab-com" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  </section>
  








  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/2021-01-10-modeling-classic-effects-dunning-kruger/2021-01-10-modeling-classic-effects-dunning-kruger/">A Series on Building Formal Models of Classic Psychological Effects: Part 1, the Dunning-Kruger Effect</a></li>
      
      <li><a href="/post/2020-06-13-on-curbing-your-measurement-error/2020-06-13-on-curbing-your-measurement-error/">On Curbing Your Measurement Error: From Classical Corrections to Generative Models</a></li>
      
      <li><a href="/post/2019-05-29-thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/">Thinking generatively: Why do we use atheoretical statistical models to test substantive psychological theories?</a></li>
      
      <li><a href="/post/2018-03-24-human-choice-and-reinforcement-learning-3/2018-03-24-human-choice-and-reinforcement-learning-3/">Human Choice and Reinforcement Learning (3)</a></li>
      
      <li><a href="/post/2017-04-07-human-choice-and-reinforcement-learning-2/">Human Choice and Reinforcement Learning (2)</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  
  <p class="powered-by">
    © 2022 Nathaniel Haines, PhD
  </p>
  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    
      <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    
      <script id="dsq-count-scr" src="https://haines-lab-com.disqus.com/count.js" async></script>
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.1cb9e2fc8399acee94eab837265b73bf.js" type="module"></script>
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.247fd8f54253895301106e3006f53f38.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
