<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Computational Psychology</title>
    <link>http://haines-lab.com/post/</link>
    <description>Recent content in Posts on Computational Psychology</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Nathaniel Haines</copyright>
    <lastBuildDate>Sat, 13 Jun 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://haines-lab.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>On Curbing Your Measurement Error: From Classical Corrections to Generative Models</title>
      <link>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/</link>
      <pubDate>Sat, 13 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2020-06-13-on-curbing-your-measurement-error/</guid>
      <description>Introduction In this post, we will explore how measurement error arising from imprecise parameter estimation can be corrected for. Specifically, we will explore the case where our goal is to estimate the correlation between a self-report and behavioral measure–a common situation throughout the social and behavioral sciences.
For example, as someone who studies impulsivity and externalizing psychopathology, I am often interested in whether self-reports of trait impulsivity (e.g., the Barratt Impulsiveness Scale) correlate with performance on tasks designed to measure impulsive behavior (e.</description>
    </item>
    
    <item>
      <title>Thinking generatively: Why do we use atheoretical statistical models to test substantive psychological theories?</title>
      <link>http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/thinking-generatively-why-do-we-use-atheoretical-statistical-models-to-test-substantive-psychological-theories/</guid>
      <description>The Reliability Paradox Defining Reliability In 2017, Hedge, Powell, and Sumner (2017) conducted a study to determine the reliability of a variety of of behavioral tasks. Reliability has many different meanings throughout the psychological literature, but what Hedge et al. were interested in was how well a behavioral measure consistently ranks individuals. In other words, when I have people perform a task and then measure their performance, does the measure that I use to summarize their behavior show high test-retest reliability?</description>
    </item>
    
    <item>
      <title>On the equivalency between frequentist Ridge (and LASSO) regression and hierarchial Bayesian regression</title>
      <link>http://haines-lab.com/post/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/</link>
      <pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/on-the-equivalency-between-the-lasso-ridge-regression-and-specific-bayesian-priors/</guid>
      <description>Introduction In this post, we will explore frequentist and Bayesian analogues of regularized/penalized linear regression models (e.g., LASSO [L1 penalty], Ridge regression [L2 penalty]), which are an extention of traditional linear regression models of the form:
\[y = \beta_{0}+X\beta + \epsilon\tag{1}\] where \(\epsilon\) is the error, which is normally distributed as:
\[\epsilon \sim \mathcal{N}(0, \sigma)\tag{2}\] Unlike these traditional linear regression models, regularized linear regression models produce biased estimates for the \(\beta\) weights.</description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (3)</title>
      <link>http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2018-03-24-human-choice-and-reinforcement-learning-3/</guid>
      <description>1. Goals of Paramter Estimation When estimating paramters for a given model, we typically aim to make an inference on an individual’s underlying decision process. We may be inferring a variety of different factors, such as the rate at which someone updates their expectations, the way that someone subjectively values an outcome, or the amount of exploration versus exploitation that someone engages in. Once we estimate an individual’s parameters, we can compare then to other people or even other groups of people.</description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (2)</title>
      <link>http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/</link>
      <pubDate>Fri, 07 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2017-04-07-human-choice-and-reinforcement-learning-2/</guid>
      <description>1. Answer to post 1 In the previous post, I reviewed the Rescorla-Wagner updating (Delta) rule and its contemporary instantiation. At the end, I asked the following question:
 How should you change the learning rate so that the expected win rate is always the average of all past outcomes?  We will go over the answer to this question before progressing to the use of the Delta rule in modeling human choice.</description>
    </item>
    
    <item>
      <title>Human Choice and Reinforcement Learning (1)</title>
      <link>http://haines-lab.com/post/2017-04-04-choice_rl_1/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>http://haines-lab.com/post/2017-04-04-choice_rl_1/</guid>
      <description>1. Short history In 1972, Robert Rescorla and Allan Wagner developed a formal theory of associative learning, the process through which multiple stimuli are associated with one-another. The most widely used example (Fig. 1) of associative learning comes straight from Psychology 101–Pavlov’s dog.
Figure 1 The idea is simple, and it’s something that we experience quite often in everyday life. In the same way that Pavlov’s dog begins to drool after hearing a bell, certain cognitive and/or biological processes are triggered when we are exposed to stimuli that we have been exposed to in the past.</description>
    </item>
    
  </channel>
</rss>